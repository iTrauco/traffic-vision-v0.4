{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b9d10c5-195d-41d2-b74c-12b3e06f642c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## ğŸ““ Notebook Manager\n",
    "\n",
    "This cell initializes the widgets required for managing your research notebook. Please run the cell below to enable functionality for:\n",
    "- Exporting cells tagged with `export` into a `clean` notebook\n",
    "- Generating a dynamic Table of Contents (TOC)\n",
    "- Exporting the notebook to GitHub-compatible Markdown\n",
    "\n",
    "â¡ï¸ **Be sure to execute the next cell before continuing with any editing or exporting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95312728-5bf6-40b4-854e-fbc5ff4b14e1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd84c51e44f346eab94876149e307cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Button(button_style='primary', description='Generate TOC', icon='list', style=Buâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 1 - Workflow Tools\n",
    "import sys\n",
    "sys.path.insert(0, '../../lib')\n",
    "\n",
    "from notebook_tools import TOCWidget, ExportWidget\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "# Create widget instances\n",
    "toc = TOCWidget()\n",
    "export = ExportWidget()\n",
    "\n",
    "# Create horizontal layout\n",
    "left_side = widgets.VBox([toc.button, export.button, toc.status])\n",
    "right_side = widgets.VBox([toc.output, export.output])\n",
    "\n",
    "# Display side by side\n",
    "display(widgets.HBox([left_side, right_side]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ba2379-457a-4ef1-b843-5cd9235dcd47",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "# ğŸš¦ Traffic Video Preprocessing - Methodology [VERSION]\n",
    "\n",
    "## ğŸ¯ Purpose\n",
    "This notebook implements the data preprocessing workflow for GDOT traffic camera videos. We process raw video files into frame sequences suitable for computer vision tasks.\n",
    "\n",
    "## ğŸ“‹ Context\n",
    "- **Data Source**: 30 GDOT traffic camera feeds (recorded locally)\n",
    "- **Video Specs**: 480 resolution, 15 fps\n",
    "- **Methodology Goal**: Establish reproducible preprocessing workflow with clear documentation\n",
    "\n",
    "## ğŸ”„ Workflow Overview\n",
    "1. Video ingestion and cataloging\n",
    "2. Frame extraction\n",
    "3. Quality control\n",
    "4. Spatial transformations\n",
    "5. Color normalization\n",
    "6. Temporal downsampling\n",
    "7. Data organization\n",
    "8. Export and storage\n",
    "\n",
    "## âš¡ Key Improvements (Methodology [VERSION])\n",
    "- Added reproducibility checkpoints\n",
    "- Streamlined workflow touchpoints\n",
    "- Enhanced error handling and logging\n",
    "\n",
    "## ğŸ“š Notebook Structure\n",
    "- **Setup**: Environment and dependencies\n",
    "- **Processing**: Step-by-step video preprocessing\n",
    "- **Validation**: Quality checks and verification\n",
    "- **Summary**: Results and analysis (see end of notebook)\n",
    "\n",
    "*Processing completed: [DATE] | Methodology version: [VERSION]*\n",
    "\n",
    "**Last Updated**: [DATE]  \n",
    "**Author**: [NAME]  \n",
    "**Version**: [VERSION]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc58eef-463e-465b-9b53-acdd3bc398dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## ğŸ“‘ Table of Contents (Auto-Generated)\n",
    "\n",
    "This section will automatically generate a table of contents for your research notebook once you run the **Generate TOC** function. The table of contents will help you navigate through your data collection, analysis, and findings as your citizen science project develops.\n",
    "\n",
    "â¡ï¸ **Do not edit this cell manually. It will be overwritten automatically.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877dad16-2ca4-4acd-acc0-064c3f5a8cd0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "<!-- TOC -->\n",
    "# Table of Contents\n",
    "\n",
    "- [ğŸ““ Notebook Manager](#ğŸ““-notebook-manager)\n",
    "- [ğŸ¯ Purpose](#ğŸ¯-purpose)\n",
    "- [ğŸ“‹ Context](#ğŸ“‹-context)\n",
    "- [ğŸ”„ Workflow Overview](#ğŸ”„-workflow-overview)\n",
    "- [âš¡ Key Improvements (Methodology [VERSION])](#âš¡-key-improvements-(methodology-[version]))\n",
    "- [ğŸ“š Notebook Structure](#ğŸ“š-notebook-structure)\n",
    "- [ğŸ“‘ Table of Contents (Auto-Generated)](#ğŸ“‘-table-of-contents-(auto-generated))\n",
    "- [ğŸ”§ Environment Setup](#ğŸ”§-environment-setup)\n",
    "  - [ğŸ“Š Analysis & Observations](#ğŸ“Š-analysis-&-observations)\n",
    "    - [Results](#results)\n",
    "    - [Observations](#observations)\n",
    "    - [Notes](#notes)\n",
    "- [ğŸ”„ Progress Tracking & Checkpoint System](#ğŸ”„-progress-tracking-&-checkpoint-system)\n",
    "- [ğŸ’¾ Initialize Checkpoint and Progress Tracking Functions](#ğŸ’¾-initialize-checkpoint-and-progress-tracking-functions)\n",
    "  - [ğŸ“Š Analysis & ObservationS](#ğŸ“Š-analysis-&-observations)\n",
    "    - [Results](#results)\n",
    "    - [Observations](#observations)\n",
    "    - [Notes](#notes)\n",
    "- [ğŸ“¹ Video Ingestion & Cataloging](#ğŸ“¹-video-ingestion-&-cataloging)\n",
    "  - [ğŸ“Š Analysis & Observations](#ğŸ“Š-analysis-&-observations)\n",
    "    - [Results](#results)\n",
    "    - [Observations](#observations)\n",
    "    - [Notes](#notes)\n",
    "- [ğŸï¸ Frame Extraction](#ğŸï¸-frame-extraction)\n",
    "  - [ğŸ“Š Analysis & Observations](#ğŸ“Š-analysis-&-observations)\n",
    "    - [Results](#results)\n",
    "    - [Observations](#observations)\n",
    "    - [Notes](#notes)\n",
    "- [ğŸ” Image Quality Control](#ğŸ”-image-quality-control)\n",
    "  - [ğŸ“Š Analysis & Observations](#ğŸ“Š-analysis-&-observations)\n",
    "    - [Results](#results)\n",
    "    - [Observations](#observations)\n",
    "    - [Notes](#notes)\n",
    "- [ğŸ“ Spatial Transformations](#ğŸ“-spatial-transformations)\n",
    "  - [ğŸ“Š Analysis & Observations](#ğŸ“Š-analysis-&-observations)\n",
    "    - [Results](#results)\n",
    "    - [Observations](#observations)\n",
    "    - [Notes](#notes)\n",
    "- [ğŸ¨ Color Space Normalization](#ğŸ¨-color-space-normalization)\n",
    "  - [ğŸ“Š Analysis & Observations](#ğŸ“Š-analysis-&-observations)\n",
    "    - [Results](#results)\n",
    "    - [Observations](#observations)\n",
    "    - [Notes](#notes)\n",
    "- [â±ï¸ Temporal Downsampling](#â±ï¸-temporal-downsampling)\n",
    "  - [ğŸ“Š Analysis & Observations](#ğŸ“Š-analysis-&-observations)\n",
    "    - [Results](#results)\n",
    "    - [Observations](#observations)\n",
    "    - [Notes](#notes)\n",
    "  - [ğŸ“Š Analysis & Observations](#ğŸ“Š-analysis-&-observations)\n",
    "    - [Results](#results)\n",
    "    - [Observations](#observations)\n",
    "    - [Notes](#notes)\n",
    "  - [ğŸ“Š Analysis & Observations](#ğŸ“Š-analysis-&-observations)\n",
    "    - [Results](#results)\n",
    "    - [Observations](#observations)\n",
    "    - [Notes](#notes)\n",
    "\n",
    "<!-- /TOC -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fedfb01-d2fa-4487-9acc-d87ed78e77b5",
   "metadata": {},
   "source": [
    "\n",
    "## ğŸ”§ Environment Setup\n",
    "\n",
    "This cell establishes the preprocessing environment by:\n",
    "\n",
    "1. **Import Required Libraries**\n",
    "   - OpenCV (cv2) for video processing and frame extraction\n",
    "   - NumPy for array operations and numerical computations\n",
    "   - Pandas for data organization and metadata management\n",
    "   - Logging for process tracking and error reporting\n",
    "   - System utilities for path handling and file operations\n",
    "\n",
    "2. **Library Verification**\n",
    "   - Check OpenCV installation and version\n",
    "   - Verify NumPy and Pandas availability\n",
    "   - Display version information for debugging\n",
    "\n",
    "3. **Initialize Helper Functions**\n",
    "   - **calculate_brightness()**: Compute average pixel intensity (0-255)\n",
    "   - **calculate_blur_score()**: Measure sharpness using Laplacian variance\n",
    "   - **get_video_metadata()**: Extract video properties (fps, resolution, duration)\n",
    "\n",
    "4. **Directory Setup**\n",
    "   - Create output directory structure\n",
    "   - Ensure path exists before processing begins\n",
    "\n",
    "5. **Codec Validation**\n",
    "   - Test preferred video codec availability\n",
    "   - Confirm fourcc code generation\n",
    "\n",
    "**Note**: This cell must run successfully before proceeding with video processing. Any import errors or missing dependencies will be reported here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41584407-6c34-4cd4-bc68-51809fcaf8ec",
   "metadata": {},
   "source": [
    "### ğŸ“ Preprocessing Configuration Parameters\n",
    "\n",
    "This cell defines all parameters for individual video preprocessing. Parameters are organized into categories with emoji indicators:\n",
    "\n",
    "#### Target Parameters\n",
    "- ğŸ¯ **VIDEO_ID**: Specific camera to process (e.g., ATL-1005)\n",
    "- ğŸ¯ **BATCH_DATE**: Date from batch analysis (YYYYMMDD format)\n",
    "\n",
    "#### Path Configuration  \n",
    "- ğŸ“ **INPUT_BASE**: Root directory for video recordings\n",
    "- ğŸ“ **OUTPUT_BASE**: Root directory for processed output\n",
    "- ğŸ“ **VIDEO_DIR**: Derived path to specific camera/date videos\n",
    "- ğŸ“ **OUTPUT_DIR**: Derived path for this preprocessing run\n",
    "\n",
    "#### Processing Settings\n",
    "- ğŸ“Š **FRAMES_TO_EXTRACT**: Total number of frames to extract\n",
    "- ğŸ“Š **SAMPLE_RATE**: Extract every Nth frame from video\n",
    "\n",
    "#### Quality Thresholds\n",
    "Values from batch analysis quality metrics:\n",
    "- ğŸ” **brightness_min**: Minimum acceptable brightness (0-255)\n",
    "- ğŸ” **brightness_max**: Maximum acceptable brightness (0-255)  \n",
    "- ğŸ” **blur_min**: Minimum blur score (Laplacian variance)\n",
    "\n",
    "#### Video Settings\n",
    "- ğŸ¥ **PREFERRED_CODEC**: Primary video codec for processing\n",
    "- ğŸ¥ **FALLBACK_CODECS**: Alternative codecs if primary fails\n",
    "- ğŸ¥ **MAX_FRAME_WIDTH**: Maximum frame width for resizing\n",
    "- ğŸ¥ **MAX_FRAME_HEIGHT**: Maximum frame height for resizing\n",
    "- ğŸ¥ **JPEG_QUALITY**: Output quality for saved frames (0-100)\n",
    "\n",
    "**Note**: These values are hardcoded for initial testing. Future versions will read from `preprocessing_config.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "276a0658-274e-4798-9220-d3db425eb890",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  Processing: ATL-1005 from 2025-06-20\n",
      "  Output to: ../../data/preprocessing/individual_analysis/2025-06-20/ATL-1005\n"
     ]
    }
   ],
   "source": [
    "# preprocessing configuration parameters\n",
    "from pathlib import Path\n",
    "\n",
    "CONFIG = {\n",
    "    # target parameters\n",
    "    'VIDEO_ID': 'ATL-1005',  # ğŸ¯ camera to process\n",
    "    'BATCH_DATE': '20250620',  # ğŸ¯ date from batch analysis\n",
    "    'TARGET_HOUR': 12,  # ğŸ¯ target hour (noon)\n",
    "    \n",
    "    # path configuration  \n",
    "    'INPUT_BASE': Path.home() / 'traffic-recordings',  # ğŸ“ video source\n",
    "    'OUTPUT_BASE': Path('../../data/preprocessing/individual_analysis'),  # ğŸ“ output base\n",
    "    \n",
    "    # processing settings\n",
    "    'FRAMES_TO_EXTRACT': 300,  # ğŸ“Š total frames to extract\n",
    "    'SAMPLE_RATE': 15,  # ğŸ“Š extract every Nth frame\n",
    "    \n",
    "    # quality thresholds (from batch analysis)\n",
    "    'QUALITY_THRESHOLD': {\n",
    "        'brightness_min': 104.47,  # ğŸ” minimum brightness\n",
    "        'brightness_max': 114.75,  # ğŸ” maximum brightness  \n",
    "        'blur_min': 3494.35  # ğŸ” minimum blur score\n",
    "    },\n",
    "    \n",
    "    # video settings\n",
    "    'PREFERRED_CODEC': 'mp4v',  # ğŸ¥ primary codec\n",
    "    'FALLBACK_CODECS': ['h264', 'xvid'],  # ğŸ¥ alternatives\n",
    "    'MAX_FRAME_WIDTH': 1920,  # ğŸ¥ max width\n",
    "    'MAX_FRAME_HEIGHT': 1080,  # ğŸ¥ max height\n",
    "    'JPEG_QUALITY': 95  # ğŸ¥ output quality (0-100)\n",
    "}\n",
    "\n",
    "# derived paths\n",
    "date_formatted = f\"{CONFIG['BATCH_DATE'][:4]}-{CONFIG['BATCH_DATE'][4:6]}-{CONFIG['BATCH_DATE'][6:8]}\"\n",
    "CONFIG['OUTPUT_DIR'] = CONFIG['OUTPUT_BASE'] / date_formatted / CONFIG['VIDEO_ID']\n",
    "CONFIG['VIDEO_DIR'] = CONFIG['INPUT_BASE'] / CONFIG['VIDEO_ID'] / date_formatted\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Processing: {CONFIG['VIDEO_ID']} from {date_formatted}\")\n",
    "print(f\"  Output to: {CONFIG['OUTPUT_DIR']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c646e51-3fae-4fd0-bed5-487a444cc447",
   "metadata": {},
   "source": [
    "### Environment Initialization\n",
    "\n",
    "The preprocessing configuration parameters defined above will now be used to initialize the environment, import required libraries, and set up helper functions for video processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bce613-3148-4b7d-beb7-35e99e5a3ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8108366a-28e8-4598-99a5-8f283a9f076c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ OpenCV version: 4.11.0\n",
      "âœ“ Python version: 3.12.9\n",
      "âœ“ NumPy version: 2.2.4\n",
      "âœ“ Pandas version: 2.2.3\n",
      "âœ“ Preferred codec 'mp4v' fourcc: 1983148141\n",
      "\n",
      "âœ“ Environment setup complete\n",
      "  Output directory created: ../../data/preprocessing/individual_analysis/2025-06-20/ATL-1005\n"
     ]
    }
   ],
   "source": [
    "# environment setup\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# verify opencv\n",
    "try:\n",
    "    print(f\"âœ“ OpenCV version: {cv2.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ OpenCV not installed. Install with: pip install opencv-python\")\n",
    "    \n",
    "print(f\"âœ“ Python version: {sys.version.split()[0]}\")\n",
    "print(f\"âœ“ NumPy version: {np.__version__}\")\n",
    "print(f\"âœ“ Pandas version: {pd.__version__}\")\n",
    "\n",
    "# helper functions\n",
    "def calculate_brightness(frame):\n",
    "    \"\"\"Calculate average brightness of frame\"\"\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    return np.mean(gray)\n",
    "\n",
    "def calculate_blur_score(frame):\n",
    "    \"\"\"Calculate Laplacian variance (higher = sharper)\"\"\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    return cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "\n",
    "def get_video_metadata(video_path):\n",
    "    \"\"\"Extract video metadata\"\"\"\n",
    "    metadata = {}\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if cap.isOpened():\n",
    "        metadata['fps'] = cap.get(cv2.CAP_PROP_FPS)\n",
    "        metadata['frame_count'] = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        metadata['width'] = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        metadata['height'] = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        metadata['duration_seconds'] = metadata['frame_count'] / metadata['fps'] if metadata['fps'] > 0 else 0\n",
    "        metadata['codec'] = int(cap.get(cv2.CAP_PROP_FOURCC))\n",
    "        cap.release()\n",
    "    return metadata\n",
    "\n",
    "# create output directory\n",
    "CONFIG['OUTPUT_DIR'].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# verify codec support\n",
    "fourcc_test = cv2.VideoWriter_fourcc(*CONFIG['PREFERRED_CODEC'])\n",
    "print(f\"âœ“ Preferred codec '{CONFIG['PREFERRED_CODEC']}' fourcc: {fourcc_test}\")\n",
    "\n",
    "print(f\"\\nâœ“ Environment setup complete\")\n",
    "print(f\"  Output directory created: {CONFIG['OUTPUT_DIR']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2287d492-558d-4eb7-808f-039d01294068",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "### ğŸ“Š Analysis & Observations\n",
    "\n",
    "**Record your findings from the code execution above:**\n",
    "\n",
    "#### Results\n",
    "*What outputs or data were generated?*\n",
    "\n",
    "#### Observations\n",
    "*What patterns or behaviors did you notice?*\n",
    "\n",
    "#### Notes\n",
    "*Any issues, performance observations, or follow-up needed?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a86a85-9a42-43a6-9524-a78860aab6a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "*End of Environment Setup*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dba3644-cdd9-435d-9bdd-0fdaa83420d8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## ğŸ”„ Progress Tracking & Checkpoint System\n",
    "\n",
    "The following cells implement simple progress tracking and checkpoint functionality to:\n",
    "\n",
    "1. **Track Processing Progress**\n",
    "   - Monitor which video is currently being processed\n",
    "   - Count successful vs failed videos\n",
    "   - Display elapsed time\n",
    "\n",
    "2. **Enable Restart Capability**\n",
    "   - Save progress after each video completes\n",
    "   - Automatically skip already-processed videos on rerun\n",
    "   - Maintain list of failed videos for retry\n",
    "\n",
    "This ensures we don't lose work if the kernel crashes and provides visibility into long-running processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9533f832-2bea-401c-9181-c0cc41e572bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## ğŸ’¾ Initialize Checkpoint and Progress Tracking Functions\n",
    "\n",
    "This module establishes checkpoint and progress tracking capabilities for the preprocessing workflow. The system creates functions for saving and loading processing state, initializes timing and counting variables, recovers from any existing checkpoints, and provides real-time progress monitoring with completion status.\n",
    "\n",
    "**Implemented below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccf61bec-dcc8-4f36-89c5-59e8726db565",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to process videos. Checkpoint system initialized.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize tracking variables\n",
    "CHECKPOINT_FILE = \"preprocessing_checkpoint.json\"\n",
    "start_time = time.time()\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"Load previous progress if it exists\"\"\"\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        with open(CHECKPOINT_FILE, 'r') as f:\n",
    "            checkpoint = json.load(f)\n",
    "            print(f\"âœ“ Loaded checkpoint: {len(checkpoint['processed'])} videos already processed\")\n",
    "            return checkpoint\n",
    "    return {\n",
    "        \"processed\": [], \n",
    "        \"failed\": [], \n",
    "        \"last_completed\": None, \n",
    "        \"start_time\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "def save_checkpoint(checkpoint):\n",
    "    \"\"\"Save current progress\"\"\"\n",
    "    checkpoint['last_updated'] = datetime.now().isoformat()\n",
    "    with open(CHECKPOINT_FILE, 'w') as f:\n",
    "        json.dump(checkpoint, f, indent=2)\n",
    "\n",
    "def log_progress(video_name, status, checkpoint, total_videos):\n",
    "    \"\"\"Log progress and update checkpoint\"\"\"\n",
    "    if status == \"success\":\n",
    "        checkpoint['processed'].append(video_name)\n",
    "    else:\n",
    "        checkpoint['failed'].append(video_name)\n",
    "    \n",
    "    checkpoint['last_completed'] = video_name\n",
    "    save_checkpoint(checkpoint)\n",
    "    \n",
    "    # Display progress\n",
    "    elapsed = time.time() - start_time\n",
    "    processed_count = len(checkpoint['processed'])\n",
    "    failed_count = len(checkpoint['failed'])\n",
    "    \n",
    "    print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] {video_name}: {status}\")\n",
    "    print(f\"Progress: {processed_count}/{total_videos} | Failed: {failed_count} | Elapsed: {elapsed/60:.1f}min\")\n",
    "\n",
    "# Load any existing checkpoint\n",
    "checkpoint = load_checkpoint()\n",
    "print(f\"Ready to process videos. Checkpoint system initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a99890-dc00-47e7-ad4f-111e8514e520",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "### ğŸ“Š Analysis & ObservationS\n",
    "\n",
    "**Record your findings from the code execution above:**\n",
    "\n",
    "#### Results\n",
    "*What outputs or data were generated?*\n",
    "\n",
    "#### Observations\n",
    "*What patterns or behaviors did you notice?*\n",
    "\n",
    "#### Notes\n",
    "*Any issues, performance observations, or follow-up needed?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638ff93b-ff91-4ad6-b1fa-8dadc2e4239e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "*Initialize Checkpoint and Progress Tracking Functions*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470fa738-95f0-4932-b604-080b8d4e8116",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## ğŸ“¹ Video Ingestion & Cataloging\n",
    "\n",
    "This module loads video files from the source directory and extracts technical metadata including resolution, frame rate, duration, and codec specifications. The cataloging process builds a comprehensive data inventory and identifies format variations that may impact downstream processing stages.\n",
    "\n",
    "*The following code cell implements the video ingestion module using FFmpeg and OpenCV for metadata extraction.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa976e32-bb3a-4896-b462-16ae23f24d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: ATL-1005_20250620_120641.mp4\n",
      "  Starts at: 12:06:41\n"
     ]
    }
   ],
   "source": [
    "# video ingestion and cataloging\n",
    "def parse_timestamp(filename):\n",
    "    \"\"\"extract timestamp from filename\"\"\"\n",
    "    parts = filename.stem.split('_')\n",
    "    if len(parts) >= 3:\n",
    "        time_str = parts[2]\n",
    "        hours = int(time_str[:2])\n",
    "        minutes = int(time_str[2:4])\n",
    "        return hours * 60 + minutes  # minutes from midnight\n",
    "    return None\n",
    "\n",
    "# find videos\n",
    "video_files = list(CONFIG['VIDEO_DIR'].glob(f\"{CONFIG['VIDEO_ID']}_*.mp4\"))\n",
    "\n",
    "if not video_files:\n",
    "    raise FileNotFoundError(f\"No videos found for {CONFIG['VIDEO_ID']} on {CONFIG['BATCH_DATE']}\")\n",
    "\n",
    "# find closest to noon\n",
    "target_minutes = CONFIG['TARGET_HOUR'] * 60  # 720 minutes\n",
    "closest_video = None\n",
    "min_diff = float('inf')\n",
    "\n",
    "for video in video_files:\n",
    "    minutes = parse_timestamp(video)\n",
    "    if minutes is not None:\n",
    "        diff = abs(minutes - target_minutes)\n",
    "        if diff < min_diff:\n",
    "            min_diff = diff\n",
    "            closest_video = video\n",
    "\n",
    "CONFIG['selected_video'] = closest_video\n",
    "time_str = closest_video.stem.split('_')[2]\n",
    "print(f\"Selected: {closest_video.name}\")\n",
    "print(f\"  Starts at: {time_str[:2]}:{time_str[2:4]}:{time_str[4:6]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e2ddcf-b619-457c-8234-a8274b19fa72",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "### ğŸ“Š Analysis & Observations\n",
    "\n",
    "**Record your findings from the code execution above:**\n",
    "\n",
    "#### Results\n",
    "*What outputs or data were generated?*\n",
    "\n",
    "#### Observations\n",
    "*What patterns or behaviors did you notice?*\n",
    "\n",
    "#### Notes\n",
    "*Any issues, performance observations, or follow-up needed?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4131dd0-1091-4d35-9b00-2d0cc99177cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "*End of Video Ingestion & Cataloging*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debdeb37-39f5-4e56-93ae-28a4c289767f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## ğŸï¸ Frame Extraction\n",
    "\n",
    "This module samples frames from video sequences at specified temporal intervals. The extraction process converts temporal video data into spatial image representations suitable for computer vision processing and analysis.\n",
    "\n",
    "*The following code cell implements frame extraction using OpenCV with configurable sampling rates and output formats.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d498e8d1-9cb0-416c-b19e-ff0f1c82e0de",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Extraction\n",
      "Extracting 300 frames (every 15 frames)\n",
      "  Extracted 50/300 frames\n",
      "  Extracted 100/300 frames\n",
      "  Extracted 150/300 frames\n",
      "  Extracted 200/300 frames\n",
      "  Extracted 250/300 frames\n",
      "  Extracted 300/300 frames\n",
      "\n",
      "âœ“ Extracted 300 frames to ../../data/preprocessing/individual_analysis/2025-06-20/ATL-1005/frames\n"
     ]
    }
   ],
   "source": [
    "# frame extraction\n",
    "import cv2\n",
    "\n",
    "print(f\"Frame Extraction\")\n",
    "print(f\"Extracting {CONFIG['FRAMES_TO_EXTRACT']} frames (every {CONFIG['SAMPLE_RATE']} frames)\")\n",
    "\n",
    "video_path = CONFIG['selected_video']\n",
    "cap = cv2.VideoCapture(str(video_path))\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise ValueError(f\"Cannot open video: {video_path}\")\n",
    "\n",
    "# create frames directory\n",
    "frames_dir = CONFIG['OUTPUT_DIR'] / 'frames'\n",
    "frames_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# extract frames\n",
    "frames_extracted = 0\n",
    "frame_index = 0\n",
    "\n",
    "while frames_extracted < CONFIG['FRAMES_TO_EXTRACT'] and cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # extract every Nth frame\n",
    "    if frame_index % CONFIG['SAMPLE_RATE'] == 0:\n",
    "        frame_filename = f\"frame_{frames_extracted:04d}.jpg\"\n",
    "        frame_path = frames_dir / frame_filename\n",
    "        \n",
    "        # save frame\n",
    "        cv2.imwrite(str(frame_path), frame, [cv2.IMWRITE_JPEG_QUALITY, CONFIG['JPEG_QUALITY']])\n",
    "        \n",
    "        frames_extracted += 1\n",
    "        if frames_extracted % 50 == 0:\n",
    "            print(f\"  Extracted {frames_extracted}/{CONFIG['FRAMES_TO_EXTRACT']} frames\")\n",
    "    \n",
    "    frame_index += 1\n",
    "\n",
    "cap.release()\n",
    "\n",
    "CONFIG['frames_dir'] = frames_dir\n",
    "CONFIG['frames_extracted'] = frames_extracted\n",
    "\n",
    "print(f\"\\nâœ“ Extracted {frames_extracted} frames to {frames_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7fa3c9-5eeb-42b5-8ce3-1d2f08629c0b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "### ğŸ“Š Analysis & Observations\n",
    "\n",
    "**Record your findings from the code execution above:**\n",
    "\n",
    "#### Results\n",
    "*What outputs or data were generated?*\n",
    "\n",
    "#### Observations\n",
    "*What patterns or behaviors did you notice?*\n",
    "\n",
    "#### Notes\n",
    "*Any issues, performance observations, or follow-up needed?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbc7a1a-18d4-4a72-825a-861e8e8c405a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "*End of Frame Extraction*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc524ea6-eb7c-4032-b0cf-ba8b6869aaaf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## ğŸ” Image Quality Control\n",
    "\n",
    "This module filters out blurry, dark, or corrupted frames using automated quality metrics. The quality control process ensures only processable frames continue through the workflow, optimizing compute resources and improving downstream analysis reliability.\n",
    "\n",
    "\n",
    "*The following code cell implements quality filtering using Laplacian variance for blur detection, histogram analysis for exposure assessment, and file integrity checks.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa5af51-ae05-489a-a738-28c7cce3c330",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e84c4530-0a70-43d8-8cf2-88006a6b0a7c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "### ğŸ“Š Analysis & Observations\n",
    "**Record your findings from the code execution above:**\n",
    "\n",
    "#### Results\n",
    "*What outputs or data were generated?*\n",
    "\n",
    "#### Observations\n",
    "*What patterns or behaviors did you notice?*\n",
    "\n",
    "#### Notes\n",
    "*Any issues, performance observations, or follow-up needed?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cf554d-f7c8-418c-bb6a-9040699fb23f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "*End of Image Quality Control*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25076404-a9b5-4169-8da2-e88bf2268f40",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## ğŸ“ Spatial Transformations\n",
    "\n",
    "This module applies geometric transformations including resize, crop, and padding operations to achieve consistent frame dimensions. The standardization process ensures uniform input sizes for batch processing and meets model requirements for downstream analysis.\n",
    "\n",
    "**ğŸš§ IMPLEMENTATION REQUIRED ğŸš§**\n",
    "\n",
    "*The following code cell implements spatial transformations using OpenCV and PIL with configurable target dimensions and padding strategies.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8675b31-af9f-4c14-b780-8a33aa8675bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0abf14e-b84c-4031-a8fc-88e4c651b98c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "### ğŸ“Š Analysis & Observations\n",
    "**Record your findings from the code execution above:**\n",
    "\n",
    "#### Results\n",
    "*What outputs or data were generated?*\n",
    "\n",
    "#### Observations\n",
    "*What patterns or behaviors did you notice?*\n",
    "\n",
    "#### Notes\n",
    "*Any issues, performance observations, or follow-up needed?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93493f0b-1f8a-4256-a863-f9645372ce57",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "*End of Spatial Transformations*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74a8cfe-eef3-415c-9338-f49bce337a6b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## ğŸ¨ Color Space Normalization\n",
    "\n",
    "This module converts frames to a consistent color space (RGB/BGR) and normalizes pixel values to standardized ranges. The normalization process ensures uniform data representation across different camera sensors and lighting conditions.\n",
    "\n",
    "**ğŸš§ IMPLEMENTATION REQUIRED ğŸš§**\n",
    "\n",
    "*The following code cell implements color space conversion and pixel normalization using OpenCV with configurable target color spaces and normalization ranges.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ea6a9a-e727-482f-a05c-7444a673a84c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "accea736-aaf3-4710-9ff3-892ac6766ecd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "### ğŸ“Š Analysis & Observations\n",
    "\n",
    "**Record your findings from the code execution above:**\n",
    "\n",
    "#### Results\n",
    "*What outputs or data were generated?*\n",
    "\n",
    "#### Observations\n",
    "*What patterns or behaviors did you notice?*\n",
    "\n",
    "#### Notes\n",
    "*Any issues, performance observations, or follow-up needed?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb15063-6cfc-4514-8230-5776f4371f6d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "*End of Color Space Normalization*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1da84f-4635-430d-a9ea-092d08a3f2f8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## â±ï¸ Temporal Downsampling\n",
    "\n",
    "This module selects keyframes or applies temporal windowing techniques to reduce data redundancy. The downsampling process manages data volume while preserving important temporal events and motion patterns for analysis.\n",
    "\n",
    "**ğŸš§ IMPLEMENTATION REQUIRED ğŸš§**\n",
    "\n",
    "*The following code cell implements temporal downsampling using keyframe detection algorithms and configurable windowing strategies with OpenCV and custom temporal analysis functions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e53307-f209-4897-b90e-56421b700ada",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "129648f9-36b8-49ec-b90e-82dbf66a9963",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "### ğŸ“Š Analysis & Observations\n",
    "\n",
    "**Record your findings from the code execution above:**\n",
    "\n",
    "#### Results\n",
    "*What outputs or data were generated?*\n",
    "\n",
    "#### Observations\n",
    "*What patterns or behaviors did you notice?*\n",
    "\n",
    "#### Notes\n",
    "*Any issues, performance observations, or follow-up needed?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86293766-f77a-468a-8d09-77e3d4e18d81",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "*End of Temporal Downsampling*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c5193-17d0-42d8-b21f-cc5fe04bed55",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "# ğŸ“ Data Organization\n",
    "\n",
    "This module structures processed frames with comprehensive metadata linking back to source videos. The organization system maintains full traceability throughout the processing workflow and enables efficient data loading for downstream analysis.\n",
    "\n",
    "**ğŸš§ IMPLEMENTATION REQUIRED ğŸš§**\n",
    "\n",
    "*The following code cell implements data structuring using JSON metadata files and hierarchical directory organization with pandas for efficient data indexing and retrieval.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f8dd7b-2abd-4d1f-bf39-4fbce636156a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d02c98d-7040-420b-b724-81fc0d7e2ecc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "### ğŸ“Š Analysis & Observations\n",
    "\n",
    "**Record your findings from the code execution above:**\n",
    "\n",
    "#### Results\n",
    "*What outputs or data were generated?*\n",
    "\n",
    "#### Observations\n",
    "*What patterns or behaviors did you notice?*\n",
    "\n",
    "#### Notes\n",
    "*Any issues, performance observations, or follow-up needed?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594814c5-c34c-409e-931c-51146bdf0235",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "*End of Data Organization*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7105f31-f8eb-4dbb-9f98-082f9fe78304",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "# ğŸ’¾ Export & Storage\n",
    "\n",
    "This module saves processed frames in optimized formats for efficient storage and retrieval. The export process optimizes I/O performance for training workflows and ensures data accessibility for downstream analysis.\n",
    "\n",
    "**ğŸš§ IMPLEMENTATION REQUIRED ğŸš§**\n",
    "\n",
    "*The following code cell implements data export with compression and batch writing optimizations.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ada715-bb53-4035-9655-671042f4f22f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12ca80aa-1968-4a35-a760-910497aee906",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "### ğŸ“Š Analysis & Observations\n",
    "\n",
    "**Record your findings from the code execution above:**\n",
    "\n",
    "#### Results\n",
    "*What outputs or data were generated?*\n",
    "\n",
    "#### Observations\n",
    "*What patterns or behaviors did you notice?*\n",
    "\n",
    "#### Notes\n",
    "*Any issues, performance observations, or follow-up needed?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd234ea-a33d-4a91-87a5-e27c73e9c750",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "*End of Export & Storage*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f02280-67a5-4613-869e-e937541dd359",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7049047-f055-4555-9a37-5b48a59022be",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
