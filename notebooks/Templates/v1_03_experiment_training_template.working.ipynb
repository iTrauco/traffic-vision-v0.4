{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6651b2d2-abc8-4c14-ae81-cfc14e847ad8",
   "metadata": {},
   "source": [
    "# ðŸš€ Experiment Training - Car Counting v1.03\n",
    "\n",
    "## ðŸŽ¯ Purpose\n",
    "This notebook trains a YOLOv8 model using CVAT-annotated frames for vehicle counting and creates a live counting application.\n",
    "\n",
    "## ðŸ“‹ Context\n",
    "- **Input**: YOLO dataset exported from CVAT\n",
    "- **Model**: YOLOv8n (nano - fastest training)\n",
    "- **Output**: Trained model + live counting script\n",
    "- **Target**: Real-time vehicle counting on traffic video\n",
    "\n",
    "## ðŸ”„ Workflow Overview\n",
    "1. Extract and organize CVAT dataset\n",
    "2. Validate annotations and dataset structure\n",
    "3. Train YOLOv8 model with custom data\n",
    "4. Evaluate model performance\n",
    "5. Create live counting application\n",
    "6. Test on sample video clips\n",
    "\n",
    "## ðŸ“ Expected Input Structure\n",
    "```\n",
    "car_counting_ATL1005_20250620_yolo_dataset.zip\n",
    "â”œâ”€â”€ images/          # Annotated frame images\n",
    "â”œâ”€â”€ labels/          # YOLO format annotations\n",
    "â””â”€â”€ dataset.yaml     # Dataset configuration\n",
    "```\n",
    "\n",
    "## ðŸŽ® Output Applications\n",
    "1. **Live video counter**: Real-time vehicle counting\n",
    "2. **Batch processing**: Count vehicles in recorded videos\n",
    "3. **Model evaluation**: Performance metrics and validation\n",
    "\n",
    "**Version**: 1.03 | **Type**: Model Training | **Target**: Vehicle Counting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edf06ca-a1f2-453b-8a85-09679742dbd6",
   "metadata": {},
   "source": [
    "## ðŸ”§ Training Configuration Parameters\n",
    "\n",
    "This cell defines all parameters for YOLOv8 model training and dataset preparation.\n",
    "\n",
    "### Dataset Parameters\n",
    "- ðŸŽ¯ **DATASET_ZIP**: Path to CVAT exported dataset\n",
    "- ðŸŽ¯ **EXPERIMENT_NAME**: Unique identifier for training run\n",
    "- ðŸ“ **OUTPUT_BASE**: Root directory for training outputs\n",
    "\n",
    "### Training Settings  \n",
    "- ðŸ“Š **MODEL_SIZE**: YOLOv8 variant (n=nano, s=small, m=medium)\n",
    "- ðŸ“Š **EPOCHS**: Training iterations\n",
    "- ðŸ“Š **BATCH_SIZE**: Images per training batch\n",
    "- ðŸ“Š **IMAGE_SIZE**: Input image dimensions\n",
    "\n",
    "### Validation Settings\n",
    "- ðŸ” **TRAIN_SPLIT**: Percentage for training data\n",
    "- ðŸ” **VAL_SPLIT**: Percentage for validation data\n",
    "- ðŸ” **CONFIDENCE_THRESHOLD**: Detection confidence cutoff\n",
    "\n",
    "**Note**: Nano model (yolov8n) is fastest for training and inference on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaef3756-0222-40e2-b555-55f2aa757be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "  Dataset: car_counting_ATL1005_20250620_yolo_dataset.zip\n",
      "  Model: yolov8x\n",
      "  Epochs: 10\n",
      "  Output: ../../data/experiments/car_counting_v1/2025-06-20/ATL-1005/training\n",
      "  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# training configuration parameters\n",
    "from pathlib import Path\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    # dataset parameters\n",
    "    'DATASET_ZIP': '/home/trauco/v3-traffic-vision/data/experiments/car_counting_v1/2025-06-20/ATL-1005/car_counting_ATL1005_20250620_yolo_dataset.zip',\n",
    "    'EXPERIMENT_NAME': 'car_counting_training_v1',\n",
    "    'OUTPUT_BASE': Path('../../data/experiments/car_counting_v1/2025-06-20/ATL-1005'),\n",
    "    \n",
    "    # training settings\n",
    "    'MODEL_SIZE': 'yolov8x',  # ðŸ“Š extra large model (most powerful)\n",
    "    'EPOCHS': 10,  # ðŸ“Š more training iterations\n",
    "    'BATCH_SIZE': 16,  # ðŸ“Š conservative for large model\n",
    "    'IMAGE_SIZE': 640,  # ðŸ“Š input dimensions\n",
    "    \n",
    "    # validation settings\n",
    "    'TRAIN_SPLIT': 0.8,  # ðŸ” 80% for training\n",
    "    'VAL_SPLIT': 0.2,  # ðŸ” 20% for validation\n",
    "    'CONFIDENCE_THRESHOLD': 0.5,  # ðŸ” detection confidence\n",
    "    \n",
    "    # output settings\n",
    "    'SAVE_PERIOD': 10,  # ðŸ“Š save every N epochs\n",
    "    'DEVICE': 'cuda'  # ðŸ”§ use GPU\n",
    "}\n",
    "\n",
    "# derived paths\n",
    "TRAINING_CONFIG['DATASET_DIR'] = TRAINING_CONFIG['OUTPUT_BASE'] / 'dataset'\n",
    "TRAINING_CONFIG['TRAINING_DIR'] = TRAINING_CONFIG['OUTPUT_BASE'] / 'training'\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Dataset: {Path(TRAINING_CONFIG['DATASET_ZIP']).name}\")\n",
    "print(f\"  Model: {TRAINING_CONFIG['MODEL_SIZE']}\")\n",
    "print(f\"  Epochs: {TRAINING_CONFIG['EPOCHS']}\")\n",
    "print(f\"  Output: {TRAINING_CONFIG['TRAINING_DIR']}\")\n",
    "print(f\"  Device: {TRAINING_CONFIG['DEVICE']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b444e46b-2ff7-48c1-a6d5-eecc86edb120",
   "metadata": {},
   "source": [
    "## ðŸ”§ Environment Setup & Dependencies\n",
    "\n",
    "This cell installs YOLOv8 and verifies system requirements for model training.\n",
    "\n",
    "### Installation Process\n",
    "- **Ultralytics**: YOLOv8 training framework\n",
    "- **Dependencies**: PyTorch, OpenCV, matplotlib for training\n",
    "- **System Check**: Verify Python version and hardware capabilities\n",
    "\n",
    "### Hardware Detection\n",
    "- **CPU/GPU**: Automatic device detection\n",
    "- **Memory**: Available RAM for training\n",
    "- **Storage**: Disk space for dataset and models\n",
    "\n",
    "**Note**: Training will proceed on available hardware (CPU fallback if no GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "803675c7-a19a-4274-a124-cedcf98e9a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Setup & Dependencies\n",
      "========================================\n",
      "âœ“ Ultralytics already installed: 8.3.158\n",
      "âœ“ PyTorch: 2.7.1+cu126\n",
      "âœ“ OpenCV: 4.11.0\n",
      "âœ“ Matplotlib: Available\n",
      "âœ“ YAML: Available\n",
      "\n",
      "Hardware Detection:\n",
      "  GPU: NVIDIA RTX A5500\n",
      "  CUDA: 12.6\n",
      "  RAM: 30.8 GB\n",
      "\n",
      "Output directories created:\n",
      "  Dataset: ../../data/experiments/car_counting_v1/2025-06-20/ATL-1005/dataset\n",
      "  Training: ../../data/experiments/car_counting_v1/2025-06-20/ATL-1005/training\n",
      "\n",
      "âœ“ Environment ready for training\n"
     ]
    }
   ],
   "source": [
    "# environment setup and dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Environment Setup & Dependencies\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# install ultralytics\n",
    "try:\n",
    "    import ultralytics\n",
    "    print(f\"âœ“ Ultralytics already installed: {ultralytics.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Installing ultralytics...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ultralytics\"])\n",
    "    import ultralytics\n",
    "    print(f\"âœ“ Ultralytics installed: {ultralytics.__version__}\")\n",
    "\n",
    "# verify additional dependencies\n",
    "try:\n",
    "    import torch\n",
    "    import cv2\n",
    "    import matplotlib.pyplot as plt\n",
    "    import yaml\n",
    "    print(f\"âœ“ PyTorch: {torch.__version__}\")\n",
    "    print(f\"âœ“ OpenCV: {cv2.__version__}\")\n",
    "    print(\"âœ“ Matplotlib: Available\")\n",
    "    print(\"âœ“ YAML: Available\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Missing dependency: {e}\")\n",
    "\n",
    "# hardware detection\n",
    "print(f\"\\nHardware Detection:\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  CUDA: {torch.version.cuda}\")\n",
    "    TRAINING_CONFIG['DEVICE'] = 'cuda'\n",
    "else:\n",
    "    print(\"  GPU: Not available (using CPU)\")\n",
    "    TRAINING_CONFIG['DEVICE'] = 'cpu'\n",
    "\n",
    "# memory check\n",
    "import psutil\n",
    "memory_gb = psutil.virtual_memory().total / (1024**3)\n",
    "print(f\"  RAM: {memory_gb:.1f} GB\")\n",
    "\n",
    "# create output directories\n",
    "TRAINING_CONFIG['DATASET_DIR'].mkdir(parents=True, exist_ok=True)\n",
    "TRAINING_CONFIG['TRAINING_DIR'].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nOutput directories created:\")\n",
    "print(f\"  Dataset: {TRAINING_CONFIG['DATASET_DIR']}\")\n",
    "print(f\"  Training: {TRAINING_CONFIG['TRAINING_DIR']}\")\n",
    "\n",
    "print(f\"\\nâœ“ Environment ready for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d25dd3-f68f-4156-866a-e745de930f01",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Dataset Extraction & Organization\n",
    "\n",
    "This cell extracts the CVAT dataset and organizes it into YOLOv8 training format.\n",
    "\n",
    "### Process\n",
    "- **Extract ZIP**: Unpack CVAT exported dataset\n",
    "- **Validate Structure**: Verify images and labels are present\n",
    "- **Split Dataset**: Organize into train/validation sets\n",
    "- **Create Config**: Generate dataset.yaml for YOLOv8\n",
    "\n",
    "### Expected CVAT Structure\n",
    "```\n",
    "dataset/\n",
    "â”œâ”€â”€ images/     # Frame images from annotation\n",
    "â”œâ”€â”€ labels/     # YOLO format text files\n",
    "â””â”€â”€ classes.txt # Class definitions\n",
    "```\n",
    "\n",
    "**Note**: CVAT exports may vary in structure - script adapts automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26868eb2-b47d-4f8a-ad01-9a6ff1653276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Extraction & Organization\n",
      "===================================\n",
      "Extracting: car_counting_ATL1005_20250620_yolo_dataset.zip\n",
      "Found 50 images\n",
      "Found 51 label files\n",
      "\n",
      "Dataset organized:\n",
      "  Training: 40 images\n",
      "  Validation: 10 images\n",
      "  Config: ../../data/experiments/car_counting_v1/2025-06-20/ATL-1005/dataset/dataset.yaml\n",
      "\n",
      "âœ“ Dataset ready for training\n"
     ]
    }
   ],
   "source": [
    "# dataset extraction and organization\n",
    "import zipfile\n",
    "import shutil\n",
    "import yaml\n",
    "import random\n",
    "\n",
    "# install sklearn if missing\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError:\n",
    "    print(\"Installing scikit-learn...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"scikit-learn\"])\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Dataset Extraction & Organization\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# clear and recreate dataset directory\n",
    "if TRAINING_CONFIG['DATASET_DIR'].exists():\n",
    "    shutil.rmtree(TRAINING_CONFIG['DATASET_DIR'])\n",
    "TRAINING_CONFIG['DATASET_DIR'].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# extract dataset\n",
    "dataset_zip = Path(TRAINING_CONFIG['DATASET_ZIP'])\n",
    "if not dataset_zip.exists():\n",
    "    raise FileNotFoundError(f\"Dataset not found: {dataset_zip}\")\n",
    "\n",
    "print(f\"Extracting: {dataset_zip.name}\")\n",
    "with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:\n",
    "    zip_ref.extractall(TRAINING_CONFIG['DATASET_DIR'])\n",
    "\n",
    "# find extracted content\n",
    "extracted_files = list(TRAINING_CONFIG['DATASET_DIR'].rglob('*'))\n",
    "image_files = [f for f in extracted_files if f.suffix.lower() in ['.jpg', '.jpeg', '.png']]\n",
    "label_files = [f for f in extracted_files if f.suffix == '.txt' and 'classes' not in f.name.lower()]\n",
    "\n",
    "print(f\"Found {len(image_files)} images\")\n",
    "print(f\"Found {len(label_files)} label files\")\n",
    "\n",
    "# organize into train/val structure\n",
    "train_img_dir = TRAINING_CONFIG['DATASET_DIR'] / 'train' / 'images'\n",
    "train_lbl_dir = TRAINING_CONFIG['DATASET_DIR'] / 'train' / 'labels'\n",
    "val_img_dir = TRAINING_CONFIG['DATASET_DIR'] / 'val' / 'images'\n",
    "val_lbl_dir = TRAINING_CONFIG['DATASET_DIR'] / 'val' / 'labels'\n",
    "\n",
    "for dir_path in [train_img_dir, train_lbl_dir, val_img_dir, val_lbl_dir]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# split dataset\n",
    "image_basenames = [f.stem for f in image_files]\n",
    "train_names, val_names = train_test_split(\n",
    "    image_basenames, \n",
    "    train_size=TRAINING_CONFIG['TRAIN_SPLIT'], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# copy files to train/val directories\n",
    "for img_file in image_files:\n",
    "    if img_file.exists():  # check file exists\n",
    "        if img_file.stem in train_names:\n",
    "            shutil.copy2(img_file, train_img_dir / img_file.name)\n",
    "        else:\n",
    "            shutil.copy2(img_file, val_img_dir / img_file.name)\n",
    "\n",
    "for lbl_file in label_files:\n",
    "    if lbl_file.exists():  # check file exists\n",
    "        if lbl_file.stem in train_names:\n",
    "            shutil.copy2(lbl_file, train_lbl_dir / lbl_file.name)\n",
    "        else:\n",
    "            shutil.copy2(lbl_file, val_lbl_dir / lbl_file.name)\n",
    "\n",
    "# create dataset.yaml\n",
    "dataset_config = {\n",
    "    'path': str(TRAINING_CONFIG['DATASET_DIR']),\n",
    "    'train': 'train/images',\n",
    "    'val': 'val/images',\n",
    "    'nc': 1,  # number of classes\n",
    "    'names': ['vehicle']  # class names\n",
    "}\n",
    "\n",
    "yaml_path = TRAINING_CONFIG['DATASET_DIR'] / 'dataset.yaml'\n",
    "with open(yaml_path, 'w') as f:\n",
    "    yaml.dump(dataset_config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"\\nDataset organized:\")\n",
    "print(f\"  Training: {len(train_names)} images\")\n",
    "print(f\"  Validation: {len(val_names)} images\")\n",
    "print(f\"  Config: {yaml_path}\")\n",
    "\n",
    "TRAINING_CONFIG['DATASET_YAML'] = yaml_path\n",
    "\n",
    "print(f\"\\nâœ“ Dataset ready for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692f7b79-cc5b-49a5-a193-809d68b017ac",
   "metadata": {},
   "source": [
    "## ðŸš€ Model Training\n",
    "\n",
    "This cell trains the YOLOv8 model on the annotated vehicle dataset.\n",
    "\n",
    "### Training Process\n",
    "- **Model Initialization**: Load pre-trained YOLOv8n weights\n",
    "- **Custom Training**: Fine-tune on vehicle detection task\n",
    "- **Progress Monitoring**: Track loss and validation metrics\n",
    "- **Model Saving**: Automatic best model checkpointing\n",
    "\n",
    "### Training Outputs\n",
    "- **Weights**: best.pt (best model) and last.pt (final epoch)\n",
    "- **Metrics**: Training/validation loss curves\n",
    "- **Predictions**: Sample detection results\n",
    "- **Logs**: Training progress and statistics\n",
    "\n",
    "### Expected Duration\n",
    "- **CPU**: 10-30 minutes for 50 epochs\n",
    "- **GPU**: 2-5 minutes for 50 epochs\n",
    "\n",
    "**Note**: Training progress will be displayed with real-time metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac48e67f-71d3-40ce-ae39-ff631455d7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training Complete!\n",
      "====================\n",
      "ðŸ† Model: best.pt\n",
      "ðŸ“ Results: ../../data/experiments/car_counting_v1/2025-06-20/ATL-1005/training/vehicle_detection\n",
      "â±ï¸  Total time: 0.5 minutes\n",
      "ðŸŽ¯ Final Precision: 0.140\n",
      "ðŸ“‹ Final Recall: 0.571\n",
      "ðŸ“Š Final mAP50: 0.595\n",
      "\n",
      "ðŸŽ¯ Ready for vehicle counting!\n"
     ]
    }
   ],
   "source": [
    "# model training with live status\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "print(\"Model Training\")\n",
    "print(\"=\" * 15)\n",
    "\n",
    "# initialize model\n",
    "model = YOLO(f\"{TRAINING_CONFIG['MODEL_SIZE']}.pt\")\n",
    "print(f\"âœ“ Loaded {TRAINING_CONFIG['MODEL_SIZE']} model\")\n",
    "\n",
    "# training configuration display\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Model: {TRAINING_CONFIG['MODEL_SIZE']}\")\n",
    "print(f\"  Epochs: {TRAINING_CONFIG['EPOCHS']}\")\n",
    "print(f\"  Device: {TRAINING_CONFIG['DEVICE']}\")\n",
    "print(f\"  Batch size: {TRAINING_CONFIG['BATCH_SIZE']}\")\n",
    "print(f\"  Dataset: {TRAINING_CONFIG['DATASET_YAML'].name}\")\n",
    "\n",
    "print(f\"\\nðŸš€ Starting training...\")\n",
    "time.sleep(2)\n",
    "\n",
    "# create custom callback for progress tracking\n",
    "class TrainingCallback:\n",
    "    def __init__(self, total_epochs):\n",
    "        self.total_epochs = total_epochs\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    def on_train_epoch_end(self, trainer):\n",
    "        epoch = trainer.epoch + 1\n",
    "        elapsed = time.time() - self.start_time\n",
    "        eta = (elapsed / epoch) * (self.total_epochs - epoch)\n",
    "        \n",
    "        # get metrics\n",
    "        metrics = trainer.metrics\n",
    "        loss = getattr(trainer, 'loss', None)\n",
    "        \n",
    "        # clear and update display\n",
    "        clear_output(wait=True)\n",
    "        print(\"Model Training - Live Status\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"ðŸ“Š Progress: Epoch {epoch}/{self.total_epochs} ({epoch/self.total_epochs*100:.1f}%)\")\n",
    "        print(f\"â±ï¸  Elapsed: {elapsed/60:.1f} min | ETA: {eta/60:.1f} min\")\n",
    "        \n",
    "        if hasattr(trainer, 'loss') and trainer.loss is not None:\n",
    "            print(f\"ðŸ“‰ Training Loss: {trainer.loss:.4f}\")\n",
    "        \n",
    "        if metrics:\n",
    "            if 'fitness' in metrics:\n",
    "                print(f\"ðŸŽ¯ Fitness: {metrics['fitness']:.4f}\")\n",
    "            if 'precision' in metrics:\n",
    "                print(f\"ðŸ” Precision: {metrics['precision']:.3f}\")\n",
    "            if 'recall' in metrics:\n",
    "                print(f\"ðŸ“‹ Recall: {metrics['recall']:.3f}\")\n",
    "        \n",
    "        # progress bar\n",
    "        progress = \"â–ˆ\" * int(30 * epoch / self.total_epochs)\n",
    "        remaining = \"â–‘\" * (30 - len(progress))\n",
    "        print(f\"[{progress}{remaining}] {epoch}/{self.total_epochs}\")\n",
    "\n",
    "# add callback\n",
    "callback = TrainingCallback(TRAINING_CONFIG['EPOCHS'])\n",
    "\n",
    "results = model.train(\n",
    "    data=str(TRAINING_CONFIG['DATASET_YAML']),\n",
    "    epochs=TRAINING_CONFIG['EPOCHS'],\n",
    "    batch=TRAINING_CONFIG['BATCH_SIZE'],\n",
    "    imgsz=TRAINING_CONFIG['IMAGE_SIZE'],\n",
    "    device=TRAINING_CONFIG['DEVICE'],\n",
    "    project=str(TRAINING_CONFIG['TRAINING_DIR']),\n",
    "    name='vehicle_detection',\n",
    "    save_period=TRAINING_CONFIG['SAVE_PERIOD'],\n",
    "    plots=True,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# final results\n",
    "clear_output(wait=True)\n",
    "training_output = TRAINING_CONFIG['TRAINING_DIR'] / 'vehicle_detection'\n",
    "best_model = training_output / 'weights' / 'best.pt'\n",
    "\n",
    "TRAINING_CONFIG['BEST_MODEL'] = best_model\n",
    "\n",
    "print(\"âœ… Training Complete!\")\n",
    "print(\"=\" * 20)\n",
    "print(f\"ðŸ† Model: {best_model.name}\")\n",
    "print(f\"ðŸ“ Results: {training_output}\")\n",
    "print(f\"â±ï¸  Total time: {(time.time() - callback.start_time)/60:.1f} minutes\")\n",
    "\n",
    "# show final metrics\n",
    "if hasattr(results, 'results_dict'):\n",
    "    metrics = results.results_dict\n",
    "    if 'metrics/precision(B)' in metrics:\n",
    "        print(f\"ðŸŽ¯ Final Precision: {metrics['metrics/precision(B)']:.3f}\")\n",
    "    if 'metrics/recall(B)' in metrics:\n",
    "        print(f\"ðŸ“‹ Final Recall: {metrics['metrics/recall(B)']:.3f}\")\n",
    "    if 'metrics/mAP50(B)' in metrics:\n",
    "        print(f\"ðŸ“Š Final mAP50: {metrics['metrics/mAP50(B)']:.3f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Ready for vehicle counting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff453e8-781e-47fb-9580-578dbe9adb61",
   "metadata": {},
   "source": [
    "## ðŸ“Š Training Results Diagnostic\n",
    "\n",
    "This cell analyzes the completed training to verify the model learned to detect vehicles properly.\n",
    "\n",
    "### What We're Checking\n",
    "- **Precision**: How accurate are the detections? (0.0 = terrible, 1.0 = perfect)\n",
    "- **Recall**: How many vehicles does it find? (0.0 = finds nothing, 1.0 = finds all)\n",
    "- **mAP50**: Overall detection quality at 50% confidence threshold\n",
    "\n",
    "### Expected Results\n",
    "- **Good training**: Precision > 0.5, Recall > 0.5, mAP50 > 0.5\n",
    "- **Poor training**: Values near 0.0 indicate bad annotations or insufficient data\n",
    "- **Failed training**: All metrics < 0.1 means model learned nothing\n",
    "\n",
    "### Troubleshooting\n",
    "If metrics are poor, likely causes:\n",
    "- Empty or incorrect annotation files from CVAT\n",
    "- Mismatch between training images and video content\n",
    "- Insufficient training data (need more annotated frames)\n",
    "\n",
    "**Note**: This diagnostic helps identify if detection issues are from training problems vs. inference problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "933aa3ea-93a1-446a-ba9a-5be8817cff95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training metrics:\n",
      "Precision: 0.698\n",
      "Recall: 0.857\n",
      "mAP50: 0.855\n",
      "âœ… Model learned something\n"
     ]
    }
   ],
   "source": [
    "# Check training results\n",
    "results_dir = TRAINING_CONFIG['OUTPUT_BASE'] / 'training' / 'vehicle_detection'\n",
    "results_file = results_dir / 'results.csv'\n",
    "\n",
    "if results_file.exists():\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(results_file)\n",
    "    print(\"Final training metrics:\")\n",
    "    print(f\"Precision: {df['metrics/precision(B)'].iloc[-1]:.3f}\")\n",
    "    print(f\"Recall: {df['metrics/recall(B)'].iloc[-1]:.3f}\")\n",
    "    print(f\"mAP50: {df['metrics/mAP50(B)'].iloc[-1]:.3f}\")\n",
    "    \n",
    "    if df['metrics/precision(B)'].iloc[-1] < 0.1:\n",
    "        print(\"âŒ Model didn't learn - training failed!\")\n",
    "    else:\n",
    "        print(\"âœ… Model learned something\")\n",
    "else:\n",
    "    print(\"âŒ No results file found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a6ed16-71a0-41cc-83f1-989f49d554e6",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Annotation Data Validation\n",
    "\n",
    "This cell inspects the actual annotation files exported from CVAT to verify they contain valid vehicle bounding box data.\n",
    "\n",
    "### What We're Checking\n",
    "- **File existence**: Are label files present for each image?\n",
    "- **File content**: Do files contain YOLO format annotations?\n",
    "- **Data format**: Proper YOLO format: `class_id x_center y_center width height`\n",
    "\n",
    "### YOLO Format Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b2de7a9-62df-4714-8d4e-7fec91ac0c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40 label files\n",
      "\n",
      "Label file 1: frame_0039.txt\n",
      "âŒ EMPTY FILE!\n",
      "\n",
      "Label file 2: frame_0003.txt\n",
      "âŒ EMPTY FILE!\n",
      "\n",
      "Label file 3: frame_0004.txt\n",
      "âŒ EMPTY FILE!\n"
     ]
    }
   ],
   "source": [
    "# Check annotation files from CVAT export\n",
    "label_dir = TRAINING_CONFIG['DATASET_DIR'] / 'train' / 'labels'\n",
    "label_files = list(label_dir.glob('*.txt'))\n",
    "print(f\"Found {len(label_files)} label files\")\n",
    "\n",
    "# Check a few labels\n",
    "for i, label_file in enumerate(label_files[:3]):\n",
    "    print(f\"\\nLabel file {i+1}: {label_file.name}\")\n",
    "    with open(label_file, 'r') as f:\n",
    "        content = f.read().strip()\n",
    "        if content:\n",
    "            print(f\"Content: {content}\")\n",
    "        else:\n",
    "            print(\"âŒ EMPTY FILE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67319cd-5c9f-4c60-8b06-548926feea01",
   "metadata": {},
   "source": [
    "## ðŸŽ¬ Live Counting Application\n",
    "\n",
    "This cell creates a real-time vehicle counting application using the trained model.\n",
    "\n",
    "### Application Features\n",
    "- **Video Playback**: Load and play traffic intersection video\n",
    "- **Real-time Detection**: Apply trained model to each frame\n",
    "- **Live Counter**: Display current vehicle count\n",
    "- **Bounding Boxes**: Visual detection indicators\n",
    "- **Performance Metrics**: FPS and inference time\n",
    "\n",
    "### Controls\n",
    "- **Space**: Pause/resume video\n",
    "- **Q**: Quit application\n",
    "- **S**: Save current frame with detections\n",
    "- **R**: Reset counter\n",
    "\n",
    "### Video Input Options\n",
    "- **Original source video**: Same intersection as training data\n",
    "- **Custom video file**: Any traffic video\n",
    "- **Webcam feed**: Real-time camera input (device 0)\n",
    "\n",
    "**Note**: Application runs in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dafde51-efa6-481e-b661-1deb48c86363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Live Counting Application\n",
      "=========================\n",
      "Model path: ../../data/experiments/car_counting_v1/2025-06-20/ATL-1005/training/vehicle_detection/weights/best.pt\n",
      "âœ“ Model loaded\n",
      "Looking in: /home/trauco/traffic-recordings/ATL-1005/2025-06-20\n",
      "Found video: /home/trauco/traffic-recordings/ATL-1005/2025-06-20/ATL-1005_20250620_184919.mp4\n",
      "âœ“ Video loaded: /home/trauco/traffic-recordings/ATL-1005/2025-06-20/ATL-1005_20250620_184919.mp4\n",
      "\n",
      "0: 384x640 (no detections), 42.8ms\n",
      "Speed: 1.6ms preprocess, 42.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.6ms\n",
      "Speed: 1.2ms preprocess, 3.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.1ms\n",
      "Speed: 1.5ms preprocess, 3.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.7ms\n",
      "Speed: 1.4ms preprocess, 2.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.9ms\n",
      "Speed: 1.8ms preprocess, 2.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.7ms\n",
      "Speed: 1.0ms preprocess, 2.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.9ms\n",
      "Speed: 1.3ms preprocess, 2.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.5ms\n",
      "Speed: 1.4ms preprocess, 3.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.8ms\n",
      "Speed: 3.0ms preprocess, 3.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.2ms\n",
      "Speed: 2.8ms preprocess, 3.2ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.6ms\n",
      "Speed: 2.2ms preprocess, 2.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 vehicle, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 vehicle, 3.2ms\n",
      "Speed: 1.1ms preprocess, 3.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.3ms\n",
      "Speed: 1.3ms preprocess, 4.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 3.4ms preprocess, 4.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.1ms\n",
      "Speed: 2.6ms preprocess, 3.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.3ms\n",
      "Speed: 1.6ms preprocess, 3.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.6ms\n",
      "Speed: 1.8ms preprocess, 2.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.0ms\n",
      "Speed: 1.8ms preprocess, 3.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.9ms\n",
      "Speed: 3.1ms preprocess, 2.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.0ms\n",
      "Speed: 2.1ms preprocess, 3.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.2ms\n",
      "Speed: 1.7ms preprocess, 3.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.3ms\n",
      "Speed: 1.7ms preprocess, 3.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.9ms\n",
      "Speed: 1.6ms preprocess, 3.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.3ms\n",
      "Speed: 2.2ms preprocess, 3.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.0ms\n",
      "Speed: 3.1ms preprocess, 3.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.2ms\n",
      "Speed: 2.5ms preprocess, 3.2ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.6ms\n",
      "Speed: 1.7ms preprocess, 3.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.2ms\n",
      "Speed: 2.0ms preprocess, 3.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.6ms\n",
      "Speed: 1.6ms preprocess, 3.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.0ms\n",
      "Speed: 2.4ms preprocess, 3.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.2ms\n",
      "Speed: 1.4ms preprocess, 3.2ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.0ms\n",
      "Speed: 3.1ms preprocess, 3.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.7ms\n",
      "Speed: 1.5ms preprocess, 3.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.3ms\n",
      "Speed: 2.2ms preprocess, 3.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.1ms\n",
      "Speed: 2.6ms preprocess, 3.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.7ms\n",
      "Speed: 1.5ms preprocess, 3.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.2ms\n",
      "Speed: 2.2ms preprocess, 3.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.5ms\n",
      "Speed: 1.3ms preprocess, 3.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.3ms\n",
      "Speed: 1.3ms preprocess, 3.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.1ms\n",
      "Speed: 1.2ms preprocess, 3.1ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.9ms\n",
      "Speed: 1.3ms preprocess, 2.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.9ms\n",
      "Speed: 1.9ms preprocess, 2.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.3ms\n",
      "Speed: 3.1ms preprocess, 4.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.0ms\n",
      "Speed: 1.8ms preprocess, 3.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.8ms\n",
      "Speed: 3.1ms preprocess, 3.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.0ms\n",
      "Speed: 2.0ms preprocess, 3.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.4ms\n",
      "Speed: 1.4ms preprocess, 3.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.5ms\n",
      "Speed: 1.5ms preprocess, 3.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.0ms\n",
      "Speed: 2.9ms preprocess, 3.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.1ms\n",
      "Speed: 2.2ms preprocess, 3.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.5ms\n",
      "Speed: 1.3ms preprocess, 3.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.8ms\n",
      "Speed: 1.5ms preprocess, 3.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.1ms\n",
      "Speed: 3.1ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.6ms\n",
      "Speed: 1.2ms preprocess, 3.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.2ms\n",
      "Speed: 1.5ms preprocess, 3.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.7ms\n",
      "Speed: 2.0ms preprocess, 3.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.2ms\n",
      "Speed: 1.3ms preprocess, 3.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.2ms\n",
      "Speed: 1.9ms preprocess, 3.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.7ms\n",
      "Speed: 2.7ms preprocess, 2.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.4ms\n",
      "Speed: 1.4ms preprocess, 3.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.7ms\n",
      "Speed: 2.4ms preprocess, 2.7ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.2ms\n",
      "Speed: 2.5ms preprocess, 3.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.7ms\n",
      "Speed: 1.8ms preprocess, 2.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.3ms\n",
      "Speed: 1.3ms preprocess, 3.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.1ms\n",
      "Speed: 1.7ms preprocess, 3.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.2ms\n",
      "Speed: 1.6ms preprocess, 3.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.9ms\n",
      "Speed: 2.1ms preprocess, 3.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.1ms\n",
      "Speed: 1.6ms preprocess, 3.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.3ms\n",
      "Speed: 1.5ms preprocess, 3.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.2ms\n",
      "Speed: 1.8ms preprocess, 3.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.3ms\n",
      "Speed: 1.7ms preprocess, 3.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.0ms\n",
      "Speed: 2.6ms preprocess, 3.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.1ms\n",
      "Speed: 1.2ms preprocess, 3.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.2ms\n",
      "Speed: 1.1ms preprocess, 3.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.1ms\n",
      "Speed: 1.3ms preprocess, 3.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.1ms\n",
      "Speed: 1.2ms preprocess, 3.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.1ms\n",
      "Speed: 1.8ms preprocess, 3.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.1ms\n",
      "Speed: 2.2ms preprocess, 3.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.1ms\n",
      "Speed: 1.8ms preprocess, 3.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.3ms\n",
      "Speed: 2.0ms preprocess, 3.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.4ms\n",
      "Speed: 1.2ms preprocess, 3.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.1ms\n",
      "Speed: 2.2ms preprocess, 3.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.1ms\n",
      "Speed: 3.0ms preprocess, 3.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.2ms\n",
      "Speed: 2.1ms preprocess, 3.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.4ms\n",
      "Speed: 1.1ms preprocess, 3.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.6ms\n",
      "Speed: 1.2ms preprocess, 3.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.3ms\n",
      "Speed: 3.2ms preprocess, 3.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.5ms\n",
      "Speed: 2.2ms preprocess, 3.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.2ms\n",
      "Speed: 1.6ms preprocess, 3.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.3ms\n",
      "Speed: 1.3ms preprocess, 3.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.5ms\n",
      "Speed: 1.7ms preprocess, 3.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.2ms\n",
      "Speed: 2.4ms preprocess, 3.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.9ms\n",
      "Speed: 3.2ms preprocess, 3.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.4ms\n",
      "Speed: 2.1ms preprocess, 3.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.0ms\n",
      "Speed: 1.7ms preprocess, 4.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.1ms\n",
      "Speed: 1.9ms preprocess, 3.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.1ms\n",
      "Speed: 2.1ms preprocess, 3.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.4ms\n",
      "Speed: 3.0ms preprocess, 3.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.9ms\n",
      "Speed: 2.8ms preprocess, 2.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.9ms\n",
      "Speed: 1.1ms preprocess, 2.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.5ms\n",
      "Speed: 1.5ms preprocess, 3.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.9ms\n",
      "Speed: 1.0ms preprocess, 2.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.3ms\n",
      "Speed: 1.6ms preprocess, 3.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.9ms\n",
      "Speed: 2.4ms preprocess, 2.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.4ms\n",
      "Speed: 2.0ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.2ms\n",
      "Speed: 1.6ms preprocess, 4.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.5ms\n",
      "Speed: 1.7ms preprocess, 4.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.9ms\n",
      "Speed: 2.9ms preprocess, 2.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.9ms\n",
      "Speed: 3.1ms preprocess, 2.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.0ms\n",
      "Speed: 1.8ms preprocess, 3.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.3ms\n",
      "Speed: 1.3ms preprocess, 3.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 vehicle, 3.4ms\n",
      "Speed: 1.2ms preprocess, 3.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.3ms\n",
      "Speed: 1.1ms preprocess, 3.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.4ms\n",
      "Speed: 1.1ms preprocess, 3.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.3ms\n",
      "Speed: 1.1ms preprocess, 3.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.3ms\n",
      "Speed: 1.5ms preprocess, 3.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.5ms\n",
      "Speed: 1.5ms preprocess, 3.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.5ms\n",
      "Speed: 1.1ms preprocess, 3.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.4ms\n",
      "Speed: 1.4ms preprocess, 3.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.3ms\n",
      "Speed: 1.3ms preprocess, 3.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.3ms\n",
      "Speed: 1.2ms preprocess, 3.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.1ms\n",
      "Speed: 1.3ms preprocess, 4.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.3ms\n",
      "Speed: 1.4ms preprocess, 4.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.5ms\n",
      "Speed: 1.2ms preprocess, 4.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.4ms\n",
      "Speed: 1.1ms preprocess, 3.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.2ms\n",
      "Speed: 1.1ms preprocess, 3.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.5ms\n",
      "Speed: 1.7ms preprocess, 3.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.5ms\n",
      "Speed: 1.3ms preprocess, 3.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.1ms\n",
      "Speed: 2.5ms preprocess, 3.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.0ms\n",
      "Speed: 1.9ms preprocess, 3.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.0ms\n",
      "Speed: 2.1ms preprocess, 3.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.1ms\n",
      "Speed: 2.3ms preprocess, 3.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.5ms\n",
      "Speed: 1.3ms preprocess, 3.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.1ms\n",
      "Speed: 2.2ms preprocess, 3.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.5ms\n",
      "Speed: 1.4ms preprocess, 3.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.0ms\n",
      "Speed: 1.9ms preprocess, 3.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.1ms\n",
      "Speed: 2.1ms preprocess, 3.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.0ms\n",
      "Speed: 2.4ms preprocess, 3.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.7ms\n",
      "Speed: 2.6ms preprocess, 3.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.0ms\n",
      "Speed: 2.4ms preprocess, 3.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.1ms\n",
      "Speed: 2.6ms preprocess, 3.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.5ms\n",
      "Speed: 1.3ms preprocess, 3.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.2ms\n",
      "Speed: 1.7ms preprocess, 3.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.5ms\n",
      "Speed: 1.3ms preprocess, 3.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.1ms\n",
      "Speed: 1.4ms preprocess, 3.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.3ms\n",
      "Speed: 2.1ms preprocess, 3.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "# live counting application (simplified)\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Live Counting Application\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# load model (set path directly)\n",
    "model_path = TRAINING_CONFIG['OUTPUT_BASE'] / 'training' / 'vehicle_detection' / 'weights' / 'best.pt'\n",
    "print(f\"Model path: {model_path}\")\n",
    "\n",
    "if not model_path.exists():\n",
    "    raise FileNotFoundError(f\"Model not found: {model_path}\")\n",
    "\n",
    "model = YOLO(str(model_path))\n",
    "print(f\"âœ“ Model loaded\")\n",
    "\n",
    "# find video file\n",
    "video_dir = Path.home() / \"traffic-recordings\" / \"ATL-1005\" / \"2025-06-20\"\n",
    "print(f\"Looking in: {video_dir}\")\n",
    "\n",
    "if video_dir.exists():\n",
    "    video_files = list(video_dir.glob(\"ATL-1005_20250620_*.mp4\"))\n",
    "    if video_files:\n",
    "        VIDEO_PATH = str(video_files[0])  # use first video found\n",
    "        print(f\"Found video: {VIDEO_PATH}\")\n",
    "    else:\n",
    "        print(\"No video files found in directory\")\n",
    "        VIDEO_PATH = 0\n",
    "else:\n",
    "    print(f\"Directory doesn't exist: {video_dir}\")\n",
    "    VIDEO_PATH = 0\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "if not cap.isOpened():\n",
    "    print(f\"âŒ Can't open video: {VIDEO_PATH}\")\n",
    "    print(\"Please check the file path or use webcam with VIDEO_PATH = 0\")\n",
    "else:\n",
    "    print(f\"âœ“ Video loaded: {VIDEO_PATH}\")\n",
    "    \n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # detect vehicles\n",
    "        results = model(frame, conf=0.5)\n",
    "        count = len(results[0].boxes) if results[0].boxes is not None else 0\n",
    "        \n",
    "        # draw results\n",
    "        annotated = results[0].plot()\n",
    "        \n",
    "        # add count\n",
    "        cv2.putText(annotated, f'Vehicles: {count}', (50, 50), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 3)\n",
    "        \n",
    "        # show frame\n",
    "        cv2.imshow('Vehicle Count', annotated)\n",
    "        \n",
    "        # proper timing for video playback\n",
    "        if cv2.waitKey(33) & 0xFF == ord('q'):  # ~30 FPS\n",
    "            break\n",
    "            \n",
    "        frame_count += 1\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"Processed {frame_count} frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cec84ba-e04a-4565-8c8d-149b45d1e004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04b858b-2db6-42a2-8b2c-b33d6b1e5ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710c3909-ddd7-42b6-aacd-06cdbac2f46e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
