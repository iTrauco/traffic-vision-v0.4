{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "713d5330-48b7-4e7c-88f9-8495d758e49d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83d\udcd3 Notebook Manager\n",
    "\n",
    "This cell initializes the widgets required for managing your research notebook. Please run the cell below to enable functionality for:\n",
    "- Exporting cells tagged with `export` into a `clean` notebook\n",
    "- Generating a dynamic Table of Contents (TOC)\n",
    "- Exporting the notebook to GitHub-compatible Markdown\n",
    "\n",
    "\u27a1\ufe0f **Be sure to execute the next cell before continuing with any editing or exporting.**"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efa06b1-c256-4aa8-bfe9-74662431d2f0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell 1 - Workflow Tools\n",
    "import sys\n",
    "sys.path.insert(0, '../../lib')\n",
    "sys.path.insert(0, '../../scripts') \n",
    "\n",
    "from notebook_tools import TOCWidget, ExportWidget\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "# Create widget instances\n",
    "toc = TOCWidget()\n",
    "export = ExportWidget()\n",
    "\n",
    "# Create horizontal layout\n",
    "left_side = widgets.VBox([toc.button, export.button, toc.status])\n",
    "right_side = widgets.VBox([toc.output, export.output])\n",
    "\n",
    "# Display side by side\n",
    "display(widgets.HBox([left_side, right_side]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184eb830-4d98-4a09-b868-fc630de227f2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "# \ud83d\ude97 Experiment Preprocessing - Car Counting v2.01\n",
    "\n",
    "## \ud83c\udfaf Purpose\n",
    "This notebook implements a simplified preprocessing workflow for car counting experiments. It extracts frames from GDOT traffic camera videos for manual annotation and YOLO model training.\n",
    "\n",
    "## \ud83d\udccb Context\n",
    "- **Data Source**: GDOT traffic camera recordings\n",
    "- **Video Specs**: 480p resolution, 15 fps, ~15 minute duration\n",
    "- **Goal**: Extract frames for vehicle counting model training\n",
    "\n",
    "## \ud83d\udd04 Workflow Overview\n",
    "1. Video selection (closest to noon from target date)\n",
    "2. Frame extraction (evenly spaced samples)\n",
    "3. Quality filtering (brightness/blur)\n",
    "4. Color normalization (BGR\u2192RGB)\n",
    "5. Data organization for annotation\n",
    "\n",
    "## \ud83d\udcc1 Output Structure\n",
    "```\n",
    "data/experiments/[experiment_name]/[date]/[camera]/\n",
    "\u251c\u2500\u2500 frames/           # raw extracted frames\n",
    "\u251c\u2500\u2500 quality/          # quality-filtered frames  \n",
    "\u251c\u2500\u2500 normalized/       # RGB-normalized frames\n",
    "\u2514\u2500\u2500 metadata.json     # processing summary\n",
    "```\n",
    "\n",
    "## \ud83c\udfae Next Steps\n",
    "1. Run preprocessing \u2192 manual annotation in CVAT \u2192 YOLO training\n",
    "2. Start with simplest YOLO model for vehicle counting\n",
    "\n",
    "**Version**: 2.01 | **Type**: Experiment Preprocessing | **Target**: Car Counting"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "85875eff-6875-4b42-a0b0-7c05bbbf73ac",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83d\udd27 Experiment Configuration Parameters\n",
    "\n",
    "This cell defines all parameters for the experiment preprocessing workflow. Parameters are organized into categories with emoji indicators:\n",
    "\n",
    "### Target Parameters  \n",
    "- \ud83c\udfaf **VIDEO_ID**: Specific camera to process (e.g., ATL-1005)\n",
    "- \ud83c\udfaf **BATCH_DATE**: Date from batch analysis (YYYYMMDD format)  \n",
    "- \ud83c\udfaf **TARGET_HOUR**: Target hour for video selection (12 = noon)\n",
    "- \ud83c\udfaf **EXPERIMENT_NAME**: Unique identifier for this experiment\n",
    "\n",
    "### Path Configuration\n",
    "- \ud83d\udcc1 **INPUT_BASE**: Root directory for video recordings\n",
    "- \ud83d\udcc1 **OUTPUT_BASE**: Root directory for experiment output\n",
    "\n",
    "### Processing Settings\n",
    "- \ud83d\udcca **FRAMES_TO_EXTRACT**: Total number of frames to extract\n",
    "- \ud83d\udcca **SAMPLE_RATE**: Extract every Nth frame from video\n",
    "\n",
    "### Quality Thresholds\n",
    "Relaxed values for experiment data collection:\n",
    "- \ud83d\udd0d **brightness_min/max**: Acceptable brightness range (0-255)\n",
    "- \ud83d\udd0d **blur_min**: Minimum blur score (Laplacian variance)\n",
    "\n",
    "**Note**: These values are hardcoded for initial experiments. Future versions will support parameter files.\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233dd972-9370-4c97-a874-22016189f409",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# experiment configuration parameters\n",
    "from pathlib import Path\n",
    "\n",
    "CONFIG = {\n",
    "    # target parameters\n",
    "    'VIDEO_ID': 'ATL-1005',  # \ud83c\udfaf camera to process\n",
    "    'BATCH_DATE': '20250620',  # \ud83c\udfaf date from batch analysis\n",
    "    'TARGET_HOUR': 12,  # \ud83c\udfaf target hour (noon)\n",
    "    'EXPERIMENT_NAME': 'car_counting_v1',  # \ud83c\udfaf experiment identifier\n",
    "    \n",
    "    # path configuration  \n",
    "    'INPUT_BASE': Path.home() / 'traffic-recordings',  # \ud83d\udcc1 video source\n",
    "    'OUTPUT_BASE': Path('../../data/experiments'),  # \ud83d\udcc1 experiment output\n",
    "    \n",
    "    # processing settings\n",
    "    'FRAMES_TO_EXTRACT': 50,  # \ud83d\udcca total frames to extract\n",
    "    'SAMPLE_RATE': 15,  # \ud83d\udcca extract every Nth frame\n",
    "    \n",
    "    # quality measurement (no hardcoded thresholds)\n",
    "    'MEASURE_QUALITY': True,  # \ud83d\udcca calculate brightness/blur metrics\n",
    "    \n",
    "    # video settings (480p, 15fps)\n",
    "    'JPEG_QUALITY': 95  # \ud83c\udfa5 output quality (0-100)\n",
    "}\n",
    "\n",
    "# derived paths\n",
    "date_formatted = f\"{CONFIG['BATCH_DATE'][:4]}-{CONFIG['BATCH_DATE'][4:6]}-{CONFIG['BATCH_DATE'][6:8]}\"\n",
    "CONFIG['OUTPUT_DIR'] = CONFIG['OUTPUT_BASE'] / CONFIG['EXPERIMENT_NAME'] / date_formatted / CONFIG['VIDEO_ID']\n",
    "CONFIG['VIDEO_DIR'] = CONFIG['INPUT_BASE'] / CONFIG['VIDEO_ID'] / date_formatted\n",
    "\n",
    "print(\"Experiment Configuration:\")\n",
    "print(f\"  Experiment: {CONFIG['EXPERIMENT_NAME']}\")\n",
    "print(f\"  Processing: {CONFIG['VIDEO_ID']} from {date_formatted}\")\n",
    "print(f\"  Output to: {CONFIG['OUTPUT_DIR']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e546e8-b2fa-456d-85ae-9c1a3586a840",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83d\udd27 Environment Setup\n",
    "\n",
    "This cell establishes the experiment preprocessing environment by:\n",
    "\n",
    "1. **Import Required Libraries**\n",
    "   - OpenCV (cv2) for video processing and frame extraction\n",
    "   - NumPy for array operations and numerical computations\n",
    "   - Pandas for data organization and metadata management\n",
    "   - System utilities for path handling and file operations\n",
    "\n",
    "2. **Library Verification**\n",
    "   - Check OpenCV installation and version\n",
    "   - Verify NumPy and Pandas availability\n",
    "   - Display version information for debugging\n",
    "\n",
    "3. **Initialize Helper Functions**\n",
    "   - **calculate_brightness()**: Compute average pixel intensity (0-255)\n",
    "   - **calculate_blur_score()**: Measure sharpness using Laplacian variance\n",
    "   - **get_video_metadata()**: Extract video properties (fps, resolution, duration)\n",
    "\n",
    "4. **Directory Setup**\n",
    "   - Create experiment output directory structure\n",
    "   - Ensure path exists before processing begins\n",
    "\n",
    "**Note**: This cell must run successfully before proceeding with video processing."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c75aeb-b75f-4473-aaf5-437bcb7aa490",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# environment setup\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# verify opencv\n",
    "try:\n",
    "    print(f\"\u2713 OpenCV version: {cv2.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"\u26a0\ufe0f OpenCV not installed. Install with: pip install opencv-python\")\n",
    "    \n",
    "print(f\"\u2713 Python version: {sys.version.split()[0]}\")\n",
    "print(f\"\u2713 NumPy version: {np.__version__}\")\n",
    "print(f\"\u2713 Pandas version: {pd.__version__}\")\n",
    "\n",
    "# helper functions\n",
    "def calculate_brightness(frame):\n",
    "    \"\"\"Calculate average brightness of frame\"\"\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    return np.mean(gray)\n",
    "\n",
    "def calculate_blur_score(frame):\n",
    "    \"\"\"Calculate Laplacian variance (higher = sharper)\"\"\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    return cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "\n",
    "def get_video_metadata(video_path):\n",
    "    \"\"\"Extract video metadata\"\"\"\n",
    "    metadata = {}\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if cap.isOpened():\n",
    "        metadata['fps'] = cap.get(cv2.CAP_PROP_FPS)\n",
    "        metadata['frame_count'] = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        metadata['width'] = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        metadata['height'] = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        metadata['duration_seconds'] = metadata['frame_count'] / metadata['fps'] if metadata['fps'] > 0 else 0\n",
    "        cap.release()\n",
    "    return metadata\n",
    "\n",
    "# create output directory\n",
    "CONFIG['OUTPUT_DIR'].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n\u2713 Environment setup complete\")\n",
    "print(f\"  Output directory created: {CONFIG['OUTPUT_DIR']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527fd756-6182-43c2-8ca9-41b90de644c7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83d\udcf9 Video Ingestion & Selection\n",
    "\n",
    "This module finds the target video from the specified camera and date using the same logic as the batch processing workflow.\n",
    "\n",
    "### Process\n",
    "- Searches for videos matching the camera ID and batch date\n",
    "- Parses timestamps from filenames (format: CAMERA_YYYYMMDD_HHMMSS.mp4)\n",
    "- Calculates time difference from target hour (noon)\n",
    "- Selects video closest to target time\n",
    "- Extracts basic metadata (resolution, duration, frame count)\n",
    "\n",
    "### Expected File Pattern\n",
    "`ATL-1005_20250620_120347.mp4` (camera_date_time.mp4)\n",
    "\n",
    "**Note**: If no videos are found, check the INPUT_BASE path and date format."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c14e6f-e047-4f89-b5b8-87275db8b071",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# video ingestion and selection\n",
    "def parse_timestamp(filename):\n",
    "    \"\"\"extract timestamp from filename\"\"\"\n",
    "    parts = filename.stem.split('_')\n",
    "    if len(parts) >= 3:\n",
    "        time_str = parts[2]\n",
    "        hours = int(time_str[:2])\n",
    "        minutes = int(time_str[2:4])\n",
    "        return hours * 60 + minutes  # minutes from midnight\n",
    "    return None\n",
    "\n",
    "# find videos\n",
    "video_files = list(CONFIG['VIDEO_DIR'].glob(f\"{CONFIG['VIDEO_ID']}_*.mp4\"))\n",
    "\n",
    "if not video_files:\n",
    "    raise FileNotFoundError(f\"No videos found for {CONFIG['VIDEO_ID']} on {CONFIG['BATCH_DATE']}\")\n",
    "\n",
    "# find closest to target hour\n",
    "target_minutes = CONFIG['TARGET_HOUR'] * 60  # 720 minutes for noon\n",
    "closest_video = None\n",
    "min_diff = float('inf')\n",
    "\n",
    "for video in video_files:\n",
    "    minutes = parse_timestamp(video)\n",
    "    if minutes is not None:\n",
    "        diff = abs(minutes - target_minutes)\n",
    "        if diff < min_diff:\n",
    "            min_diff = diff\n",
    "            closest_video = video\n",
    "\n",
    "CONFIG['selected_video'] = closest_video\n",
    "metadata = get_video_metadata(closest_video)\n",
    "CONFIG['video_metadata'] = metadata\n",
    "\n",
    "time_str = closest_video.stem.split('_')[2]\n",
    "print(f\"Selected: {closest_video.name}\")\n",
    "print(f\"  Starts at: {time_str[:2]}:{time_str[2:4]}:{time_str[4:6]}\")\n",
    "print(f\"  Resolution: {metadata['width']}x{metadata['height']}\")\n",
    "print(f\"  Duration: {metadata['duration_seconds']:.1f}s ({metadata['duration_seconds']/60:.1f} min)\")\n",
    "print(f\"  Frame rate: {metadata['fps']:.1f} fps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1144123-87d0-4056-936d-65e5765b321e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83c\udf9e\ufe0f Frame Extraction\n",
    "\n",
    "This module extracts frames from the selected video at specified intervals for experiment processing.\n",
    "\n",
    "### Process\n",
    "- Opens the selected video using OpenCV\n",
    "- Extracts frames using the SAMPLE_RATE (every Nth frame)\n",
    "- Continues until FRAMES_TO_EXTRACT is reached or video ends\n",
    "- Saves frames as JPEG files with specified quality\n",
    "- Creates frames directory in experiment output folder\n",
    "\n",
    "### Frame Naming\n",
    "- Format: `frame_0001.jpg`, `frame_0002.jpg`, etc.\n",
    "- Sequential numbering for easy sorting and reference\n",
    "\n",
    "**Note**: For 15-minute videos at 15fps, total frames \u2248 13,500. Sample rate of 15 gives ~900 available frames."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867a0d73-7b8d-4ed8-8b9a-c280fef8da3d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# frame extraction\n",
    "print(f\"Frame Extraction\")\n",
    "print(f\"Extracting {CONFIG['FRAMES_TO_EXTRACT']} frames (every {CONFIG['SAMPLE_RATE']} frames)\")\n",
    "\n",
    "video_path = CONFIG['selected_video']\n",
    "cap = cv2.VideoCapture(str(video_path))\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise ValueError(f\"Cannot open video: {video_path}\")\n",
    "\n",
    "# create frames directory\n",
    "frames_dir = CONFIG['OUTPUT_DIR'] / 'frames'\n",
    "frames_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# extract frames\n",
    "frames_extracted = 0\n",
    "frame_index = 0\n",
    "\n",
    "while frames_extracted < CONFIG['FRAMES_TO_EXTRACT'] and cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # extract every Nth frame\n",
    "    if frame_index % CONFIG['SAMPLE_RATE'] == 0:\n",
    "        frame_filename = f\"frame_{frames_extracted:04d}.jpg\"\n",
    "        frame_path = frames_dir / frame_filename\n",
    "        \n",
    "        # save frame\n",
    "        cv2.imwrite(str(frame_path), frame, [cv2.IMWRITE_JPEG_QUALITY, CONFIG['JPEG_QUALITY']])\n",
    "        \n",
    "        frames_extracted += 1\n",
    "        if frames_extracted % 10 == 0:\n",
    "            print(f\"  Extracted {frames_extracted}/{CONFIG['FRAMES_TO_EXTRACT']} frames\")\n",
    "    \n",
    "    frame_index += 1\n",
    "\n",
    "cap.release()\n",
    "\n",
    "CONFIG['frames_dir'] = frames_dir\n",
    "CONFIG['frames_extracted'] = frames_extracted\n",
    "\n",
    "print(f\"\\n\u2713 Extracted {frames_extracted} frames to {frames_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5332f99e-8b78-45b4-9580-ac2a4e226288",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# quality analysis\n",
    "print(\"Quality Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# get all extracted frames\n",
    "frame_files = sorted(CONFIG['frames_dir'].glob(\"frame_*.jpg\"))\n",
    "print(f\"Analyzing quality of {len(frame_files)} frames\")\n",
    "\n",
    "quality_results = []\n",
    "\n",
    "for frame_path in frame_files:\n",
    "    # read frame\n",
    "    frame = cv2.imread(str(frame_path))\n",
    "    if frame is None:\n",
    "        continue\n",
    "    \n",
    "    # calculate metrics\n",
    "    brightness = calculate_brightness(frame)\n",
    "    blur_score = calculate_blur_score(frame)\n",
    "    \n",
    "    result = {\n",
    "        'frame': frame_path.name,\n",
    "        'brightness': brightness,\n",
    "        'blur_score': blur_score\n",
    "    }\n",
    "    quality_results.append(result)\n",
    "\n",
    "# save quality report\n",
    "quality_df = pd.DataFrame(quality_results)\n",
    "quality_df.to_csv(CONFIG['OUTPUT_DIR'] / 'quality_report.csv', index=False)\n",
    "\n",
    "CONFIG['quality_results'] = quality_df\n",
    "\n",
    "print(f\"\\nQuality Metrics Summary:\")\n",
    "print(f\"  Brightness range: {quality_df['brightness'].min():.1f} - {quality_df['brightness'].max():.1f}\")\n",
    "print(f\"  Brightness average: {quality_df['brightness'].mean():.1f}\")\n",
    "print(f\"  Blur score range: {quality_df['blur_score'].min():.1f} - {quality_df['blur_score'].max():.1f}\")\n",
    "print(f\"  Blur score average: {quality_df['blur_score'].mean():.1f}\")\n",
    "\n",
    "print(f\"\\n\u2713 Quality analysis complete\")\n",
    "print(f\"  Report saved: quality_report.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c789a6f2-9b78-4386-9c36-2dad58fbdcf3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83c\udfa8 Color Normalization\n",
    "\n",
    "This module converts frames from BGR (Blue-Green-Red) to RGB (Red-Green-Blue) color space for consistency with display systems and ML frameworks.\n",
    "\n",
    "### Process\n",
    "- OpenCV reads images in BGR format by default\n",
    "- Converts BGR to RGB using cv2.cvtColor()\n",
    "- Saves normalized frames for downstream processing\n",
    "- Creates normalized directory in experiment output\n",
    "\n",
    "### Technical Details\n",
    "- BGR: pixel[0]=Blue, pixel[1]=Green, pixel[2]=Red\n",
    "- RGB: pixel[0]=Red, pixel[1]=Green, pixel[2]=Blue\n",
    "- Essential for correct color representation in training data\n",
    "\n",
    "**Note**: This ensures traffic lights appear red (not blue) in annotated data."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea17dfa-d1f2-4caf-b0ad-c5f6bed68b7d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# color normalization\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "print(\"Color Space Normalization\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "frame_files = sorted(CONFIG['frames_dir'].glob(\"frame_*.jpg\"))\n",
    "print(f\"Normalizing {len(frame_files)} frames\")\n",
    "\n",
    "# create normalized directory\n",
    "normalized_dir = CONFIG['OUTPUT_DIR'] / 'normalized'\n",
    "normalized_dir.mkdir(exist_ok=True)\n",
    "\n",
    "normalized_frames = []\n",
    "\n",
    "for i, frame_path in enumerate(frame_files):\n",
    "    # read frame (BGR)\n",
    "    frame_bgr = cv2.imread(str(frame_path))\n",
    "    \n",
    "    # convert BGR to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # save as true RGB using PIL\n",
    "    output_path = normalized_dir / frame_path.name\n",
    "    rgb_image = Image.fromarray(frame_rgb)\n",
    "    rgb_image.save(str(output_path), 'JPEG', quality=CONFIG['JPEG_QUALITY'])\n",
    "    normalized_frames.append(output_path)\n",
    "\n",
    "# show color space difference using first frame\n",
    "sample = cv2.imread(str(frame_files[0]))\n",
    "sample_rgb = cv2.cvtColor(sample, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "# BGR channels\n",
    "axes[0, 0].imshow(sample[:,:,0], cmap='Blues')\n",
    "axes[0, 0].set_title('BGR - Blue Channel (index 0)')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(sample[:,:,1], cmap='Greens')\n",
    "axes[0, 1].set_title('BGR - Green Channel (index 1)')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[0, 2].imshow(sample[:,:,2], cmap='Reds')\n",
    "axes[0, 2].set_title('BGR - Red Channel (index 2)')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# RGB display comparison\n",
    "axes[1, 0].imshow(sample)\n",
    "axes[1, 0].set_title('Direct Display (BGR - Wrong Colors)')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(sample_rgb)\n",
    "axes[1, 1].set_title('After Conversion (RGB - Correct Colors)')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# show pixel values\n",
    "pixel_y, pixel_x = 100, 100\n",
    "bgr_pixel = sample[pixel_y, pixel_x]\n",
    "rgb_pixel = sample_rgb[pixel_y, pixel_x]\n",
    "axes[1, 2].text(0.1, 0.7, f\"Sample pixel at ({pixel_x},{pixel_y}):\", fontsize=12, weight='bold')\n",
    "axes[1, 2].text(0.1, 0.5, f\"BGR: [{bgr_pixel[0]}, {bgr_pixel[1]}, {bgr_pixel[2]}]\", fontsize=11)\n",
    "axes[1, 2].text(0.1, 0.3, f\"RGB: [{rgb_pixel[0]}, {rgb_pixel[1]}, {rgb_pixel[2]}]\", fontsize=11)\n",
    "axes[1, 2].text(0.1, 0.1, \"Note: B\u2194R values swapped\", fontsize=10, style='italic')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.suptitle('Color Space Normalization: BGR \u2192 RGB')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "CONFIG['normalized_frames'] = normalized_frames\n",
    "\n",
    "print(f\"\\n\u2713 Color normalization complete\")\n",
    "print(f\"  Frames processed: {len(normalized_frames)}\")\n",
    "print(f\"  Output: {normalized_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78850818-0418-4926-9539-376cbac034f3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83d\udcc1 Data Organization\n",
    "\n",
    "This module creates comprehensive metadata and organizes processed frames for downstream use.\n",
    "\n",
    "### Process\n",
    "- Compiles experiment metadata (camera, date, parameters, processing summary)\n",
    "- Creates frame inventory with quality metrics\n",
    "- Saves metadata as JSON for experiment tracking\n",
    "- Saves frame inventory as CSV for easy review\n",
    "- Links all processing stages and file paths\n",
    "\n",
    "### Output Files\n",
    "- `metadata.json`: Complete experiment configuration and results\n",
    "- `frame_inventory.csv`: Frame-by-frame quality metrics and paths\n",
    "- Directory structure with clear organization\n",
    "\n",
    "**Note**: Metadata enables reproducibility and traceability for model training."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02310497-d312-460f-9ec4-a5cd6093f880",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# data organization\n",
    "print(\"Data Organization\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# compile metadata\n",
    "metadata = {\n",
    "    'experiment_name': CONFIG['EXPERIMENT_NAME'],\n",
    "    'camera_id': CONFIG['VIDEO_ID'],\n",
    "    'batch_date': CONFIG['BATCH_DATE'],\n",
    "    'source_video': CONFIG['selected_video'].name,\n",
    "    'processing_timestamp': datetime.now().isoformat(),\n",
    "    'parameters': {\n",
    "        'target_hour': CONFIG['TARGET_HOUR'],\n",
    "        'frames_to_extract': CONFIG['FRAMES_TO_EXTRACT'],\n",
    "        'sample_rate': CONFIG['SAMPLE_RATE'],\n",
    "        'jpeg_quality': CONFIG['JPEG_QUALITY']\n",
    "    },\n",
    "    'video_metadata': CONFIG['video_metadata'],\n",
    "    'processing_summary': {\n",
    "        'frames_extracted': CONFIG['frames_extracted'],\n",
    "        'frames_with_quality_data': len(CONFIG['quality_results'])\n",
    "    }\n",
    "}\n",
    "\n",
    "# build frame inventory\n",
    "frame_inventory = []\n",
    "\n",
    "for stage, frame_list, stage_name in [\n",
    "    ('extracted', frame_files, 'Raw extracted frames'),\n",
    "    ('normalized', normalized_frames, 'RGB normalized frames')\n",
    "]:\n",
    "    for frame_path in frame_list:\n",
    "        # get quality metrics\n",
    "        quality_row = CONFIG['quality_results'][CONFIG['quality_results']['frame'] == frame_path.name]\n",
    "        \n",
    "        entry = {\n",
    "            'frame_name': frame_path.name,\n",
    "            'stage': stage,\n",
    "            'stage_description': stage_name,\n",
    "            'path': str(frame_path),\n",
    "            'brightness': quality_row['brightness'].values[0] if not quality_row.empty else None,\n",
    "            'blur_score': quality_row['blur_score'].values[0] if not quality_row.empty else None\n",
    "        }\n",
    "        frame_inventory.append(entry)\n",
    "\n",
    "# save metadata\n",
    "metadata_path = CONFIG['OUTPUT_DIR'] / 'metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "# save frame inventory\n",
    "inventory_df = pd.DataFrame(frame_inventory)\n",
    "inventory_path = CONFIG['OUTPUT_DIR'] / 'frame_inventory.csv'\n",
    "inventory_df.to_csv(inventory_path, index=False)\n",
    "\n",
    "print(f\"\\nExperiment data structure:\")\n",
    "print(f\"  {CONFIG['OUTPUT_DIR']}/\")\n",
    "print(f\"  \u251c\u2500\u2500 frames/          ({CONFIG['frames_extracted']} raw frames)\")\n",
    "print(f\"  \u251c\u2500\u2500 normalized/      ({len(normalized_frames)} RGB frames)\")\n",
    "print(f\"  \u251c\u2500\u2500 metadata.json\")\n",
    "print(f\"  \u251c\u2500\u2500 frame_inventory.csv\")\n",
    "print(f\"  \u2514\u2500\u2500 quality_report.csv\")\n",
    "\n",
    "CONFIG['metadata'] = metadata\n",
    "CONFIG['inventory'] = inventory_df\n",
    "\n",
    "print(f\"\\n\u2713 Data organization complete\")\n",
    "print(f\"  Metadata: {metadata_path}\")\n",
    "print(f\"  Inventory: {inventory_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54bfa10-ef34-4b1e-b242-828c3de7a1e7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83d\udcbe Export & Summary\n",
    "\n",
    "This module creates a final summary of the experiment preprocessing workflow and confirms frames are ready for annotation.\n",
    "\n",
    "### Process\n",
    "- Creates preprocessing completion summary\n",
    "- Documents frame counts and processing stages\n",
    "- Confirms location of frames ready for CVAT annotation\n",
    "- Saves final summary report\n",
    "\n",
    "### Next Steps\n",
    "1. **Manual Annotation**: Import normalized frames into CVAT\n",
    "2. **Vehicle Labeling**: Annotate cars, trucks, motorcycles with bounding boxes\n",
    "3. **Export Labels**: Download YOLO format annotations\n",
    "4. **Model Training**: Train simplest YOLO model for vehicle counting\n",
    "\n",
    "**Output**: Normalized RGB frames in `/normalized/` directory ready for annotation."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012fae88-e40c-46f7-a81a-8af76ca515de",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# export and summary\n",
    "print(\"Export & Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# create final summary\n",
    "summary = {\n",
    "    'experiment_complete': datetime.now().isoformat(),\n",
    "    'experiment_name': CONFIG['EXPERIMENT_NAME'],\n",
    "    'camera': CONFIG['VIDEO_ID'],\n",
    "    'video_processed': CONFIG['selected_video'].name,\n",
    "    'frames_ready_for_annotation': len(normalized_frames),\n",
    "    'annotation_directory': str(CONFIG['OUTPUT_DIR'] / 'normalized'),\n",
    "    'processing_stages': {\n",
    "        '1_extracted': CONFIG['frames_extracted'],\n",
    "        '2_quality_analyzed': len(CONFIG['quality_results']),\n",
    "        '3_rgb_normalized': len(normalized_frames)\n",
    "    },\n",
    "    'quality_metrics': {\n",
    "        'avg_brightness': float(CONFIG['quality_results']['brightness'].mean()),\n",
    "        'avg_blur_score': float(CONFIG['quality_results']['blur_score'].mean()),\n",
    "        'brightness_range': [float(CONFIG['quality_results']['brightness'].min()), \n",
    "                           float(CONFIG['quality_results']['brightness'].max())],\n",
    "        'blur_range': [float(CONFIG['quality_results']['blur_score'].min()), \n",
    "                      float(CONFIG['quality_results']['blur_score'].max())]\n",
    "    }\n",
    "}\n",
    "\n",
    "# save summary\n",
    "summary_path = CONFIG['OUTPUT_DIR'] / 'experiment_summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"Experiment Complete: {CONFIG['EXPERIMENT_NAME']}\")\n",
    "print(f\"\\nFrames ready for annotation: {len(normalized_frames)}\")\n",
    "print(f\"Location: {CONFIG['OUTPUT_DIR'] / 'normalized'}\")\n",
    "print(f\"\\nProcessing pipeline: {CONFIG['frames_extracted']} \u2192 {len(normalized_frames)} frames\")\n",
    "print(f\"Quality metrics saved: {summary_path}\")\n",
    "\n",
    "print(f\"\\n\ud83c\udfaf Next Steps:\")\n",
    "print(f\"  1. Import frames from 'normalized/' into CVAT\")\n",
    "print(f\"  2. Annotate vehicles with bounding boxes\")\n",
    "print(f\"  3. Export YOLO format labels\")\n",
    "print(f\"  4. Train vehicle counting model\")\n",
    "\n",
    "print(f\"\\n\u2713 Experiment preprocessing complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}