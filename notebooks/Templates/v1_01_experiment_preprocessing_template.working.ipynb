{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e067ae6-c621-46d2-84d2-bd702b4b6bc9",
   "metadata": {},
   "source": [
    "## üìì Notebook Manager\n",
    "\n",
    "This cell initializes the widgets required for managing your research notebook. Please run the cell below to enable functionality for:\n",
    "- Exporting cells tagged with `export` into a `clean` notebook\n",
    "- Generating a dynamic Table of Contents (TOC)\n",
    "- Exporting the notebook to GitHub-compatible Markdown\n",
    "\n",
    "‚û°Ô∏è **Be sure to execute the next cell before continuing with any editing or exporting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a3d032a-3a9c-4f05-8a8d-b8b5cbc135ef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f54f29331a8469b8968be2491aa9bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Button(button_style='primary', description='Generate TOC', icon='list', style=Bu‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Cell 1 - Workflow Tools\n",
    "import sys\n",
    "sys.path.insert(0, '../../lib')\n",
    "sys.path.insert(0, '../../scripts') \n",
    "\n",
    "from notebook_tools import TOCWidget, ExportWidget\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "# Create widget instances\n",
    "toc = TOCWidget()\n",
    "export = ExportWidget()\n",
    "\n",
    "# Create horizontal layout\n",
    "left_side = widgets.VBox([toc.button, export.button, toc.status])\n",
    "right_side = widgets.VBox([toc.output, export.output])\n",
    "\n",
    "# Display side by side\n",
    "display(widgets.HBox([left_side, right_side]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5aaf7c-5b3a-47b9-a099-522d8c877d8d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## üìë Table of Contents (Auto-Generated)\n",
    "\n",
    "This section will automatically generate a table of contents for your research notebook once you run the **Generate TOC** function. The table of contents will help you navigate through your data collection, analysis, and findings as your citizen science project develops.\n",
    "\n",
    "‚û°Ô∏è **Do not edit this cell manually. It will be overwritten automatically.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef3a9d2-f7c5-414f-aed9-813e436acf7d",
   "metadata": {},
   "source": [
    "<!-- TOC -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e0c6f-4094-459d-b2db-2512e3c09a93",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "# üß™ Experiment Preprocessing - [VERSION]\n",
    "\n",
    "## üéØ Purpose\n",
    "This notebook preprocesses GDOT traffic camera videos specifically for machine learning experiments. It follows the same workflow as general preprocessing but with experiment-specific parameters.\n",
    "\n",
    "## üìã Context\n",
    "- **Data Source**: Raw GDOT traffic camera recordings\n",
    "- **Output**: Frames optimized for experiment workflows\n",
    "- **Destination**: `data/preprocessing/experiments/`\n",
    "- **Next Steps**: Annotation ‚Üí Training ‚Üí Evaluation\n",
    "\n",
    "## üîÑ Workflow Overview\n",
    "1. Video ingestion from recordings\n",
    "2. Frame extraction (experiment-specific rate)\n",
    "3. Quality control (experiment thresholds)\n",
    "4. Spatial transformations\n",
    "5. Export to experiments directory\n",
    "\n",
    "## üìö Notebook Structure\n",
    "- **Setup**: Environment and dependencies\n",
    "- **Processing**: Experiment-specific preprocessing\n",
    "- **Export**: Organized output for annotation\n",
    "\n",
    "*Processing completed: [DATE] | Version: [VERSION]*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4e1fb7-e87c-46ed-8e2a-d1252806cc28",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## üîß Experiment Configuration\n",
    "\n",
    "This cell defines parameters specific to experiment preprocessing. Values are optimized for machine learning workflows rather than general analysis.\n",
    "\n",
    "#### Problem Statement\n",
    "- üéØ **Objective**: Count vehicles in GDOT traffic camera feeds\n",
    "- üéØ **Challenge**: Detect and count cars, trucks, buses in various conditions\n",
    "- üéØ **Approach**: Preprocess frames specifically for vehicle detection training\n",
    "\n",
    "#### Target Parameters\n",
    "- üéØ **VIDEO_ID**: Specific camera to process\n",
    "- üéØ **BATCH_DATE**: Recording date (YYYYMMDD format)\n",
    "- üéØ **EXPERIMENT_TYPE**: 'car_counting'\n",
    "\n",
    "#### Path Configuration  \n",
    "- üìÅ **INPUT_BASE**: Root directory for video recordings\n",
    "- üìÅ **OUTPUT_BASE**: Root for experiment preprocessing (`data/preprocessing/experiments`)\n",
    "\n",
    "#### Experiment-Specific Settings\n",
    "- üìä **FRAMES_TO_EXTRACT**: Higher count for training data (1000+)\n",
    "- üìä **SAMPLE_RATE**: Denser sampling for better coverage (every 5 frames)\n",
    "- üîç **INCLUDE_EDGE_CASES**: Keep some lower quality frames for model robustness\n",
    "- üîç **MIN_VEHICLES_PER_FRAME**: Prefer frames with vehicles present\n",
    "\n",
    "The following cell initializes the experiment configuration with parameters optimized for vehicle counting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47a31ac3-becb-4da4-a34a-088d3b700101",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# experiment configuration parameters\n",
    "from pathlib import Path\n",
    "\n",
    "CONFIG = {\n",
    "    # target parameters\n",
    "    'VIDEO_ID': 'ATL-1005',  # üéØ camera to process\n",
    "    'BATCH_DATE': '20250620',  # üéØ date from batch analysis\n",
    "    'TARGET_HOUR': 12,  # üéØ target hour (noon)\n",
    "    'EXPERIMENT_TYPE': 'car_counting',  # üéØ experiment type\n",
    "    \n",
    "    # path configuration  \n",
    "    'INPUT_BASE': Path.home() / 'traffic-recordings',  # üìÅ video source\n",
    "    'OUTPUT_BASE': Path('../../data/preprocessing/experiments'),  # üìÅ experiment output\n",
    "    \n",
    "    # processing settings\n",
    "    'FRAMES_TO_EXTRACT': 1000,  # üìä total frames to extract\n",
    "    'SAMPLE_RATE': 5,  # üìä extract every Nth frame\n",
    "    \n",
    "    # quality thresholds (relaxed for experiments)\n",
    "    'QUALITY_THRESHOLD': {\n",
    "        'brightness_min': 90,  # üîç minimum brightness\n",
    "        'brightness_max': 130,  # üîç maximum brightness  \n",
    "        'blur_min': 2500  # üîç minimum blur score\n",
    "    },\n",
    "    \n",
    "    # video settings\n",
    "    'PREFERRED_CODEC': 'mp4v',  # üé• primary codec\n",
    "    'FALLBACK_CODECS': ['h264', 'xvid'],  # üé• alternatives\n",
    "    'MAX_FRAME_WIDTH': 1920,  # üé• max width\n",
    "    'MAX_FRAME_HEIGHT': 1080,  # üé• max height\n",
    "    'JPEG_QUALITY': 95  # üé• output quality (0-100)\n",
    "}\n",
    "\n",
    "# derived paths\n",
    "date_formatted = f\"{CONFIG['BATCH_DATE'][:4]}-{CONFIG['BATCH_DATE'][4:6]}-{CONFIG['BATCH_DATE'][6:8]}\"\n",
    "CONFIG['OUTPUT_DIR'] = CONFIG['OUTPUT_BASE'] / CONFIG['EXPERIMENT_TYPE'] / date_formatted / CONFIG['VIDEO_ID']\n",
    "CONFIG['VIDEO_DIR'] = CONFIG['INPUT_BASE'] / CONFIG['VIDEO_ID'] / date_formatted\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a216541a-bf02-4cca-80c7-b69ca1a02e81",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "*End of Experiment Configuration*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88852afe-ef34-4ee1-aa69-814c96968d0a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## üîß Environment Setup\n",
    "\n",
    "This cell establishes the preprocessing environment with the same core libraries as general preprocessing, plus experiment-specific additions.\n",
    "\n",
    "1. **Core Libraries**\n",
    "   - OpenCV for video processing\n",
    "   - NumPy for array operations\n",
    "   - Pandas for data organization\n",
    "   - Logging for process tracking\n",
    "\n",
    "2. **Same Helper Functions**\n",
    "   - calculate_brightness()\n",
    "   - calculate_blur_score()\n",
    "   - get_video_metadata()\n",
    "\n",
    "3. **Experiment Additions**\n",
    "   - Vehicle detection helpers\n",
    "   - Frame selection priorities\n",
    "   - Experiment metadata tracking\n",
    "\n",
    "The following cell imports libraries and initializes the preprocessing environment.\n",
    "\n",
    "üü¢ **IMPLEMENTATION COMPLETE** üü¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ef92501-2f9c-46a9-8ee4-af94872fc057",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì OpenCV version: 4.11.0\n",
      "‚úì Python version: 3.12.9\n",
      "‚úì NumPy version: 2.2.4\n",
      "‚úì Pandas version: 2.2.3\n",
      "\n",
      "‚úì Environment setup complete\n",
      "  Output directory created: ../../data/preprocessing/experiments/car_counting/2025-06-20/ATL-1005\n"
     ]
    }
   ],
   "source": [
    "# environment setup\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# verify opencv\n",
    "print(f\"‚úì OpenCV version: {cv2.__version__}\")\n",
    "print(f\"‚úì Python version: {sys.version.split()[0]}\")\n",
    "print(f\"‚úì NumPy version: {np.__version__}\")\n",
    "print(f\"‚úì Pandas version: {pd.__version__}\")\n",
    "\n",
    "# helper functions\n",
    "def calculate_brightness(frame):\n",
    "    \"\"\"Calculate average brightness of frame\"\"\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    return np.mean(gray)\n",
    "\n",
    "def calculate_blur_score(frame):\n",
    "    \"\"\"Calculate Laplacian variance (higher = sharper)\"\"\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    return cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "\n",
    "def get_video_metadata(video_path):\n",
    "    \"\"\"Extract video metadata\"\"\"\n",
    "    metadata = {}\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if cap.isOpened():\n",
    "        metadata['fps'] = cap.get(cv2.CAP_PROP_FPS)\n",
    "        metadata['frame_count'] = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        metadata['width'] = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        metadata['height'] = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        metadata['duration_seconds'] = metadata['frame_count'] / metadata['fps'] if metadata['fps'] > 0 else 0\n",
    "        metadata['codec'] = int(cap.get(cv2.CAP_PROP_FOURCC))\n",
    "        cap.release()\n",
    "    return metadata\n",
    "\n",
    "# create output directory\n",
    "CONFIG['OUTPUT_DIR'].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n‚úì Environment setup complete\")\n",
    "print(f\"  Output directory created: {CONFIG['OUTPUT_DIR']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c59502-a557-4b79-90ff-405dc43d3d83",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "*End of Environment Setup*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65fdcf4-5a4e-473c-9c93-54e3adbbb308",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## üîÑ Progress Tracking & Checkpoint System\n",
    "\n",
    "The following cells implement simple progress tracking and checkpoint functionality to:\n",
    "\n",
    "1. **Track Processing Progress**\n",
    "   - Monitor experiment preprocessing status\n",
    "   - Count frames extracted and processed\n",
    "   - Display elapsed time\n",
    "\n",
    "2. **Enable Restart Capability**\n",
    "   - Save progress after each stage\n",
    "   - Automatically resume from last checkpoint\n",
    "   - Maintain experiment metadata\n",
    "\n",
    "This ensures experiment preprocessing can be resumed if interrupted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8437e3-9c32-4447-993c-1103c07336ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## üíæ Initialize Checkpoint and Progress Tracking Functions\n",
    "\n",
    "This module establishes checkpoint and progress tracking for experiment preprocessing. Functions track which videos have been processed for experiments and enable recovery from interruptions.\n",
    "\n",
    "The following cell sets up checkpoint functionality specific to experiment preprocessing.\n",
    "\n",
    "üü¢ **IMPLEMENTATION COMPLETE** üü¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d42ad75-0f3f-40b8-833b-a43efb91f417",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to process experiment data. Checkpoint initialized.\n"
     ]
    }
   ],
   "source": [
    "# checkpoint and progress tracking\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "CHECKPOINT_FILE = CONFIG['OUTPUT_DIR'] / \"experiment_checkpoint.json\"\n",
    "start_time = time.time()\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"Load previous progress if it exists\"\"\"\n",
    "    if CHECKPOINT_FILE.exists():\n",
    "        with open(CHECKPOINT_FILE, 'r') as f:\n",
    "            checkpoint = json.load(f)\n",
    "            print(f\"‚úì Loaded checkpoint: {checkpoint.get('stage', 'unknown')} stage\")\n",
    "            return checkpoint\n",
    "    return {\n",
    "        \"stage\": \"started\",\n",
    "        \"processed\": {}, \n",
    "        \"start_time\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "def save_checkpoint(checkpoint):\n",
    "    \"\"\"Save current progress\"\"\"\n",
    "    checkpoint['last_updated'] = datetime.now().isoformat()\n",
    "    with open(CHECKPOINT_FILE, 'w') as f:\n",
    "        json.dump(checkpoint, f, indent=2)\n",
    "\n",
    "def update_progress(stage, details=None):\n",
    "    \"\"\"Update and save progress\"\"\"\n",
    "    checkpoint = load_checkpoint()\n",
    "    checkpoint['stage'] = stage\n",
    "    if details:\n",
    "        checkpoint['processed'][stage] = details\n",
    "    save_checkpoint(checkpoint)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Stage: {stage}\")\n",
    "    print(f\"Elapsed: {elapsed/60:.1f}min\")\n",
    "\n",
    "# Initialize checkpoint\n",
    "checkpoint = load_checkpoint()\n",
    "print(f\"Ready to process experiment data. Checkpoint initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb33b27-4381-476f-a4b7-31f366165abb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## üìπ Video Ingestion & Cataloging\n",
    "\n",
    "This module loads video files for experiment preprocessing with focus on selecting videos that contain vehicle activity.\n",
    "\n",
    "1. **Find Video Files**\n",
    "   - Locate videos for specified camera and date\n",
    "   - Parse timestamps from filenames\n",
    "   - Select videos during high-traffic periods\n",
    "\n",
    "2. **Prioritize for Experiments**\n",
    "   - Prefer daytime hours (better visibility)\n",
    "   - Avoid night/dawn/dusk for initial experiments\n",
    "   - Focus on peak traffic times\n",
    "\n",
    "3. **Extract Metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d17b5c7e-9c6f-4723-be2e-2542ba0fb936",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: ATL-1005_20250620_120641.mp4\n",
      "  Starts at: 12:06:41\n",
      "\n",
      "[14:47:03] Stage: video_selected\n",
      "Elapsed: 0.0min\n"
     ]
    }
   ],
   "source": [
    "# video ingestion and cataloging\n",
    "def parse_timestamp(filename):\n",
    "    \"\"\"extract timestamp from filename\"\"\"\n",
    "    parts = filename.stem.split('_')\n",
    "    if len(parts) >= 3:\n",
    "        time_str = parts[2]\n",
    "        hours = int(time_str[:2])\n",
    "        minutes = int(time_str[2:4])\n",
    "        return hours * 60 + minutes  # minutes from midnight\n",
    "    return None\n",
    "\n",
    "# find videos\n",
    "video_files = list(CONFIG['VIDEO_DIR'].glob(f\"{CONFIG['VIDEO_ID']}_*.mp4\"))\n",
    "\n",
    "if not video_files:\n",
    "    raise FileNotFoundError(f\"No videos found for {CONFIG['VIDEO_ID']} on {CONFIG['BATCH_DATE']}\")\n",
    "\n",
    "# find closest to noon\n",
    "target_minutes = CONFIG['TARGET_HOUR'] * 60  # 720 minutes\n",
    "closest_video = None\n",
    "min_diff = float('inf')\n",
    "\n",
    "for video in video_files:\n",
    "    minutes = parse_timestamp(video)\n",
    "    if minutes is not None:\n",
    "        diff = abs(minutes - target_minutes)\n",
    "        if diff < min_diff:\n",
    "            min_diff = diff\n",
    "            closest_video = video\n",
    "\n",
    "CONFIG['selected_video'] = closest_video\n",
    "time_str = closest_video.stem.split('_')[2]\n",
    "print(f\"Selected: {closest_video.name}\")\n",
    "print(f\"  Starts at: {time_str[:2]}:{time_str[2:4]}:{time_str[4:6]}\")\n",
    "\n",
    "# update checkpoint\n",
    "update_progress('video_selected', {'video': closest_video.name})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39785d35-172d-4087-87ad-57f73bc573a9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## üéûÔ∏è Frame Extraction\n",
    "\n",
    "This module samples frames from video sequences with parameters optimized for experiment training data.\n",
    "\n",
    "1. **Experiment-Specific Extraction**\n",
    "   - Higher frame count for training diversity\n",
    "   - Denser sampling rate (every 5 frames vs 15)\n",
    "   - Extract from multiple time periods\n",
    "\n",
    "2. **Quality Over Compression**\n",
    "   - Higher JPEG quality for annotation clarity\n",
    "   - Full resolution preservation\n",
    "   - No aggressive downsampling\n",
    "\n",
    "3. **Metadata Tracking**\n",
    "   - Frame timestamp mapping\n",
    "   - Source video reference\n",
    "   - Frame sequence numbering\n",
    "\n",
    "The following cell extracts frames using experiment-optimized parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9fa51292-a8ee-43bc-915e-4478fe95be5a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Extraction\n",
      "Extracting 1000 frames (every 5 frames)\n",
      "  Extracted 100/1000 frames\n",
      "  Extracted 200/1000 frames\n",
      "  Extracted 300/1000 frames\n",
      "  Extracted 400/1000 frames\n",
      "  Extracted 500/1000 frames\n",
      "  Extracted 600/1000 frames\n",
      "  Extracted 700/1000 frames\n",
      "  Extracted 800/1000 frames\n",
      "  Extracted 900/1000 frames\n",
      "  Extracted 1000/1000 frames\n",
      "\n",
      "‚úì Extracted 1000 frames to ../../data/preprocessing/experiments/car_counting/2025-06-20/ATL-1005/frames\n",
      "‚úì Loaded checkpoint: video_selected stage\n",
      "\n",
      "[14:47:35] Stage: frames_extracted\n",
      "Elapsed: 0.5min\n"
     ]
    }
   ],
   "source": [
    "# frame extraction\n",
    "print(f\"Frame Extraction\")\n",
    "print(f\"Extracting {CONFIG['FRAMES_TO_EXTRACT']} frames (every {CONFIG['SAMPLE_RATE']} frames)\")\n",
    "\n",
    "video_path = CONFIG['selected_video']\n",
    "cap = cv2.VideoCapture(str(video_path))\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise ValueError(f\"Cannot open video: {video_path}\")\n",
    "\n",
    "# create frames directory\n",
    "frames_dir = CONFIG['OUTPUT_DIR'] / 'frames'\n",
    "frames_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# extract frames\n",
    "frames_extracted = 0\n",
    "frame_index = 0\n",
    "\n",
    "while frames_extracted < CONFIG['FRAMES_TO_EXTRACT'] and cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # extract every Nth frame\n",
    "    if frame_index % CONFIG['SAMPLE_RATE'] == 0:\n",
    "        frame_filename = f\"frame_{frames_extracted:04d}.jpg\"\n",
    "        frame_path = frames_dir / frame_filename\n",
    "        \n",
    "        # save frame\n",
    "        cv2.imwrite(str(frame_path), frame, [cv2.IMWRITE_JPEG_QUALITY, CONFIG['JPEG_QUALITY']])\n",
    "        \n",
    "        frames_extracted += 1\n",
    "        if frames_extracted % 100 == 0:\n",
    "            print(f\"  Extracted {frames_extracted}/{CONFIG['FRAMES_TO_EXTRACT']} frames\")\n",
    "    \n",
    "    frame_index += 1\n",
    "\n",
    "cap.release()\n",
    "\n",
    "CONFIG['frames_dir'] = frames_dir\n",
    "CONFIG['frames_extracted'] = frames_extracted\n",
    "\n",
    "print(f\"\\n‚úì Extracted {frames_extracted} frames to {frames_dir}\")\n",
    "\n",
    "# update checkpoint\n",
    "update_progress('frames_extracted', {'count': frames_extracted})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfeae58-f6b6-4ba5-99a4-d04267561e96",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## üîç Image Quality Control\n",
    "\n",
    "This module filters frames with experiment-specific quality thresholds that balance data quality with training diversity.\n",
    "\n",
    "1. **Relaxed Thresholds**\n",
    "   - Slightly lower brightness bounds (include dawn/dusk)\n",
    "   - Accept moderate blur (real-world conditions)\n",
    "   - Keep edge cases for model robustness\n",
    "\n",
    "2. **Vehicle Presence Priority**\n",
    "   - Prioritize frames with motion\n",
    "   - Check for object-like shapes\n",
    "   - Balance empty vs occupied frames\n",
    "\n",
    "3. **Training Set Diversity**\n",
    "   - Include various lighting conditions\n",
    "   - Keep some challenging frames\n",
    "   - Document quality distribution\n",
    "\n",
    "The following cell applies quality filtering optimized for ML training diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "265986d5-0215-4b48-bf18-59a3423414ec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Quality Control\n",
      "Checking quality of 1000 frames\n",
      "\n",
      "Results:\n",
      "  Good frames: 1000\n",
      "  Poor frames: 0\n",
      "  Pass rate: 100.0%\n",
      "‚úì Loaded checkpoint: frames_extracted stage\n",
      "\n",
      "[14:48:16] Stage: quality_control\n",
      "Elapsed: 1.2min\n"
     ]
    }
   ],
   "source": [
    "# image quality control\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Image Quality Control\")\n",
    "\n",
    "# get all extracted frames\n",
    "frame_files = sorted(CONFIG['frames_dir'].glob(\"frame_*.jpg\"))\n",
    "print(f\"Checking quality of {len(frame_files)} frames\")\n",
    "\n",
    "quality_results = []\n",
    "good_frames = []\n",
    "poor_frames = []\n",
    "\n",
    "for frame_path in frame_files:\n",
    "    # read frame\n",
    "    frame = cv2.imread(str(frame_path))\n",
    "    if frame is None:\n",
    "        poor_frames.append(frame_path)\n",
    "        continue\n",
    "    \n",
    "    # calculate metrics\n",
    "    brightness = calculate_brightness(frame)\n",
    "    blur_score = calculate_blur_score(frame)\n",
    "    \n",
    "    # check thresholds\n",
    "    passes_quality = (\n",
    "        brightness >= CONFIG['QUALITY_THRESHOLD']['brightness_min'] and\n",
    "        brightness <= CONFIG['QUALITY_THRESHOLD']['brightness_max'] and\n",
    "        blur_score >= CONFIG['QUALITY_THRESHOLD']['blur_min']\n",
    "    )\n",
    "    \n",
    "    result = {\n",
    "        'frame': frame_path.name,\n",
    "        'brightness': brightness,\n",
    "        'blur_score': blur_score,\n",
    "        'passes': passes_quality\n",
    "    }\n",
    "    quality_results.append(result)\n",
    "    \n",
    "    if passes_quality:\n",
    "        good_frames.append(frame_path)\n",
    "    else:\n",
    "        poor_frames.append(frame_path)\n",
    "\n",
    "# save quality report\n",
    "quality_df = pd.DataFrame(quality_results)\n",
    "quality_df.to_csv(CONFIG['OUTPUT_DIR'] / 'quality_report.csv', index=False)\n",
    "\n",
    "CONFIG['good_frames'] = good_frames\n",
    "CONFIG['quality_results'] = quality_df\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Good frames: {len(good_frames)}\")\n",
    "print(f\"  Poor frames: {len(poor_frames)}\")\n",
    "print(f\"  Pass rate: {len(good_frames)/len(frame_files)*100:.1f}%\")\n",
    "\n",
    "# update checkpoint\n",
    "update_progress('quality_control', {'good': len(good_frames), 'poor': len(poor_frames)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6cb5db-1880-47d4-844c-0a92b3321336",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## üìê Spatial Transformations\n",
    "\n",
    "This module resizes frames if needed. Same as general preprocessing.\n",
    "\n",
    "The following cell applies standard transformations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fee01c97-f98b-45b2-a43a-92edceaaecaf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial Transformations\n",
      "Transforming 1000 frames\n",
      "\n",
      "‚úì Completed spatial transformations\n",
      "‚úì Loaded checkpoint: quality_control stage\n",
      "\n",
      "[14:48:48] Stage: spatial_transformations\n",
      "Elapsed: 1.8min\n"
     ]
    }
   ],
   "source": [
    "# spatial transformations\n",
    "print(\"Spatial Transformations\")\n",
    "\n",
    "frames_to_transform = CONFIG['good_frames']\n",
    "print(f\"Transforming {len(frames_to_transform)} frames\")\n",
    "\n",
    "# create transformed directory\n",
    "transformed_dir = CONFIG['OUTPUT_DIR'] / 'transformed'\n",
    "transformed_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# target dimensions\n",
    "target_width = CONFIG['MAX_FRAME_WIDTH']\n",
    "target_height = CONFIG['MAX_FRAME_HEIGHT']\n",
    "\n",
    "transformed_frames = []\n",
    "\n",
    "for frame_path in frames_to_transform:\n",
    "    # read frame\n",
    "    frame = cv2.imread(str(frame_path))\n",
    "    height, width = frame.shape[:2]\n",
    "    \n",
    "    # resize if needed\n",
    "    if width > target_width or height > target_height:\n",
    "        scale = min(target_width/width, target_height/height)\n",
    "        new_width = int(width * scale)\n",
    "        new_height = int(height * scale)\n",
    "        frame = cv2.resize(frame, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # save transformed frame\n",
    "    output_path = transformed_dir / frame_path.name\n",
    "    cv2.imwrite(str(output_path), frame, [cv2.IMWRITE_JPEG_QUALITY, CONFIG['JPEG_QUALITY']])\n",
    "    transformed_frames.append(output_path)\n",
    "\n",
    "CONFIG['transformed_frames'] = transformed_frames\n",
    "print(f\"\\n‚úì Completed spatial transformations\")\n",
    "\n",
    "# update checkpoint\n",
    "update_progress('spatial_transformations', {'count': len(transformed_frames)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a738b3c-5df5-4a45-89c3-8fc32a585aaf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## üé® Color Space Normalization\n",
    "\n",
    "Convert BGR to RGB. Same as general preprocessing.\n",
    "\n",
    "The following cell normalizes color space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df442fee-39f3-4ac6-ac4d-8a1344e074b7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color Space Normalization\n",
      "Normalizing 1000 frames\n",
      "\n",
      "‚úì Color normalization complete\n",
      "‚úì Loaded checkpoint: spatial_transformations stage\n",
      "\n",
      "[14:49:12] Stage: color_normalization\n",
      "Elapsed: 2.2min\n"
     ]
    }
   ],
   "source": [
    "# color space normalization\n",
    "print(\"Color Space Normalization\")\n",
    "\n",
    "frames_to_normalize = CONFIG['transformed_frames']\n",
    "print(f\"Normalizing {len(frames_to_normalize)} frames\")\n",
    "\n",
    "# create normalized directory\n",
    "normalized_dir = CONFIG['OUTPUT_DIR'] / 'normalized'\n",
    "normalized_dir.mkdir(exist_ok=True)\n",
    "\n",
    "normalized_frames = []\n",
    "\n",
    "for frame_path in frames_to_normalize:\n",
    "    # read frame (BGR)\n",
    "    frame_bgr = cv2.imread(str(frame_path))\n",
    "    \n",
    "    # convert BGR to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # save as RGB\n",
    "    output_path = normalized_dir / frame_path.name\n",
    "    cv2.imwrite(str(output_path), cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR), \n",
    "                [cv2.IMWRITE_JPEG_QUALITY, CONFIG['JPEG_QUALITY']])\n",
    "    normalized_frames.append(output_path)\n",
    "\n",
    "CONFIG['normalized_frames'] = normalized_frames\n",
    "print(f\"\\n‚úì Color normalization complete\")\n",
    "\n",
    "# update checkpoint\n",
    "update_progress('color_normalization', {'count': len(normalized_frames)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d2b46c-5faf-4265-98e7-357d8489edcd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## ‚è±Ô∏è Temporal Downsampling\n",
    "\n",
    "For experiments, we keep ALL frames with vehicles (no downsampling). This maximizes training data.\n",
    "\n",
    "The following cell identifies and keeps frames with motion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "448a9f52-9a08-4e27-8323-0a17a624f92d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Downsampling\n",
      "Analyzing 1000 frames for motion\n"
     ]
    }
   ],
   "source": [
    "# temporal downsampling (keep frames with motion)\n",
    "print(\"Temporal Downsampling\")\n",
    "\n",
    "frames_to_analyze = CONFIG['normalized_frames']\n",
    "print(f\"Analyzing {len(frames_to_analyze)} frames for motion\")\n",
    "\n",
    "# motion detection\n",
    "motion_scores = []\n",
    "\n",
    "for i in range(len(frames_to_analyze) - 1):\n",
    "    frame1 = cv2.imread(str(frames_to_analyze[i]), cv2.IMREAD_GRAYSCALE)\n",
    "    frame2 = cv2.imread(str(frames_to_analyze[i+1]), cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # calculate difference\n",
    "    diff = cv2.absdiff(frame1, frame2)\n",
    "    motion_score = np.mean(diff)\n",
    "    \n",
    "    motion_scores.append({\n",
    "        'frame': frames_to_analyze[i].name,\n",
    "        'motion_score': motion_score,\n",
    "        'has_motion': motion_score > 5.0\n",
    "    })\n",
    "\n",
    "# for experiments, keep ALL frames with motion\n",
    "motion_df = pd.DataFrame(motion_scores)\n",
    "frames_with_motion = motion_df[motion_df['has_motion']]['frame'].tolist()\n",
    "\n",
    "# create downsampled directory\n",
    "downsampled_dir = CONFIG['OUTPUT_DIR'] / 'downsampled'\n",
    "downsampled_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# copy all frames with motion\n",
    "downsampled_frames = []\n",
    "for frame_name in frames_with_motion:\n",
    "    src = normalized_dir / frame_name\n",
    "    dst = downsampled_dir / frame_name\n",
    "    frame = cv2.imread(str(src))\n",
    "    cv2.imwrite(str(dst), frame, [cv2.IMWRITE_JPEG_QUALITY, CONFIG['JPEG_QUALITY']])\n",
    "    downsampled_frames.append(dst)\n",
    "\n",
    "CONFIG['downsampled_frames'] = downsampled_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fff422-9675-40a4-bb68-aedf7e7eb77f",
   "metadata": {},
   "source": [
    "## üìÅ Data Organization\n",
    "\n",
    "This module organizes frames for experiment workflows with metadata tracking.\n",
    "\n",
    "The following cell creates experiment directory structure and metadata.\n",
    "\n",
    "üöß **IMPLEMENTATION PENDING** üöß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b1311f4-e54a-46fb-a976-1d87724cdd41",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Organization\n",
      "\n",
      "‚úì Data organization complete\n",
      "  Metadata saved: ../../data/preprocessing/experiments/car_counting/2025-06-20/ATL-1005/metadata.json\n",
      "  Frame inventory: ../../data/preprocessing/experiments/car_counting/2025-06-20/ATL-1005/frame_inventory.csv\n",
      "‚úì Loaded checkpoint: color_normalization stage\n",
      "\n",
      "[14:50:10] Stage: data_organization\n",
      "Elapsed: 3.1min\n"
     ]
    }
   ],
   "source": [
    "# data organization\n",
    "print(\"Data Organization\")\n",
    "\n",
    "# compile metadata\n",
    "metadata = {\n",
    "    'camera_id': CONFIG['VIDEO_ID'],\n",
    "    'batch_date': CONFIG['BATCH_DATE'],\n",
    "    'experiment_type': CONFIG['EXPERIMENT_TYPE'],\n",
    "    'source_video': CONFIG['selected_video'].name,\n",
    "    'processing_timestamp': datetime.now().isoformat(),\n",
    "    'parameters': {\n",
    "        'target_hour': CONFIG['TARGET_HOUR'],\n",
    "        'frames_extracted': CONFIG['FRAMES_TO_EXTRACT'],\n",
    "        'sample_rate': CONFIG['SAMPLE_RATE'],\n",
    "        'quality_thresholds': CONFIG['QUALITY_THRESHOLD'],\n",
    "        'jpeg_quality': CONFIG['JPEG_QUALITY']\n",
    "    },\n",
    "    'processing_summary': {\n",
    "        'frames_extracted': CONFIG['frames_extracted'],\n",
    "        'frames_good_quality': len(CONFIG['good_frames']),\n",
    "        'frames_with_motion': len(CONFIG['downsampled_frames'])\n",
    "    }\n",
    "}\n",
    "\n",
    "# save metadata\n",
    "metadata_path = CONFIG['OUTPUT_DIR'] / 'metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "# save frame inventory\n",
    "frame_inventory = []\n",
    "for frame_path in CONFIG['downsampled_frames']:\n",
    "    frame_inventory.append({\n",
    "        'frame_name': frame_path.name,\n",
    "        'path': str(frame_path)\n",
    "    })\n",
    "\n",
    "inventory_df = pd.DataFrame(frame_inventory)\n",
    "inventory_path = CONFIG['OUTPUT_DIR'] / 'frame_inventory.csv'\n",
    "inventory_df.to_csv(inventory_path, index=False)\n",
    "\n",
    "CONFIG['metadata'] = metadata\n",
    "CONFIG['inventory'] = inventory_df\n",
    "\n",
    "print(f\"\\n‚úì Data organization complete\")\n",
    "print(f\"  Metadata saved: {metadata_path}\")\n",
    "print(f\"  Frame inventory: {inventory_path}\")\n",
    "\n",
    "# update checkpoint\n",
    "update_progress('data_organization', {'frames_cataloged': len(frame_inventory)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f2d029-757c-4460-85d2-e7bfb34cf959",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## üíæ Export & Storage\n",
    "\n",
    "Creates final summary and confirms frames are ready for annotation.\n",
    "\n",
    "The following cell saves experiment preprocessing summary.\n",
    "\n",
    "üöß **IMPLEMENTATION PENDING** üöß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d93a74c1-56b2-4e6e-b92c-d987439cd11b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export & Storage Summary\n",
      "\n",
      "Experiment Preprocessing Complete\n",
      "Camera: ATL-1005\n",
      "Frames ready: 150\n",
      "Location: ../../data/preprocessing/experiments/car_counting/2025-06-20/ATL-1005/downsampled\n",
      "Summary: ../../data/preprocessing/experiments/car_counting/2025-06-20/ATL-1005/experiment_preprocessing_summary.json\n",
      "‚úì Loaded checkpoint: data_organization stage\n",
      "\n",
      "[14:50:39] Stage: complete\n",
      "Elapsed: 3.6min\n"
     ]
    }
   ],
   "source": [
    "# export and storage\n",
    "print(\"Export & Storage Summary\")\n",
    "\n",
    "# create summary report\n",
    "summary = {\n",
    "    'experiment_preprocessing_complete': datetime.now().isoformat(),\n",
    "    'experiment_type': CONFIG['EXPERIMENT_TYPE'],\n",
    "    'camera': CONFIG['VIDEO_ID'],\n",
    "    'video_processed': CONFIG['selected_video'].name,\n",
    "    'frames_ready_for_annotation': len(CONFIG['downsampled_frames']),\n",
    "    'annotation_directory': str(CONFIG['OUTPUT_DIR'] / 'downsampled'),\n",
    "    'processing_stages': {\n",
    "        '1_extracted': CONFIG['frames_extracted'],\n",
    "        '2_quality_filtered': len(CONFIG['good_frames']),\n",
    "        '3_transformed': len(CONFIG['transformed_frames']),\n",
    "        '4_normalized': len(CONFIG['normalized_frames']),\n",
    "        '5_motion_filtered': len(CONFIG['downsampled_frames'])\n",
    "    }\n",
    "}\n",
    "\n",
    "# save summary\n",
    "summary_path = CONFIG['OUTPUT_DIR'] / 'experiment_preprocessing_summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nExperiment Preprocessing Complete\")\n",
    "print(f\"Camera: {CONFIG['VIDEO_ID']}\")\n",
    "print(f\"Frames ready: {len(CONFIG['downsampled_frames'])}\")\n",
    "print(f\"Location: {CONFIG['OUTPUT_DIR'] / 'downsampled'}\")\n",
    "print(f\"Summary: {summary_path}\")\n",
    "\n",
    "# update checkpoint\n",
    "update_progress('complete', summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb275bd-098a-4640-bb9b-f63b796c22bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## üè∑Ô∏è Prepare Frames for CVAT Annotation\n",
    "\n",
    "This module packages the preprocessed frames for upload to CVAT (Computer Vision Annotation Tool).\n",
    "\n",
    "The following cell creates a zip file of frames for CVAT import.\n",
    "\n",
    "üöß **IMPLEMENTATION PENDING** üöß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66e32059-6a1b-4407-b42f-2d3f67f41dfc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created CVAT upload file: ../../data/preprocessing/experiments/annotations/car_counting/2025-06-20/ATL-1005/ATL-1005_frames_for_cvat.zip\n",
      "  Contains 150 frames\n"
     ]
    }
   ],
   "source": [
    "# prepare frames for CVAT\n",
    "import zipfile\n",
    "\n",
    "frames_to_annotate = CONFIG['OUTPUT_DIR'] / 'downsampled'\n",
    "frame_files = sorted(frames_to_annotate.glob('*.jpg'))\n",
    "\n",
    "# create annotation directory\n",
    "annotation_dir = CONFIG['OUTPUT_DIR'].parent.parent.parent / 'annotations' / CONFIG['EXPERIMENT_TYPE'] / date_formatted / CONFIG['VIDEO_ID']\n",
    "annotation_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# zip frames for CVAT\n",
    "zip_path = annotation_dir / f\"{CONFIG['VIDEO_ID']}_frames_for_cvat.zip\"\n",
    "with zipfile.ZipFile(zip_path, 'w') as zf:\n",
    "    for frame in frame_files:\n",
    "        zf.write(frame, frame.name)\n",
    "\n",
    "print(f\"‚úì Created CVAT upload file: {zip_path}\")\n",
    "print(f\"  Contains {len(frame_files)} frames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c9e0f9-a466-48e4-94c7-aaa69937f67c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## üê≥ CVAT Docker Setup\n",
    "\n",
    "Run CVAT locally using Docker:\n",
    "\n",
    "```bash\n",
    "# Start CVAT\n",
    "docker run -d --name cvat -p 8080:8080 openvino/cvat\n",
    "\n",
    "# Access at http://localhost:8080\n",
    "# Default: username=admin, password=admin\n",
    "```\n",
    "\n",
    "1. Create new task\n",
    "2. Upload the zip file\n",
    "3. Set labels: car, truck, bus\n",
    "4. Annotate with bounding boxes\n",
    "5. Export as CVAT XML\n",
    "\n",
    "üöß **MANUAL PROCESS** üöß"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cfb551-d8af-4feb-9028-2296020ab057",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## üì• Convert CVAT Annotations\n",
    "\n",
    "This module converts CVAT XML output to training CSV format.\n",
    "\n",
    "The following cell processes CVAT annotations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "457aec75-f4bc-443e-a88f-96e73c9687c6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è No CVAT XML file found. Complete annotation first.\n"
     ]
    }
   ],
   "source": [
    "# convert CVAT annotations to CSV\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "# path to CVAT XML export (update after annotation)\n",
    "cvat_xml_path = annotation_dir / 'annotations.xml'\n",
    "\n",
    "if cvat_xml_path.exists():\n",
    "    # parse CVAT XML\n",
    "    tree = ET.parse(cvat_xml_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    annotations = []\n",
    "    \n",
    "    # extract annotations\n",
    "    for image in root.findall('.//image'):\n",
    "        frame_name = image.get('name')\n",
    "        \n",
    "        for box in image.findall('.//box'):\n",
    "            annotations.append({\n",
    "                'frame': frame_name,\n",
    "                'label': box.get('label'),\n",
    "                'xtl': float(box.get('xtl')),\n",
    "                'ytl': float(box.get('ytl')),\n",
    "                'xbr': float(box.get('xbr')),\n",
    "                'ybr': float(box.get('ybr'))\n",
    "            })\n",
    "    \n",
    "    # save as CSV\n",
    "    annotations_df = pd.DataFrame(annotations)\n",
    "    csv_path = annotation_dir / 'annotations.csv'\n",
    "    annotations_df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"‚úì Converted {len(annotations)} annotations\")\n",
    "    print(f\"  Saved to: {csv_path}\")\n",
    "    \n",
    "    # summary\n",
    "    print(f\"\\nAnnotation summary:\")\n",
    "    print(annotations_df['label'].value_counts())\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No CVAT XML file found. Complete annotation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef01d19-30dd-4660-8bef-b8bf46b464b3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## ‚úÖ Annotation Workflow Complete\n",
    "\n",
    "At this point:\n",
    "1. Frames are preprocessed and packaged\n",
    "2. CVAT annotation can be performed\n",
    "3. Annotations are converted to CSV format\n",
    "\n",
    "**Next Steps:**\n",
    "- Use annotations.csv for training\n",
    "- Create v1_03_experiment_training_template.working.ipynb\n",
    "\n",
    "üü¢ **PREPROCESSING & ANNOTATION COMPLETE** üü¢"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37ee995-5599-406c-97af-570cb1a629c3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## üß† Training Configuration\n",
    "\n",
    "Configure YOLOv8 training parameters for car counting model.\n",
    "\n",
    "The following cell sets up training configuration.\n",
    "\n",
    "üöß **IMPLEMENTATION PENDING** üöß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4c6ee402-fd8d-4782-a0c8-6f311cf573b2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Training configuration ready\n",
      "  Model: yolov8n.pt\n",
      "  Epochs: 10\n",
      "  Output: ../../data/preprocessing/experiments/annotations/car_counting/2025-06-20/ATL-1005/runs\n"
     ]
    }
   ],
   "source": [
    "# training configuration\n",
    "TRAIN_CONFIG = {\n",
    "    'model': 'yolov8n.pt',  # nano model for quick training\n",
    "    'data_yaml': annotation_dir / 'data.yaml',\n",
    "    'epochs': 10,  # quick test\n",
    "    'imgsz': 640,\n",
    "    'batch': 16,\n",
    "    'project': annotation_dir / 'runs',\n",
    "    'name': f\"{CONFIG['VIDEO_ID']}_car_detector\"\n",
    "}\n",
    "\n",
    "# create data.yaml for YOLO\n",
    "data_yaml_content = f\"\"\"\n",
    "train: {annotation_dir / 'images'}\n",
    "val: {annotation_dir / 'images'}\n",
    "\n",
    "nc: 3\n",
    "names: ['car', 'truck', 'bus']\n",
    "\"\"\"\n",
    "\n",
    "with open(TRAIN_CONFIG['data_yaml'], 'w') as f:\n",
    "    f.write(data_yaml_content)\n",
    "\n",
    "print(\"‚úì Training configuration ready\")\n",
    "print(f\"  Model: {TRAIN_CONFIG['model']}\")\n",
    "print(f\"  Epochs: {TRAIN_CONFIG['epochs']}\")\n",
    "print(f\"  Output: {TRAIN_CONFIG['project']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff42dc1-f8f4-4aa9-86ed-3412f632113c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## üìÅ Prepare Dataset for Training\n",
    "\n",
    "Convert CSV annotations to YOLO format and organize images.\n",
    "\n",
    "The following cell prepares the training dataset.\n",
    "\n",
    "üöß **IMPLEMENTATION PENDING** üöß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e024c6cc-8e50-4c30-b23d-0f32d337c03c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è No annotations found. Complete CVAT annotation first.\n"
     ]
    }
   ],
   "source": [
    "# prepare dataset for YOLO training\n",
    "import shutil\n",
    "\n",
    "# create directories\n",
    "images_dir = annotation_dir / 'images'\n",
    "labels_dir = annotation_dir / 'labels'\n",
    "images_dir.mkdir(exist_ok=True)\n",
    "labels_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# copy images\n",
    "for frame in frame_files:\n",
    "    shutil.copy(frame, images_dir / frame.name)\n",
    "\n",
    "# convert annotations to YOLO format\n",
    "csv_path = annotation_dir / 'annotations.csv'\n",
    "if csv_path.exists():\n",
    "    annotations_df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # get image dimensions (assuming all same size)\n",
    "    sample_img = cv2.imread(str(frame_files[0]))\n",
    "    img_height, img_width = sample_img.shape[:2]\n",
    "    \n",
    "    # group by frame\n",
    "    for frame_name, group in annotations_df.groupby('frame'):\n",
    "        yolo_lines = []\n",
    "        \n",
    "        for _, ann in group.iterrows():\n",
    "            # convert to YOLO format (normalized)\n",
    "            x_center = ((ann['xtl'] + ann['xbr']) / 2) / img_width\n",
    "            y_center = ((ann['ytl'] + ann['ybr']) / 2) / img_height\n",
    "            width = (ann['xbr'] - ann['xtl']) / img_width\n",
    "            height = (ann['ybr'] - ann['ytl']) / img_height\n",
    "            \n",
    "            # class mapping\n",
    "            class_map = {'car': 0, 'truck': 1, 'bus': 2}\n",
    "            class_id = class_map.get(ann['label'], 0)\n",
    "            \n",
    "            yolo_lines.append(f\"{class_id} {x_center} {y_center} {width} {height}\")\n",
    "        \n",
    "        # save label file\n",
    "        label_path = labels_dir / frame_name.replace('.jpg', '.txt')\n",
    "        with open(label_path, 'w') as f:\n",
    "            f.write('\\n'.join(yolo_lines))\n",
    "    \n",
    "    print(f\"‚úì Dataset prepared\")\n",
    "    print(f\"  Images: {len(list(images_dir.glob('*.jpg')))}\")\n",
    "    print(f\"  Labels: {len(list(labels_dir.glob('*.txt')))}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No annotations found. Complete CVAT annotation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e02db54-44fb-49b0-b9b6-67a4cd53da58",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## üöÄ Train YOLO Model\n",
    "\n",
    "Train YOLOv8 on the annotated car counting dataset.\n",
    "\n",
    "The following cell trains the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "da8ffd64-414b-48b6-bc45-e6dbba7f44fd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Install ultralytics first:\n",
      "  pip install ultralytics\n"
     ]
    }
   ],
   "source": [
    "# train YOLO model\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "    \n",
    "    # load model\n",
    "    model = YOLO(TRAIN_CONFIG['model'])\n",
    "    \n",
    "    # train\n",
    "    results = model.train(\n",
    "        data=TRAIN_CONFIG['data_yaml'],\n",
    "        epochs=TRAIN_CONFIG['epochs'],\n",
    "        imgsz=TRAIN_CONFIG['imgsz'],\n",
    "        batch=TRAIN_CONFIG['batch'],\n",
    "        project=TRAIN_CONFIG['project'],\n",
    "        name=TRAIN_CONFIG['name']\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Training complete\")\n",
    "    print(f\"  Best model saved to: {TRAIN_CONFIG['project']}/{TRAIN_CONFIG['name']}/weights/best.pt\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Install ultralytics first:\")\n",
    "    print(\"  pip install ultralytics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00706991-7122-4f1f-9cc0-9fb68ea24668",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## üß™ Test Model\n",
    "\n",
    "Run inference on sample frames to verify model performance.\n",
    "\n",
    "The following cell tests the trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f564a552-8e69-4229-9f90-790bd4c991c6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Model not trained yet. Run training cell first.\n"
     ]
    }
   ],
   "source": [
    "# test model on sample frames\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if 'model' in locals():\n",
    "    # test on sample frames\n",
    "    test_frames = list(images_dir.glob('*.jpg'))[:5]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, min(5, len(test_frames)), figsize=(20, 4))\n",
    "    if len(test_frames) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, frame_path in enumerate(test_frames):\n",
    "        # run inference\n",
    "        results = model(frame_path)\n",
    "        \n",
    "        # plot results\n",
    "        img_with_boxes = results[0].plot()\n",
    "        axes[i].imshow(cv2.cvtColor(img_with_boxes, cv2.COLOR_BGR2RGB))\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f\"Frame {i+1}\")\n",
    "        \n",
    "        # count vehicles\n",
    "        boxes = results[0].boxes\n",
    "        if boxes is not None:\n",
    "            car_count = sum(1 for box in boxes if box.cls == 0)\n",
    "            truck_count = sum(1 for box in boxes if box.cls == 1)\n",
    "            bus_count = sum(1 for box in boxes if box.cls == 2)\n",
    "            print(f\"{frame_path.name}: Cars={car_count}, Trucks={truck_count}, Buses={bus_count}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Model not trained yet. Run training cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7add8d-f94f-48dc-a6f2-998231583b52",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## üìä Run Full Inference\n",
    "\n",
    "Process all frames and save vehicle counts.\n",
    "\n",
    "The following cell runs inference on all frames and saves results.\n",
    "\n",
    "üöß **IMPLEMENTATION PENDING** üöß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6901a22e-a5f8-46be-aa5d-bb09a04ae1ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Model not available. Train first.\n"
     ]
    }
   ],
   "source": [
    "# run full inference and save counts\n",
    "if 'model' in locals():\n",
    "    all_counts = []\n",
    "    \n",
    "    # process all frames\n",
    "    for frame_path in sorted(images_dir.glob('*.jpg')):\n",
    "        results = model(frame_path)\n",
    "        \n",
    "        # count vehicles\n",
    "        counts = {'frame': frame_path.name, 'car': 0, 'truck': 0, 'bus': 0}\n",
    "        \n",
    "        boxes = results[0].boxes\n",
    "        if boxes is not None:\n",
    "            for box in boxes:\n",
    "                if box.cls == 0:\n",
    "                    counts['car'] += 1\n",
    "                elif box.cls == 1:\n",
    "                    counts['truck'] += 1\n",
    "                elif box.cls == 2:\n",
    "                    counts['bus'] += 1\n",
    "        \n",
    "        all_counts.append(counts)\n",
    "    \n",
    "    # save counts\n",
    "    counts_df = pd.DataFrame(all_counts)\n",
    "    counts_path = annotation_dir / 'vehicle_counts.csv'\n",
    "    counts_df.to_csv(counts_path, index=False)\n",
    "    \n",
    "    print(f\"‚úì Inference complete\")\n",
    "    print(f\"  Processed {len(all_counts)} frames\")\n",
    "    print(f\"  Results saved to: {counts_path}\")\n",
    "    \n",
    "    # summary statistics\n",
    "    print(f\"\\nTotal counts:\")\n",
    "    print(f\"  Cars: {counts_df['car'].sum()}\")\n",
    "    print(f\"  Trucks: {counts_df['truck'].sum()}\")\n",
    "    print(f\"  Buses: {counts_df['bus'].sum()}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Model not available. Train first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fd9d80-1930-496e-9a56-9c457a1e6c3e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## ‚úÖ Complete Car Counting Workflow\n",
    "\n",
    "All steps completed in this notebook:\n",
    "1. ‚úì Preprocessed frames for experiments\n",
    "2. ‚úì Packaged frames for CVAT\n",
    "3. ‚úì Converted annotations to YOLO format\n",
    "4. ‚úì Trained YOLOv8 model\n",
    "5. ‚úì Generated vehicle counts\n",
    "\n",
    "**Output Files:**\n",
    "- `vehicle_counts.csv` - Frame-by-frame counts\n",
    "- `best.pt` - Trained model weights\n",
    "- `annotations.csv` - CVAT annotations\n",
    "\n",
    "üü¢ **EXPERIMENT COMPLETE** üü¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6faed09-a9a6-41eb-8f3a-761a50005e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
