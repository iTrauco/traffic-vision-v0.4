{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3d032a-3a9c-4f05-8a8d-b8b5cbc135ef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# Cell 1 - Workflow Tools\n",
    "import sys\n",
    "sys.path.insert(0, '../../lib')\n",
    "sys.path.insert(0, '../../scripts') \n",
    "\n",
    "from notebook_tools import TOCWidget, ExportWidget\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "# Create widget instances\n",
    "toc = TOCWidget()\n",
    "export = ExportWidget()\n",
    "\n",
    "# Create horizontal layout\n",
    "left_side = widgets.VBox([toc.button, export.button, toc.status])\n",
    "right_side = widgets.VBox([toc.output, export.output])\n",
    "\n",
    "# Display side by side\n",
    "display(widgets.HBox([left_side, right_side]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5aaf7c-5b3a-47b9-a099-522d8c877d8d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83d\udcd1 Table of Contents (Auto-Generated)\n",
    "\n",
    "This section will automatically generate a table of contents for your research notebook once you run the **Generate TOC** function. The table of contents will help you navigate through your data collection, analysis, and findings as your citizen science project develops.\n",
    "\n",
    "\u27a1\ufe0f **Do not edit this cell manually. It will be overwritten automatically.**\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "763e0c6f-4094-459d-b2db-2512e3c09a93",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "# \ud83e\uddea Experiment Preprocessing - [VERSION]\n",
    "\n",
    "## \ud83c\udfaf Purpose\n",
    "This notebook preprocesses GDOT traffic camera videos specifically for machine learning experiments. It follows the same workflow as general preprocessing but with experiment-specific parameters.\n",
    "\n",
    "## \ud83d\udccb Context\n",
    "- **Data Source**: Raw GDOT traffic camera recordings\n",
    "- **Output**: Frames optimized for experiment workflows\n",
    "- **Destination**: `data/preprocessing/experiments/`\n",
    "- **Next Steps**: Annotation \u2192 Training \u2192 Evaluation\n",
    "\n",
    "## \ud83d\udd04 Workflow Overview\n",
    "1. Video ingestion from recordings\n",
    "2. Frame extraction (experiment-specific rate)\n",
    "3. Quality control (experiment thresholds)\n",
    "4. Spatial transformations\n",
    "5. Export to experiments directory\n",
    "\n",
    "## \ud83d\udcda Notebook Structure\n",
    "- **Setup**: Environment and dependencies\n",
    "- **Processing**: Experiment-specific preprocessing\n",
    "- **Export**: Organized output for annotation\n",
    "\n",
    "*Processing completed: [DATE] | Version: [VERSION]*"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8f4e1fb7-e87c-46ed-8e2a-d1252806cc28",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83d\udd27 Experiment Configuration\n",
    "\n",
    "This cell defines parameters specific to experiment preprocessing. Values are optimized for machine learning workflows rather than general analysis.\n",
    "\n",
    "#### Problem Statement\n",
    "- \ud83c\udfaf **Objective**: Count vehicles in GDOT traffic camera feeds\n",
    "- \ud83c\udfaf **Challenge**: Detect and count cars, trucks, buses in various conditions\n",
    "- \ud83c\udfaf **Approach**: Preprocess frames specifically for vehicle detection training\n",
    "\n",
    "#### Target Parameters\n",
    "- \ud83c\udfaf **VIDEO_ID**: Specific camera to process\n",
    "- \ud83c\udfaf **BATCH_DATE**: Recording date (YYYYMMDD format)\n",
    "- \ud83c\udfaf **EXPERIMENT_TYPE**: 'car_counting'\n",
    "\n",
    "#### Path Configuration  \n",
    "- \ud83d\udcc1 **INPUT_BASE**: Root directory for video recordings\n",
    "- \ud83d\udcc1 **OUTPUT_BASE**: Root for experiment preprocessing (`data/preprocessing/experiments`)\n",
    "\n",
    "#### Experiment-Specific Settings\n",
    "- \ud83d\udcca **FRAMES_TO_EXTRACT**: Higher count for training data (1000+)\n",
    "- \ud83d\udcca **SAMPLE_RATE**: Denser sampling for better coverage (every 5 frames)\n",
    "- \ud83d\udd0d **INCLUDE_EDGE_CASES**: Keep some lower quality frames for model robustness\n",
    "- \ud83d\udd0d **MIN_VEHICLES_PER_FRAME**: Prefer frames with vehicles present\n",
    "\n",
    "The following cell initializes the experiment configuration with parameters optimized for vehicle counting.\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a31ac3-becb-4da4-a34a-088d3b700101",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# experiment configuration parameters\n",
    "from pathlib import Path\n",
    "\n",
    "CONFIG = {\n",
    "    # target parameters\n",
    "    'VIDEO_ID': 'ATL-1005',  # \ud83c\udfaf camera to process\n",
    "    'BATCH_DATE': '20250620',  # \ud83c\udfaf date from batch analysis\n",
    "    'TARGET_HOUR': 12,  # \ud83c\udfaf target hour (noon)\n",
    "    'EXPERIMENT_TYPE': 'car_counting',  # \ud83c\udfaf experiment type\n",
    "    \n",
    "    # path configuration  \n",
    "    'INPUT_BASE': Path.home() / 'traffic-recordings',  # \ud83d\udcc1 video source\n",
    "    'OUTPUT_BASE': Path('../../data/preprocessing/experiments'),  # \ud83d\udcc1 experiment output\n",
    "    \n",
    "    # processing settings\n",
    "    'FRAMES_TO_EXTRACT': 1000,  # \ud83d\udcca total frames to extract\n",
    "    'SAMPLE_RATE': 5,  # \ud83d\udcca extract every Nth frame\n",
    "    \n",
    "    # quality thresholds (relaxed for experiments)\n",
    "    'QUALITY_THRESHOLD': {\n",
    "        'brightness_min': 90,  # \ud83d\udd0d minimum brightness\n",
    "        'brightness_max': 130,  # \ud83d\udd0d maximum brightness  \n",
    "        'blur_min': 2500  # \ud83d\udd0d minimum blur score\n",
    "    },\n",
    "    \n",
    "    # video settings\n",
    "    'PREFERRED_CODEC': 'mp4v',  # \ud83c\udfa5 primary codec\n",
    "    'FALLBACK_CODECS': ['h264', 'xvid'],  # \ud83c\udfa5 alternatives\n",
    "    'MAX_FRAME_WIDTH': 1920,  # \ud83c\udfa5 max width\n",
    "    'MAX_FRAME_HEIGHT': 1080,  # \ud83c\udfa5 max height\n",
    "    'JPEG_QUALITY': 95  # \ud83c\udfa5 output quality (0-100)\n",
    "}\n",
    "\n",
    "# derived paths\n",
    "date_formatted = f\"{CONFIG['BATCH_DATE'][:4]}-{CONFIG['BATCH_DATE'][4:6]}-{CONFIG['BATCH_DATE'][6:8]}\"\n",
    "CONFIG['OUTPUT_DIR'] = CONFIG['OUTPUT_BASE'] / CONFIG['EXPERIMENT_TYPE'] / date_formatted / CONFIG['VIDEO_ID']\n",
    "CONFIG['VIDEO_DIR'] = CONFIG['INPUT_BASE'] / CONFIG['VIDEO_ID'] / date_formatted\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a216541a-bf02-4cca-80c7-b69ca1a02e81",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "*End of Experiment Configuration*\n",
    "\n",
    "---"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "88852afe-ef34-4ee1-aa69-814c96968d0a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83d\udd27 Environment Setup\n",
    "\n",
    "This cell establishes the preprocessing environment with the same core libraries as general preprocessing, plus experiment-specific additions.\n",
    "\n",
    "1. **Core Libraries**\n",
    "   - OpenCV for video processing\n",
    "   - NumPy for array operations\n",
    "   - Pandas for data organization\n",
    "   - Logging for process tracking\n",
    "\n",
    "2. **Same Helper Functions**\n",
    "   - calculate_brightness()\n",
    "   - calculate_blur_score()\n",
    "   - get_video_metadata()\n",
    "\n",
    "3. **Experiment Additions**\n",
    "   - Vehicle detection helpers\n",
    "   - Frame selection priorities\n",
    "   - Experiment metadata tracking\n",
    "\n",
    "The following cell imports libraries and initializes the preprocessing environment.\n",
    "\n",
    "\ud83d\udfe2 **IMPLEMENTATION COMPLETE** \ud83d\udfe2"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef92501-2f9c-46a9-8ee4-af94872fc057",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# environment setup\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# verify opencv\n",
    "print(f\"\u2713 OpenCV version: {cv2.__version__}\")\n",
    "print(f\"\u2713 Python version: {sys.version.split()[0]}\")\n",
    "print(f\"\u2713 NumPy version: {np.__version__}\")\n",
    "print(f\"\u2713 Pandas version: {pd.__version__}\")\n",
    "\n",
    "# helper functions\n",
    "def calculate_brightness(frame):\n",
    "    \"\"\"Calculate average brightness of frame\"\"\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    return np.mean(gray)\n",
    "\n",
    "def calculate_blur_score(frame):\n",
    "    \"\"\"Calculate Laplacian variance (higher = sharper)\"\"\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    return cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "\n",
    "def get_video_metadata(video_path):\n",
    "    \"\"\"Extract video metadata\"\"\"\n",
    "    metadata = {}\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if cap.isOpened():\n",
    "        metadata['fps'] = cap.get(cv2.CAP_PROP_FPS)\n",
    "        metadata['frame_count'] = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        metadata['width'] = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        metadata['height'] = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        metadata['duration_seconds'] = metadata['frame_count'] / metadata['fps'] if metadata['fps'] > 0 else 0\n",
    "        metadata['codec'] = int(cap.get(cv2.CAP_PROP_FOURCC))\n",
    "        cap.release()\n",
    "    return metadata\n",
    "\n",
    "# create output directory\n",
    "CONFIG['OUTPUT_DIR'].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n\u2713 Environment setup complete\")\n",
    "print(f\"  Output directory created: {CONFIG['OUTPUT_DIR']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c59502-a557-4b79-90ff-405dc43d3d83",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "*End of Environment Setup*\n",
    "\n",
    "---"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a65fdcf4-5a4e-473c-9c93-54e3adbbb308",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83d\udd04 Progress Tracking & Checkpoint System\n",
    "\n",
    "The following cells implement simple progress tracking and checkpoint functionality to:\n",
    "\n",
    "1. **Track Processing Progress**\n",
    "   - Monitor experiment preprocessing status\n",
    "   - Count frames extracted and processed\n",
    "   - Display elapsed time\n",
    "\n",
    "2. **Enable Restart Capability**\n",
    "   - Save progress after each stage\n",
    "   - Automatically resume from last checkpoint\n",
    "   - Maintain experiment metadata\n",
    "\n",
    "This ensures experiment preprocessing can be resumed if interrupted.\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "de8437e3-9c32-4447-993c-1103c07336ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83d\udcbe Initialize Checkpoint and Progress Tracking Functions\n",
    "\n",
    "This module establishes checkpoint and progress tracking for experiment preprocessing. Functions track which videos have been processed for experiments and enable recovery from interruptions.\n",
    "\n",
    "The following cell sets up checkpoint functionality specific to experiment preprocessing.\n",
    "\n",
    "\ud83d\udfe2 **IMPLEMENTATION COMPLETE** \ud83d\udfe2"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d42ad75-0f3f-40b8-833b-a43efb91f417",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# checkpoint and progress tracking\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "CHECKPOINT_FILE = CONFIG['OUTPUT_DIR'] / \"experiment_checkpoint.json\"\n",
    "start_time = time.time()\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"Load previous progress if it exists\"\"\"\n",
    "    if CHECKPOINT_FILE.exists():\n",
    "        with open(CHECKPOINT_FILE, 'r') as f:\n",
    "            checkpoint = json.load(f)\n",
    "            print(f\"\u2713 Loaded checkpoint: {checkpoint.get('stage', 'unknown')} stage\")\n",
    "            return checkpoint\n",
    "    return {\n",
    "        \"stage\": \"started\",\n",
    "        \"processed\": {}, \n",
    "        \"start_time\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "def save_checkpoint(checkpoint):\n",
    "    \"\"\"Save current progress\"\"\"\n",
    "    checkpoint['last_updated'] = datetime.now().isoformat()\n",
    "    with open(CHECKPOINT_FILE, 'w') as f:\n",
    "        json.dump(checkpoint, f, indent=2)\n",
    "\n",
    "def update_progress(stage, details=None):\n",
    "    \"\"\"Update and save progress\"\"\"\n",
    "    checkpoint = load_checkpoint()\n",
    "    checkpoint['stage'] = stage\n",
    "    if details:\n",
    "        checkpoint['processed'][stage] = details\n",
    "    save_checkpoint(checkpoint)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Stage: {stage}\")\n",
    "    print(f\"Elapsed: {elapsed/60:.1f}min\")\n",
    "\n",
    "# Initialize checkpoint\n",
    "checkpoint = load_checkpoint()\n",
    "print(f\"Ready to process experiment data. Checkpoint initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb33b27-4381-476f-a4b7-31f366165abb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83d\udcf9 Video Ingestion & Cataloging\n",
    "\n",
    "This module loads video files for experiment preprocessing with focus on selecting videos that contain vehicle activity.\n",
    "\n",
    "1. **Find Video Files**\n",
    "   - Locate videos for specified camera and date\n",
    "   - Parse timestamps from filenames\n",
    "   - Select videos during high-traffic periods\n",
    "\n",
    "2. **Prioritize for Experiments**\n",
    "   - Prefer daytime hours (better visibility)\n",
    "   - Avoid night/dawn/dusk for initial experiments\n",
    "   - Focus on peak traffic times\n",
    "\n",
    "3. **Extract Metadata**"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17b5c7e-9c6f-4723-be2e-2542ba0fb936",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# video ingestion and cataloging\n",
    "def parse_timestamp(filename):\n",
    "    \"\"\"extract timestamp from filename\"\"\"\n",
    "    parts = filename.stem.split('_')\n",
    "    if len(parts) >= 3:\n",
    "        time_str = parts[2]\n",
    "        hours = int(time_str[:2])\n",
    "        minutes = int(time_str[2:4])\n",
    "        return hours * 60 + minutes  # minutes from midnight\n",
    "    return None\n",
    "\n",
    "# find videos\n",
    "video_files = list(CONFIG['VIDEO_DIR'].glob(f\"{CONFIG['VIDEO_ID']}_*.mp4\"))\n",
    "\n",
    "if not video_files:\n",
    "    raise FileNotFoundError(f\"No videos found for {CONFIG['VIDEO_ID']} on {CONFIG['BATCH_DATE']}\")\n",
    "\n",
    "# find closest to noon\n",
    "target_minutes = CONFIG['TARGET_HOUR'] * 60  # 720 minutes\n",
    "closest_video = None\n",
    "min_diff = float('inf')\n",
    "\n",
    "for video in video_files:\n",
    "    minutes = parse_timestamp(video)\n",
    "    if minutes is not None:\n",
    "        diff = abs(minutes - target_minutes)\n",
    "        if diff < min_diff:\n",
    "            min_diff = diff\n",
    "            closest_video = video\n",
    "\n",
    "CONFIG['selected_video'] = closest_video\n",
    "time_str = closest_video.stem.split('_')[2]\n",
    "print(f\"Selected: {closest_video.name}\")\n",
    "print(f\"  Starts at: {time_str[:2]}:{time_str[2:4]}:{time_str[4:6]}\")\n",
    "\n",
    "# update checkpoint\n",
    "update_progress('video_selected', {'video': closest_video.name})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39785d35-172d-4087-87ad-57f73bc573a9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83c\udf9e\ufe0f Frame Extraction\n",
    "\n",
    "This module samples frames from video sequences with parameters optimized for experiment training data.\n",
    "\n",
    "1. **Experiment-Specific Extraction**\n",
    "   - Higher frame count for training diversity\n",
    "   - Denser sampling rate (every 5 frames vs 15)\n",
    "   - Extract from multiple time periods\n",
    "\n",
    "2. **Quality Over Compression**\n",
    "   - Higher JPEG quality for annotation clarity\n",
    "   - Full resolution preservation\n",
    "   - No aggressive downsampling\n",
    "\n",
    "3. **Metadata Tracking**\n",
    "   - Frame timestamp mapping\n",
    "   - Source video reference\n",
    "   - Frame sequence numbering\n",
    "\n",
    "The following cell extracts frames using experiment-optimized parameters.\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa51292-a8ee-43bc-915e-4478fe95be5a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# frame extraction\n",
    "print(f\"Frame Extraction\")\n",
    "print(f\"Extracting {CONFIG['FRAMES_TO_EXTRACT']} frames (every {CONFIG['SAMPLE_RATE']} frames)\")\n",
    "\n",
    "video_path = CONFIG['selected_video']\n",
    "cap = cv2.VideoCapture(str(video_path))\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise ValueError(f\"Cannot open video: {video_path}\")\n",
    "\n",
    "# create frames directory\n",
    "frames_dir = CONFIG['OUTPUT_DIR'] / 'frames'\n",
    "frames_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# extract frames\n",
    "frames_extracted = 0\n",
    "frame_index = 0\n",
    "\n",
    "while frames_extracted < CONFIG['FRAMES_TO_EXTRACT'] and cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # extract every Nth frame\n",
    "    if frame_index % CONFIG['SAMPLE_RATE'] == 0:\n",
    "        frame_filename = f\"frame_{frames_extracted:04d}.jpg\"\n",
    "        frame_path = frames_dir / frame_filename\n",
    "        \n",
    "        # save frame\n",
    "        cv2.imwrite(str(frame_path), frame, [cv2.IMWRITE_JPEG_QUALITY, CONFIG['JPEG_QUALITY']])\n",
    "        \n",
    "        frames_extracted += 1\n",
    "        if frames_extracted % 100 == 0:\n",
    "            print(f\"  Extracted {frames_extracted}/{CONFIG['FRAMES_TO_EXTRACT']} frames\")\n",
    "    \n",
    "    frame_index += 1\n",
    "\n",
    "cap.release()\n",
    "\n",
    "CONFIG['frames_dir'] = frames_dir\n",
    "CONFIG['frames_extracted'] = frames_extracted\n",
    "\n",
    "print(f\"\\n\u2713 Extracted {frames_extracted} frames to {frames_dir}\")\n",
    "\n",
    "# update checkpoint\n",
    "update_progress('frames_extracted', {'count': frames_extracted})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfeae58-f6b6-4ba5-99a4-d04267561e96",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83d\udd0d Image Quality Control\n",
    "\n",
    "This module filters frames with experiment-specific quality thresholds that balance data quality with training diversity.\n",
    "\n",
    "1. **Relaxed Thresholds**\n",
    "   - Slightly lower brightness bounds (include dawn/dusk)\n",
    "   - Accept moderate blur (real-world conditions)\n",
    "   - Keep edge cases for model robustness\n",
    "\n",
    "2. **Vehicle Presence Priority**\n",
    "   - Prioritize frames with motion\n",
    "   - Check for object-like shapes\n",
    "   - Balance empty vs occupied frames\n",
    "\n",
    "3. **Training Set Diversity**\n",
    "   - Include various lighting conditions\n",
    "   - Keep some challenging frames\n",
    "   - Document quality distribution\n",
    "\n",
    "The following cell applies quality filtering optimized for ML training diversity."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265986d5-0215-4b48-bf18-59a3423414ec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# image quality control\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Image Quality Control\")\n",
    "\n",
    "# get all extracted frames\n",
    "frame_files = sorted(CONFIG['frames_dir'].glob(\"frame_*.jpg\"))\n",
    "print(f\"Checking quality of {len(frame_files)} frames\")\n",
    "\n",
    "quality_results = []\n",
    "good_frames = []\n",
    "poor_frames = []\n",
    "\n",
    "for frame_path in frame_files:\n",
    "    # read frame\n",
    "    frame = cv2.imread(str(frame_path))\n",
    "    if frame is None:\n",
    "        poor_frames.append(frame_path)\n",
    "        continue\n",
    "    \n",
    "    # calculate metrics\n",
    "    brightness = calculate_brightness(frame)\n",
    "    blur_score = calculate_blur_score(frame)\n",
    "    \n",
    "    # check thresholds\n",
    "    passes_quality = (\n",
    "        brightness >= CONFIG['QUALITY_THRESHOLD']['brightness_min'] and\n",
    "        brightness <= CONFIG['QUALITY_THRESHOLD']['brightness_max'] and\n",
    "        blur_score >= CONFIG['QUALITY_THRESHOLD']['blur_min']\n",
    "    )\n",
    "    \n",
    "    result = {\n",
    "        'frame': frame_path.name,\n",
    "        'brightness': brightness,\n",
    "        'blur_score': blur_score,\n",
    "        'passes': passes_quality\n",
    "    }\n",
    "    quality_results.append(result)\n",
    "    \n",
    "    if passes_quality:\n",
    "        good_frames.append(frame_path)\n",
    "    else:\n",
    "        poor_frames.append(frame_path)\n",
    "\n",
    "# save quality report\n",
    "quality_df = pd.DataFrame(quality_results)\n",
    "quality_df.to_csv(CONFIG['OUTPUT_DIR'] / 'quality_report.csv', index=False)\n",
    "\n",
    "CONFIG['good_frames'] = good_frames\n",
    "CONFIG['quality_results'] = quality_df\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Good frames: {len(good_frames)}\")\n",
    "print(f\"  Poor frames: {len(poor_frames)}\")\n",
    "print(f\"  Pass rate: {len(good_frames)/len(frame_files)*100:.1f}%\")\n",
    "\n",
    "# update checkpoint\n",
    "update_progress('quality_control', {'good': len(good_frames), 'poor': len(poor_frames)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6cb5db-1880-47d4-844c-0a92b3321336",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83d\udcd0 Spatial Transformations\n",
    "\n",
    "This module resizes frames if needed. Same as general preprocessing.\n",
    "\n",
    "The following cell applies standard transformations.\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee01c97-f98b-45b2-a43a-92edceaaecaf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# spatial transformations\n",
    "print(\"Spatial Transformations\")\n",
    "\n",
    "frames_to_transform = CONFIG['good_frames']\n",
    "print(f\"Transforming {len(frames_to_transform)} frames\")\n",
    "\n",
    "# create transformed directory\n",
    "transformed_dir = CONFIG['OUTPUT_DIR'] / 'transformed'\n",
    "transformed_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# target dimensions\n",
    "target_width = CONFIG['MAX_FRAME_WIDTH']\n",
    "target_height = CONFIG['MAX_FRAME_HEIGHT']\n",
    "\n",
    "transformed_frames = []\n",
    "\n",
    "for frame_path in frames_to_transform:\n",
    "    # read frame\n",
    "    frame = cv2.imread(str(frame_path))\n",
    "    height, width = frame.shape[:2]\n",
    "    \n",
    "    # resize if needed\n",
    "    if width > target_width or height > target_height:\n",
    "        scale = min(target_width/width, target_height/height)\n",
    "        new_width = int(width * scale)\n",
    "        new_height = int(height * scale)\n",
    "        frame = cv2.resize(frame, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # save transformed frame\n",
    "    output_path = transformed_dir / frame_path.name\n",
    "    cv2.imwrite(str(output_path), frame, [cv2.IMWRITE_JPEG_QUALITY, CONFIG['JPEG_QUALITY']])\n",
    "    transformed_frames.append(output_path)\n",
    "\n",
    "CONFIG['transformed_frames'] = transformed_frames\n",
    "print(f\"\\n\u2713 Completed spatial transformations\")\n",
    "\n",
    "# update checkpoint\n",
    "update_progress('spatial_transformations', {'count': len(transformed_frames)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a738b3c-5df5-4a45-89c3-8fc32a585aaf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83c\udfa8 Color Space Normalization\n",
    "\n",
    "Convert BGR to RGB. Same as general preprocessing.\n",
    "\n",
    "The following cell normalizes color space.\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df442fee-39f3-4ac6-ac4d-8a1344e074b7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# color space normalization\n",
    "print(\"Color Space Normalization\")\n",
    "\n",
    "frames_to_normalize = CONFIG['transformed_frames']\n",
    "print(f\"Normalizing {len(frames_to_normalize)} frames\")\n",
    "\n",
    "# create normalized directory\n",
    "normalized_dir = CONFIG['OUTPUT_DIR'] / 'normalized'\n",
    "normalized_dir.mkdir(exist_ok=True)\n",
    "\n",
    "normalized_frames = []\n",
    "\n",
    "for frame_path in frames_to_normalize:\n",
    "    # read frame (BGR)\n",
    "    frame_bgr = cv2.imread(str(frame_path))\n",
    "    \n",
    "    # convert BGR to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # save as RGB\n",
    "    output_path = normalized_dir / frame_path.name\n",
    "    cv2.imwrite(str(output_path), cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR), \n",
    "                [cv2.IMWRITE_JPEG_QUALITY, CONFIG['JPEG_QUALITY']])\n",
    "    normalized_frames.append(output_path)\n",
    "\n",
    "CONFIG['normalized_frames'] = normalized_frames\n",
    "print(f\"\\n\u2713 Color normalization complete\")\n",
    "\n",
    "# update checkpoint\n",
    "update_progress('color_normalization', {'count': len(normalized_frames)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d2b46c-5faf-4265-98e7-357d8489edcd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \u23f1\ufe0f Temporal Downsampling\n",
    "\n",
    "For experiments, we keep ALL frames with vehicles (no downsampling). This maximizes training data.\n",
    "\n",
    "The following cell identifies and keeps frames with motion."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448a9f52-9a08-4e27-8323-0a17a624f92d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# temporal downsampling (keep frames with motion)\n",
    "print(\"Temporal Downsampling\")\n",
    "\n",
    "frames_to_analyze = CONFIG['normalized_frames']\n",
    "print(f\"Analyzing {len(frames_to_analyze)} frames for motion\")\n",
    "\n",
    "# motion detection\n",
    "motion_scores = []\n",
    "\n",
    "for i in range(len(frames_to_analyze) - 1):\n",
    "    frame1 = cv2.imread(str(frames_to_analyze[i]), cv2.IMREAD_GRAYSCALE)\n",
    "    frame2 = cv2.imread(str(frames_to_analyze[i+1]), cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # calculate difference\n",
    "    diff = cv2.absdiff(frame1, frame2)\n",
    "    motion_score = np.mean(diff)\n",
    "    \n",
    "    motion_scores.append({\n",
    "        'frame': frames_to_analyze[i].name,\n",
    "        'motion_score': motion_score,\n",
    "        'has_motion': motion_score > 5.0\n",
    "    })\n",
    "\n",
    "# for experiments, keep ALL frames with motion\n",
    "motion_df = pd.DataFrame(motion_scores)\n",
    "frames_with_motion = motion_df[motion_df['has_motion']]['frame'].tolist()\n",
    "\n",
    "# create downsampled directory\n",
    "downsampled_dir = CONFIG['OUTPUT_DIR'] / 'downsampled'\n",
    "downsampled_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# copy all frames with motion\n",
    "downsampled_frames = []\n",
    "for frame_name in frames_with_motion:\n",
    "    src = normalized_dir / frame_name\n",
    "    dst = downsampled_dir / frame_name\n",
    "    frame = cv2.imread(str(src))\n",
    "    cv2.imwrite(str(dst), frame, [cv2.IMWRITE_JPEG_QUALITY, CONFIG['JPEG_QUALITY']])\n",
    "    downsampled_frames.append(dst)\n",
    "\n",
    "CONFIG['downsampled_frames'] = downsampled_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1311f4-e54a-46fb-a976-1d87724cdd41",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# data organization\n",
    "print(\"Data Organization\")\n",
    "\n",
    "# compile metadata\n",
    "metadata = {\n",
    "    'camera_id': CONFIG['VIDEO_ID'],\n",
    "    'batch_date': CONFIG['BATCH_DATE'],\n",
    "    'experiment_type': CONFIG['EXPERIMENT_TYPE'],\n",
    "    'source_video': CONFIG['selected_video'].name,\n",
    "    'processing_timestamp': datetime.now().isoformat(),\n",
    "    'parameters': {\n",
    "        'target_hour': CONFIG['TARGET_HOUR'],\n",
    "        'frames_extracted': CONFIG['FRAMES_TO_EXTRACT'],\n",
    "        'sample_rate': CONFIG['SAMPLE_RATE'],\n",
    "        'quality_thresholds': CONFIG['QUALITY_THRESHOLD'],\n",
    "        'jpeg_quality': CONFIG['JPEG_QUALITY']\n",
    "    },\n",
    "    'processing_summary': {\n",
    "        'frames_extracted': CONFIG['frames_extracted'],\n",
    "        'frames_good_quality': len(CONFIG['good_frames']),\n",
    "        'frames_with_motion': len(CONFIG['downsampled_frames'])\n",
    "    }\n",
    "}\n",
    "\n",
    "# save metadata\n",
    "metadata_path = CONFIG['OUTPUT_DIR'] / 'metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "# save frame inventory\n",
    "frame_inventory = []\n",
    "for frame_path in CONFIG['downsampled_frames']:\n",
    "    frame_inventory.append({\n",
    "        'frame_name': frame_path.name,\n",
    "        'path': str(frame_path)\n",
    "    })\n",
    "\n",
    "inventory_df = pd.DataFrame(frame_inventory)\n",
    "inventory_path = CONFIG['OUTPUT_DIR'] / 'frame_inventory.csv'\n",
    "inventory_df.to_csv(inventory_path, index=False)\n",
    "\n",
    "CONFIG['metadata'] = metadata\n",
    "CONFIG['inventory'] = inventory_df\n",
    "\n",
    "print(f\"\\n\u2713 Data organization complete\")\n",
    "print(f\"  Metadata saved: {metadata_path}\")\n",
    "print(f\"  Frame inventory: {inventory_path}\")\n",
    "\n",
    "# update checkpoint\n",
    "update_progress('data_organization', {'frames_cataloged': len(frame_inventory)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f2d029-757c-4460-85d2-e7bfb34cf959",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83d\udcbe Export & Storage\n",
    "\n",
    "Creates final summary and confirms frames are ready for annotation.\n",
    "\n",
    "The following cell saves experiment preprocessing summary.\n",
    "\n",
    "\ud83d\udea7 **IMPLEMENTATION PENDING** \ud83d\udea7"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93a74c1-56b2-4e6e-b92c-d987439cd11b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# export and storage\n",
    "print(\"Export & Storage Summary\")\n",
    "\n",
    "# create summary report\n",
    "summary = {\n",
    "    'experiment_preprocessing_complete': datetime.now().isoformat(),\n",
    "    'experiment_type': CONFIG['EXPERIMENT_TYPE'],\n",
    "    'camera': CONFIG['VIDEO_ID'],\n",
    "    'video_processed': CONFIG['selected_video'].name,\n",
    "    'frames_ready_for_annotation': len(CONFIG['downsampled_frames']),\n",
    "    'annotation_directory': str(CONFIG['OUTPUT_DIR'] / 'downsampled'),\n",
    "    'processing_stages': {\n",
    "        '1_extracted': CONFIG['frames_extracted'],\n",
    "        '2_quality_filtered': len(CONFIG['good_frames']),\n",
    "        '3_transformed': len(CONFIG['transformed_frames']),\n",
    "        '4_normalized': len(CONFIG['normalized_frames']),\n",
    "        '5_motion_filtered': len(CONFIG['downsampled_frames'])\n",
    "    }\n",
    "}\n",
    "\n",
    "# save summary\n",
    "summary_path = CONFIG['OUTPUT_DIR'] / 'experiment_preprocessing_summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nExperiment Preprocessing Complete\")\n",
    "print(f\"Camera: {CONFIG['VIDEO_ID']}\")\n",
    "print(f\"Frames ready: {len(CONFIG['downsampled_frames'])}\")\n",
    "print(f\"Location: {CONFIG['OUTPUT_DIR'] / 'downsampled'}\")\n",
    "print(f\"Summary: {summary_path}\")\n",
    "\n",
    "# update checkpoint\n",
    "update_progress('complete', summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb275bd-098a-4640-bb9b-f63b796c22bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83c\udff7\ufe0f Prepare Frames for CVAT Annotation\n",
    "\n",
    "This module packages the preprocessed frames for upload to CVAT (Computer Vision Annotation Tool).\n",
    "\n",
    "The following cell creates a zip file of frames for CVAT import.\n",
    "\n",
    "\ud83d\udea7 **IMPLEMENTATION PENDING** \ud83d\udea7"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e32059-6a1b-4407-b42f-2d3f67f41dfc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# prepare frames for CVAT\n",
    "import zipfile\n",
    "\n",
    "frames_to_annotate = CONFIG['OUTPUT_DIR'] / 'downsampled'\n",
    "frame_files = sorted(frames_to_annotate.glob('*.jpg'))\n",
    "\n",
    "# create annotation directory\n",
    "annotation_dir = CONFIG['OUTPUT_DIR'].parent.parent.parent / 'annotations' / CONFIG['EXPERIMENT_TYPE'] / date_formatted / CONFIG['VIDEO_ID']\n",
    "annotation_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# zip frames for CVAT\n",
    "zip_path = annotation_dir / f\"{CONFIG['VIDEO_ID']}_frames_for_cvat.zip\"\n",
    "with zipfile.ZipFile(zip_path, 'w') as zf:\n",
    "    for frame in frame_files:\n",
    "        zf.write(frame, frame.name)\n",
    "\n",
    "print(f\"\u2713 Created CVAT upload file: {zip_path}\")\n",
    "print(f\"  Contains {len(frame_files)} frames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c9e0f9-a466-48e4-94c7-aaa69937f67c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83d\udc33 CVAT Docker Setup\n",
    "\n",
    "Run CVAT locally using Docker:\n",
    "\n",
    "```bash\n",
    "# Start CVAT\n",
    "docker run -d --name cvat -p 8080:8080 openvino/cvat\n",
    "\n",
    "# Access at http://localhost:8080\n",
    "# Default: username=admin, password=admin\n",
    "```\n",
    "\n",
    "1. Create new task\n",
    "2. Upload the zip file\n",
    "3. Set labels: car, truck, bus\n",
    "4. Annotate with bounding boxes\n",
    "5. Export as CVAT XML\n",
    "\n",
    "\ud83d\udea7 **MANUAL PROCESS** \ud83d\udea7"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "91cfb551-d8af-4feb-9028-2296020ab057",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83d\udce5 Convert CVAT Annotations\n",
    "\n",
    "This module converts CVAT XML output to training CSV format.\n",
    "\n",
    "The following cell processes CVAT annotations.\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457aec75-f4bc-443e-a88f-96e73c9687c6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# convert CVAT annotations to CSV\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "# path to CVAT XML export (update after annotation)\n",
    "cvat_xml_path = annotation_dir / 'annotations.xml'\n",
    "\n",
    "if cvat_xml_path.exists():\n",
    "    # parse CVAT XML\n",
    "    tree = ET.parse(cvat_xml_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    annotations = []\n",
    "    \n",
    "    # extract annotations\n",
    "    for image in root.findall('.//image'):\n",
    "        frame_name = image.get('name')\n",
    "        \n",
    "        for box in image.findall('.//box'):\n",
    "            annotations.append({\n",
    "                'frame': frame_name,\n",
    "                'label': box.get('label'),\n",
    "                'xtl': float(box.get('xtl')),\n",
    "                'ytl': float(box.get('ytl')),\n",
    "                'xbr': float(box.get('xbr')),\n",
    "                'ybr': float(box.get('ybr'))\n",
    "            })\n",
    "    \n",
    "    # save as CSV\n",
    "    annotations_df = pd.DataFrame(annotations)\n",
    "    csv_path = annotation_dir / 'annotations.csv'\n",
    "    annotations_df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"\u2713 Converted {len(annotations)} annotations\")\n",
    "    print(f\"  Saved to: {csv_path}\")\n",
    "    \n",
    "    # summary\n",
    "    print(f\"\\nAnnotation summary:\")\n",
    "    print(annotations_df['label'].value_counts())\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f No CVAT XML file found. Complete annotation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef01d19-30dd-4660-8bef-b8bf46b464b3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \u2705 Annotation Workflow Complete\n",
    "\n",
    "At this point:\n",
    "1. Frames are preprocessed and packaged\n",
    "2. CVAT annotation can be performed\n",
    "3. Annotations are converted to CSV format\n",
    "\n",
    "**Next Steps:**\n",
    "- Use annotations.csv for training\n",
    "- Create v1_03_experiment_training_template.working.ipynb\n",
    "\n",
    "\ud83d\udfe2 **PREPROCESSING & ANNOTATION COMPLETE** \ud83d\udfe2"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}