{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b9d10c5-195d-41d2-b74c-12b3e06f642c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "# \ud83d\udcd3 Notebook Manager\n",
    "\n",
    "This cell initializes the widgets required for managing your research notebook. Please run the cell below to enable functionality for:\n",
    "- Exporting cells tagged with `export` into a `clean` notebook\n",
    "- Generating a dynamic Table of Contents (TOC)\n",
    "- Exporting the notebook to GitHub-compatible Markdown\n",
    "\n",
    "\u27a1\ufe0f **Be sure to execute the next cell before continuing with any editing or exporting.**"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95312728-5bf6-40b4-854e-fbc5ff4b14e1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell 1 - Workflow Tools\n",
    "import sys\n",
    "sys.path.insert(0, '../../lib')\n",
    "\n",
    "from notebook_tools import TOCWidget, ExportWidget\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "# Create widget instances\n",
    "toc = TOCWidget()\n",
    "export = ExportWidget()\n",
    "\n",
    "# Create horizontal layout\n",
    "left_side = widgets.VBox([toc.button, export.button, toc.status])\n",
    "right_side = widgets.VBox([toc.output, export.output])\n",
    "\n",
    "# Display side by side\n",
    "display(widgets.HBox([left_side, right_side]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877dad16-2ca4-4acd-acc0-064c3f5a8cd0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "<!-- TOC -->\n",
    "# Table of Contents\n",
    "\n",
    "- [\ud83d\udd27 Environment Setup](#\ud83d\udd27-environment-setup)\n",
    "- [\ud83d\udcd0 Batch Processing Configuration](#\ud83d\udcd0-batch-processing-configuration)\n",
    "  - [Parameters](#parameters)\n",
    "  - [Configuration Legend](#configuration-legend)\n",
    "  - [File Pattern](#file-pattern)\n",
    "- [\ud83d\udcbe Initialize Checkpoint System](#\ud83d\udcbe-initialize-checkpoint-system)\n",
    "  - [Features](#features)\n",
    "- [\ud83d\udcc2 Scan Video Directories](#\ud83d\udcc2-scan-video-directories)\n",
    "  - [Process](#process)\n",
    "- [\ud83c\udfaf Find Target Videos](#\ud83c\udfaf-find-target-videos)\n",
    "  - [Algorithm](#algorithm)\n",
    "  - [Metadata to Extract](#metadata-to-extract)\n",
    "- [\ud83d\udcbe Save Video Manifest](#\ud83d\udcbe-save-video-manifest)\n",
    "  - [Output Files](#output-files)\n",
    "- [\ud83c\udfac Preview Extraction Configuration](#\ud83c\udfac-preview-extraction-configuration)\n",
    "  - [Parameters](#parameters)\n",
    "- [\ud83c\udfa5 Extract Preview Frames](#\ud83c\udfa5-extract-preview-frames)\n",
    "  - [Process](#process)\n",
    "- [\ud83d\udcca Display Frame Previews](#\ud83d\udcca-display-frame-previews)\n",
    "- [\ud83d\udcc8 Quality Analysis & Recommendations](#\ud83d\udcc8-quality-analysis-&-recommendations)\n",
    "- [\ud83c\udfc6 Quality Samples](#\ud83c\udfc6-quality-samples)\n",
    "- [\ud83d\udcda Quality Metrics Reference](#\ud83d\udcda-quality-metrics-reference)\n",
    "  - [Brightness (Luminance)](#brightness-(luminance))\n",
    "  - [Blur Score (Laplacian Variance)](#blur-score-(laplacian-variance))\n",
    "  - [Quartile-Based Outlier Detection](#quartile-based-outlier-detection)\n",
    "  - [Color Space Conversion (BGR to RGB)](#color-space-conversion-(bgr-to-rgb))\n",
    "- [\ud83d\udcbe Export Selection - Option 1: Automatic](#\ud83d\udcbe-export-selection---option-1-automatic)\n",
    "- [\ud83d\udcbe Export Selection - Option 2: Manual](#\ud83d\udcbe-export-selection---option-2-manual)\n",
    "- [\ud83d\udcbe Export Selection - Option 3: All Cameras](#\ud83d\udcbe-export-selection---option-3-all-cameras)\n",
    "- [\ud83d\udcbe Save Selection Queue](#\ud83d\udcbe-save-selection-queue)\n",
    "- [\ud83d\udccb Batch Processing Summary](#\ud83d\udccb-batch-processing-summary)\n",
    "- [\ud83c\udfaf Final Video Selection](#\ud83c\udfaf-final-video-selection)\n",
    "\n",
    "<!-- /TOC -->\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "af0da59d-2595-41e5-8394-f466cd1de53a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83d\udd27 Environment Setup\n",
    "\n",
    "This cell establishes the batch preprocessing environment by:\n",
    "\n",
    "1. **Importing Required Libraries**\n",
    "  - OpenCV (cv2) for video processing and frame extraction\n",
    "  - NumPy for array operations\n",
    "  - Pandas for organizing metadata and results\n",
    "  - Pathlib for cross-platform file path handling\n",
    "  - JSON for checkpoint persistence\n",
    "  - Datetime for timestamp parsing and filtering\n",
    "  - Logging for process tracking\n",
    "\n",
    "2. **Setting System Paths**\n",
    "  - Adding mlops_ops modules to Python path\n",
    "  - Verifying access to preprocessing utilities\n",
    "\n",
    "3. **Initializing Checkpoint System**\n",
    "  - Loading any previous processing state\n",
    "  - Setting up progress tracking variables\n",
    "  - Establishing failure recovery mechanism\n",
    "\n",
    "**Note**: Run this cell first to ensure all dependencies are available before proceeding with batch processing."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bea106-187f-47dd-a300-44aec39f2005",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# environment setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# add mlops modules\n",
    "sys.path.insert(0, '../lib')\n",
    "\n",
    "# setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# check opencv availability\n",
    "try:\n",
    "    import cv2\n",
    "    print(f\"\u2713 OpenCV version: {cv2.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"\u26a0\ufe0f OpenCV not installed. Install with: pip install opencv-python\")\n",
    "\n",
    "print(f\"\u2713 Python version: {sys.version.split()[0]}\")\n",
    "print(f\"\u2713 Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70e2904-fcff-4d19-919b-cc63c029d0a8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83d\udcd0 Batch Processing Configuration\n",
    "\n",
    "Defines core parameters for the daily batch preprocessing workflow.\n",
    "\n",
    "### Parameters\n",
    "- **Target Time**: 12:00 PM (noon) in 24-hour format\n",
    "- **Date Filter**: Previous calendar day only  \n",
    "- **Frame Count**: Frames to extract per video\n",
    "- **Input Path**: Base recordings directory\n",
    "- **Output Path**: Organized output structure in `data/preprocessing/batch_analysis/YYYY-MM-DD/`\n",
    "\n",
    "### Configuration Legend\n",
    "- \ud83c\udfaf = Adjustable targeting parameter\n",
    "- \ud83d\udcca = Data processing setting\n",
    "- \ud83d\udcc1 = Path configuration\n",
    "\n",
    "### File Pattern\n",
    "Expected format: `CAMERA_YYYYMMDD_HHMMSS.mp4`"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d90cc9-659c-4536-9062-19f701c38f6b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# batch processing configuration\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "CONFIG = {\n",
    "    # time targeting\n",
    "    'TARGET_TIME': '120000',  # \ud83c\udfaf noon target time\n",
    "    'TARGET_HOUR': 12,\n",
    "    \n",
    "    # date filtering yesterday\n",
    "    'PROCESS_DATE': (datetime.now() - timedelta(days=1)).strftime('%Y%m%d'),\n",
    "    \n",
    "    # frame extraction\n",
    "    'FRAMES_PER_VIDEO': 10,  # \ud83d\udcca frames per video\n",
    "    \n",
    "    # paths\n",
    "    'INPUT_DIR': Path.home() / 'traffic-recordings',  # \ud83d\udcc1 source directory\n",
    "    'OUTPUT_BASE': Path('../../data/preprocessing/batch_analysis'),  # \ud83d\udcc1 output base\n",
    "    \n",
    "    # file pattern\n",
    "    'VIDEO_PATTERN': '*_{date}_*.mp4',\n",
    "    'FILENAME_FORMAT': '{camera}_{date}_{time}.mp4'\n",
    "}\n",
    "\n",
    "# create dated output with hyphens\n",
    "date_formatted = f\"{CONFIG['PROCESS_DATE'][:4]}-{CONFIG['PROCESS_DATE'][4:6]}-{CONFIG['PROCESS_DATE'][6:8]}\"\n",
    "date_dir = CONFIG['OUTPUT_BASE'] / date_formatted\n",
    "date_dir.mkdir(parents=True, exist_ok=True)\n",
    "CONFIG['OUTPUT_DIR'] = date_dir\n",
    "\n",
    "# display configuration\n",
    "print(\"Batch Processing Configuration:\")\n",
    "print(f\"  Target Date: {CONFIG['PROCESS_DATE']}\")\n",
    "print(f\"  Target Time: {CONFIG['TARGET_TIME']} (12:00:00)\")\n",
    "print(f\"  Frames per video: {CONFIG['FRAMES_PER_VIDEO']}\")\n",
    "print(f\"  Input: {CONFIG['INPUT_DIR']}\")\n",
    "print(f\"  Output: {CONFIG['OUTPUT_DIR']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ad1c64-a59c-4c60-b74c-788e94b80f74",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83d\udcbe Initialize Checkpoint System\n",
    "\n",
    "Sets up checkpoint functionality to track processing progress and enable recovery from interruptions.\n",
    "\n",
    "### Features\n",
    "- Saves state after each video\n",
    "- Detects previous runs\n",
    "- Validates checkpoint date matches current processing date\n",
    "- Enables resume from last successful video"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a30d389-c60b-44de-a4ed-d77dd92bf8d9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# checkpoint system initialization\n",
    "CHECKPOINT_FILE = CONFIG['OUTPUT_DIR'] / \"batch_checkpoint.json\"\n",
    "start_time = datetime.now()\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"load previous progress\"\"\"\n",
    "    if CHECKPOINT_FILE.exists():\n",
    "        with open(CHECKPOINT_FILE, 'r') as f:\n",
    "            checkpoint = json.load(f)\n",
    "            print(f\"\u2713 Loaded checkpoint: {len(checkpoint['processed'])} videos already processed\")\n",
    "            return checkpoint\n",
    "    return {\n",
    "        \"processed\": [], \n",
    "        \"failed\": [], \n",
    "        \"last_completed\": None,\n",
    "        \"process_date\": CONFIG['PROCESS_DATE'],\n",
    "        \"start_time\": start_time.isoformat()\n",
    "    }\n",
    "\n",
    "def save_checkpoint(checkpoint):\n",
    "    \"\"\"save current progress\"\"\"\n",
    "    checkpoint['last_updated'] = datetime.now().isoformat()\n",
    "    with open(CHECKPOINT_FILE, 'w') as f:\n",
    "        json.dump(checkpoint, f, indent=2)\n",
    "\n",
    "# initialize checkpoint\n",
    "checkpoint = load_checkpoint()\n",
    "\n",
    "# verify checkpoint date\n",
    "if checkpoint.get('process_date') != CONFIG['PROCESS_DATE']:\n",
    "    print(f\"\u26a0\ufe0f  Checkpoint is from {checkpoint.get('process_date')}, starting fresh for {CONFIG['PROCESS_DATE']}\")\n",
    "    checkpoint = {\n",
    "        \"processed\": [], \n",
    "        \"failed\": [], \n",
    "        \"last_completed\": None,\n",
    "        \"process_date\": CONFIG['PROCESS_DATE'],\n",
    "        \"start_time\": start_time.isoformat()\n",
    "    }\n",
    "\n",
    "print(f\"\u2713 Checkpoint system ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891b639d-f96f-4fc1-8eeb-791af329973a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83d\udcc2 Scan Video Directories\n",
    "\n",
    "Enumerates camera subdirectories and counts videos from the target date.\n",
    "\n",
    "### Process\n",
    "- Identifies all camera directories (ATL-*)\n",
    "- Checks for date-specific subdirectories\n",
    "- Counts available videos per camera\n",
    "- Reports missing recordings"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1e55d2-0fc3-46b5-b00e-e5c4792c1ab8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# scan video directories\n",
    "camera_dirs = sorted([d for d in CONFIG['INPUT_DIR'].iterdir() if d.is_dir() and d.name.startswith('ATL-')])\n",
    "print(f\"Found {len(camera_dirs)} camera directories\\n\")\n",
    "\n",
    "# count videos per camera\n",
    "video_counts = {}\n",
    "date_folder = CONFIG['PROCESS_DATE'][:4] + '-' + CONFIG['PROCESS_DATE'][4:6] + '-' + CONFIG['PROCESS_DATE'][6:8]\n",
    "pattern = CONFIG['VIDEO_PATTERN'].format(date=CONFIG['PROCESS_DATE'])\n",
    "\n",
    "for cam_dir in camera_dirs:\n",
    "    date_dir = cam_dir / date_folder\n",
    "    if date_dir.exists():\n",
    "        videos = list(date_dir.glob(pattern))\n",
    "        video_counts[cam_dir.name] = len(videos)\n",
    "        \n",
    "        if len(videos) == 0:\n",
    "            print(f\"\u26a0\ufe0f  {cam_dir.name}: No videos in {date_folder}\")\n",
    "        else:\n",
    "            print(f\"\u2713 {cam_dir.name}: {len(videos)} videos\")\n",
    "    else:\n",
    "        video_counts[cam_dir.name] = 0\n",
    "        print(f\"\u26a0\ufe0f  {cam_dir.name}: No {date_folder} directory\")\n",
    "\n",
    "total_videos = sum(video_counts.values())\n",
    "print(f\"\\nTotal videos available: {total_videos}\")\n",
    "print(f\"Cameras with recordings: {sum(1 for v in video_counts.values() if v > 0)}/{len(camera_dirs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f988f5f6-c80b-4dba-9d01-903d07e6ed6c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83c\udfaf Find Target Videos\n",
    "\n",
    "Identifies the video closest to noon (12:00:00) for each camera and extracts metadata.\n",
    "\n",
    "### Algorithm\n",
    "- Parses timestamp from filename\n",
    "- Calculates time difference from noon\n",
    "- Selects minimum difference per camera\n",
    "\n",
    "### Metadata to Extract\n",
    "- Video duration\n",
    "- Frame rate (fps)\n",
    "- Resolution (width x height)\n",
    "- Total frame count\n",
    "- File size"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e74452-cdee-4780-8c78-a092478a1661",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# find target videos\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_timestamp(filename):\n",
    "    \"\"\"extract timestamp from filename\"\"\"\n",
    "    parts = filename.stem.split('_')\n",
    "    if len(parts) >= 3:\n",
    "        time_str = parts[2]\n",
    "        hours = int(time_str[:2])\n",
    "        minutes = int(time_str[2:4])\n",
    "        seconds = int(time_str[4:6])\n",
    "        return hours * 60 + minutes  # minutes from midnight\n",
    "    return None\n",
    "\n",
    "def find_closest_to_noon(video_list):\n",
    "    \"\"\"find video closest to noon\"\"\"\n",
    "    target_minutes = CONFIG['TARGET_HOUR'] * 60  # 720 minutes\n",
    "    \n",
    "    closest_video = None\n",
    "    min_diff = float('inf')\n",
    "    \n",
    "    for video in video_list:\n",
    "        minutes = parse_timestamp(video)\n",
    "        if minutes is not None:\n",
    "            diff = abs(minutes - target_minutes)\n",
    "            if diff < min_diff:\n",
    "                min_diff = diff\n",
    "                closest_video = video\n",
    "    \n",
    "    return closest_video, min_diff\n",
    "\n",
    "def get_video_metadata(video_path):\n",
    "    \"\"\"extract video metadata\"\"\"\n",
    "    metadata = {}\n",
    "    if cv2:\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        if cap.isOpened():\n",
    "            metadata['fps'] = cap.get(cv2.CAP_PROP_FPS)\n",
    "            metadata['frame_count'] = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            metadata['width'] = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "            metadata['height'] = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            metadata['duration_seconds'] = metadata['frame_count'] / metadata['fps'] if metadata['fps'] > 0 else 0\n",
    "            cap.release()\n",
    "    \n",
    "    metadata['file_size_mb'] = round(video_path.stat().st_size / (1024*1024), 2)\n",
    "    return metadata\n",
    "\n",
    "# find target videos\n",
    "target_videos = []\n",
    "date_folder = CONFIG['PROCESS_DATE'][:4] + '-' + CONFIG['PROCESS_DATE'][4:6] + '-' + CONFIG['PROCESS_DATE'][6:8]\n",
    "\n",
    "print(f\"Finding videos closest to {CONFIG['TARGET_TIME'][:2]}:{CONFIG['TARGET_TIME'][2:4]} (noon)...\\n\")\n",
    "\n",
    "for cam_dir in camera_dirs:\n",
    "    date_dir = cam_dir / date_folder\n",
    "    if date_dir.exists():\n",
    "        videos = list(date_dir.glob(f\"{cam_dir.name}_*.mp4\"))\n",
    "        if videos:\n",
    "            closest, diff_minutes = find_closest_to_noon(videos)\n",
    "            if closest:\n",
    "                metadata = get_video_metadata(closest)\n",
    "                target_videos.append({\n",
    "                    'camera': cam_dir.name,\n",
    "                    'video_path': closest,\n",
    "                    'time_diff_minutes': diff_minutes,\n",
    "                    **metadata\n",
    "                })\n",
    "                time_str = closest.stem.split('_')[2]\n",
    "                print(f\"{cam_dir.name}:\")\n",
    "                print(f\"  Video starts: {time_str[:2]}:{time_str[2:4]}:{time_str[4:6]}\")\n",
    "                if metadata.get('duration_seconds'):\n",
    "                    print(f\"  Resolution: {metadata['width']}x{metadata['height']}\")\n",
    "                    print(f\"  Frame rate: {metadata['fps']:.1f} fps\")\n",
    "                    print(f\"  Duration: {metadata['duration_seconds']:.1f}s ({metadata['duration_seconds']/60:.1f} min)\")\n",
    "                print(f\"  File size: {metadata['file_size_mb']:.1f} MB\")\n",
    "                print()\n",
    "\n",
    "print(f\"Total videos selected: {len(target_videos)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c68ad76-a257-4995-b5a5-68a08dbd5ce3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83d\udcbe Save Video Manifest\n",
    "\n",
    "Creates manifest files in the batch analysis directory with selected video metadata for tracking and downstream processing.\n",
    "\n",
    "### Output Files\n",
    "- JSON manifest with full metadata\n",
    "- CSV summary for quick review\n",
    "- Both saved to: `data/preprocessing/batch_analysis/YYYYMMDD/`"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d81f18d-9d9c-4fb7-8601-a159c621d497",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# save video manifest\n",
    "manifest_data = {\n",
    "    'processing_date': CONFIG['PROCESS_DATE'],\n",
    "    'target_time': CONFIG['TARGET_TIME'],\n",
    "    'created_at': datetime.now().isoformat(),\n",
    "    'total_cameras': len(camera_dirs),\n",
    "    'videos_found': len(target_videos),\n",
    "    'videos': []\n",
    "}\n",
    "\n",
    "for video_info in target_videos:\n",
    "    video_path = video_info['video_path']\n",
    "    time_str = video_path.stem.split('_')[2]\n",
    "    \n",
    "    manifest_data['videos'].append({\n",
    "        'camera': video_info['camera'],\n",
    "        'filename': video_path.name,\n",
    "        'full_path': str(video_path),\n",
    "        'recording_time': f\"{time_str[:2]}:{time_str[2:4]}:{time_str[4:6]}\",\n",
    "        'time_diff_minutes': video_info['time_diff_minutes'],\n",
    "        'file_size_mb': video_info.get('file_size_mb', 0),\n",
    "        'fps': video_info.get('fps', 0),\n",
    "        'width': video_info.get('width', 0),\n",
    "        'height': video_info.get('height', 0),\n",
    "        'duration_seconds': video_info.get('duration_seconds', 0)\n",
    "    })\n",
    "\n",
    "# save to output directory\n",
    "manifest_file = CONFIG['OUTPUT_DIR'] / f\"manifest_{CONFIG['PROCESS_DATE']}.json\"\n",
    "csv_file = CONFIG['OUTPUT_DIR'] / f\"manifest_{CONFIG['PROCESS_DATE']}.csv\"\n",
    "\n",
    "with open(manifest_file, 'w') as f:\n",
    "    json.dump(manifest_data, f, indent=2)\n",
    "\n",
    "df_manifest = pd.DataFrame(manifest_data['videos'])\n",
    "df_manifest.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"\u2713 Saved manifest: {manifest_file}\")\n",
    "print(f\"\u2713 Saved CSV: {csv_file}\")\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Videos selected: {len(target_videos)}\")\n",
    "print(f\"  Total size: {df_manifest['file_size_mb'].sum():.1f} MB\")\n",
    "print(f\"  Average duration: {df_manifest['duration_seconds'].mean()/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0ad1c2-4797-4196-a3b2-dcbbc116333f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83c\udfac Preview Extraction Configuration\n",
    "\n",
    "Sets parameters for extracting sample frames from each video for quality assessment.\n",
    "\n",
    "### Parameters\n",
    "- \ud83c\udfaf **frames_per_video**: Number of sample frames\n",
    "- \ud83c\udfaf **extraction_duration**: Seconds to sample from start\n",
    "- \ud83d\udcca **max_videos_to_preview**: Limit for testing\n",
    "- \ud83d\udcc1 **preview_dir**: Output location in batch directory"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7713a87-f1d5-4acd-a7da-c0fc530d0cac",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# preview extraction configuration\n",
    "PREVIEW_CONFIG = {\n",
    "    'frames_per_video': 5,          # \ud83c\udfaf sample frame count\n",
    "    'extraction_duration': 60,      # \ud83c\udfaf sample first 60s\n",
    "    'preview_dir': CONFIG['OUTPUT_DIR'] / 'preview_frames',\n",
    "    'max_videos_to_preview': None   # \ud83d\udcca None = all videos\n",
    "}\n",
    "\n",
    "# set preview limit\n",
    "if PREVIEW_CONFIG['max_videos_to_preview'] is None:\n",
    "    PREVIEW_CONFIG['max_videos_to_preview'] = len(target_videos)\n",
    "\n",
    "# create preview directory\n",
    "PREVIEW_CONFIG['preview_dir'].mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Preview Configuration:\")\n",
    "print(f\"  Frames per video: {PREVIEW_CONFIG['frames_per_video']}\")\n",
    "print(f\"  Duration to sample: {PREVIEW_CONFIG['extraction_duration']}s\")\n",
    "print(f\"  Output directory: {PREVIEW_CONFIG['preview_dir']}\")\n",
    "print(f\"  Videos to preview: {PREVIEW_CONFIG['max_videos_to_preview']} of {len(target_videos)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0a5a94-0850-4f15-a4ab-aaf014eb0405",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83c\udfa5 Extract Preview Frames\n",
    "\n",
    "Processes videos to extract sample frames with quality metrics for visual assessment.\n",
    "\n",
    "### Process\n",
    "- Extracts evenly-spaced frames from first N seconds\n",
    "- Calculates brightness and blur scores\n",
    "- Saves frames to preview directory"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929423e5-ab21-4b63-800b-06c456bdbe52",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# extract preview frames\n",
    "import cv2\n",
    "\n",
    "def extract_preview_frames(video_path, output_dir, num_frames=5, duration_seconds=60):\n",
    "    \"\"\"extract evenly spaced frames\"\"\"\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        return None\n",
    "    \n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration_frames = min(int(fps * duration_seconds), total_frames)\n",
    "    \n",
    "    # calculate frame indices\n",
    "    indices = np.linspace(0, duration_frames-1, num_frames, dtype=int)\n",
    "    \n",
    "    frames_data = []\n",
    "    for idx in indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            # calculate metrics\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            brightness = np.mean(gray)\n",
    "            blur_score = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "            \n",
    "            # save frame\n",
    "            frame_filename = f\"frame_{idx:04d}.jpg\"\n",
    "            frame_path = output_dir / frame_filename\n",
    "            cv2.imwrite(str(frame_path), frame)\n",
    "            \n",
    "            frames_data.append({\n",
    "                'index': idx,\n",
    "                'brightness': brightness,\n",
    "                'blur_score': blur_score,\n",
    "                'path': frame_path\n",
    "            })\n",
    "    \n",
    "    cap.release()\n",
    "    return frames_data\n",
    "\n",
    "# process videos\n",
    "print(\"Extracting preview frames...\")\n",
    "preview_results = []\n",
    "\n",
    "for i, video_info in enumerate(target_videos[:PREVIEW_CONFIG['max_videos_to_preview']]):\n",
    "    camera = video_info['camera']\n",
    "    video_path = video_info['video_path']\n",
    "    \n",
    "    # create camera subdirectory\n",
    "    output_dir = PREVIEW_CONFIG['preview_dir'] / camera\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # extract frames\n",
    "    frames_data = extract_preview_frames(\n",
    "        video_path, \n",
    "        output_dir,\n",
    "        PREVIEW_CONFIG['frames_per_video'],\n",
    "        PREVIEW_CONFIG['extraction_duration']\n",
    "    )\n",
    "    \n",
    "    if frames_data:\n",
    "        avg_brightness = np.mean([f['brightness'] for f in frames_data])\n",
    "        avg_blur = np.mean([f['blur_score'] for f in frames_data])\n",
    "        \n",
    "        preview_results.append({\n",
    "            'camera': camera,\n",
    "            'video_path': video_path,\n",
    "            'frames_extracted': len(frames_data),\n",
    "            'avg_brightness': avg_brightness,\n",
    "            'avg_blur_score': avg_blur,\n",
    "            'frames_data': frames_data\n",
    "        })\n",
    "        \n",
    "        print(f\"\u2713 {camera}: {len(frames_data)} frames, brightness={avg_brightness:.1f}, blur={avg_blur:.1f}\")\n",
    "        \n",
    "        # update checkpoint\n",
    "        checkpoint['processed'].append(camera)\n",
    "        save_checkpoint(checkpoint)\n",
    "    else:\n",
    "        print(f\"\u2717 {camera}: Failed to extract frames\")\n",
    "        checkpoint['failed'].append(camera)\n",
    "        save_checkpoint(checkpoint)\n",
    "\n",
    "print(f\"\\nCompleted: {len(preview_results)} of {PREVIEW_CONFIG['max_videos_to_preview']} videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100dcc1f-2abe-430a-800b-389245db8999",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83d\udcca Display Frame Previews\n",
    "\n",
    "Generates visual grid of extracted frames with quality metrics for review."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cacd3c6-083b-4fb2-a65d-9b74cead433d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# display frame previews\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(len(preview_results), PREVIEW_CONFIG['frames_per_video'], \n",
    "                        figsize=(PREVIEW_CONFIG['frames_per_video'] * 3, len(preview_results) * 2))\n",
    "\n",
    "if len(preview_results) == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for cam_idx, result in enumerate(preview_results):\n",
    "    camera = result['camera']\n",
    "    \n",
    "    # display each frame\n",
    "    for frame_idx, frame_data in enumerate(result['frames_data']):\n",
    "        ax = axes[cam_idx, frame_idx]\n",
    "        \n",
    "        # read and display\n",
    "        img = cv2.imread(str(frame_data['path']))\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(img_rgb)\n",
    "        ax.set_title(f\"F{frame_data['index']}\\nB:{frame_data['brightness']:.0f} S:{frame_data['blur_score']:.0f}\", \n",
    "                    fontsize=8)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # add camera label\n",
    "    axes[cam_idx, 0].text(-0.1, 0.5, f\"{camera}\\nB:{result['avg_brightness']:.0f}\\nS:{result['avg_blur_score']:.0f}\", \n",
    "                          transform=axes[cam_idx, 0].transAxes,\n",
    "                          ha='right', va='center', fontsize=9, weight='bold')\n",
    "\n",
    "plt.suptitle(f'Batch Preview - {CONFIG[\"PROCESS_DATE\"]} @ 12:00 ({len(preview_results)} cameras)', \n",
    "             fontsize=12, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# summary table\n",
    "df_preview = pd.DataFrame([{\n",
    "    'camera': r['camera'],\n",
    "    'brightness': r['avg_brightness'],\n",
    "    'blur_score': r['avg_blur_score']\n",
    "} for r in preview_results])\n",
    "\n",
    "print(f\"\\nQuality Summary ({len(preview_results)} cameras):\")\n",
    "print(f\"  Brightness: {df_preview['brightness'].min():.1f} - {df_preview['brightness'].max():.1f}\")\n",
    "print(f\"  Blur score: {df_preview['blur_score'].min():.1f} - {df_preview['blur_score'].max():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32c7631-dab2-45d9-983a-8010c9d20061",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83d\udcc8 Quality Analysis & Recommendations\n",
    "\n",
    "Analyzes frame quality metrics to identify videos needing individual review."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34d34d2-e99d-4fca-ba3b-8522da383ef3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# quality analysis\n",
    "df_quality = pd.DataFrame([{\n",
    "    'camera': r['camera'],\n",
    "    'brightness': r['avg_brightness'],\n",
    "    'blur_score': r['avg_blur_score']\n",
    "} for r in preview_results])\n",
    "\n",
    "# define thresholds\n",
    "brightness_low = df_quality['brightness'].quantile(0.25)\n",
    "brightness_high = df_quality['brightness'].quantile(0.75)\n",
    "blur_low = df_quality['blur_score'].quantile(0.25)\n",
    "blur_high = df_quality['blur_score'].quantile(0.75)\n",
    "blur_top = df_quality['blur_score'].quantile(0.90)  # top 10% sharpest\n",
    "\n",
    "# identify outliers\n",
    "needs_review = []\n",
    "high_quality = []\n",
    "\n",
    "for r in preview_results:\n",
    "    issues = []\n",
    "    if r['avg_brightness'] < brightness_low:\n",
    "        issues.append('low brightness')\n",
    "    elif r['avg_brightness'] > brightness_high:\n",
    "        issues.append('high brightness')\n",
    "    \n",
    "    if r['avg_blur_score'] < blur_low:\n",
    "        issues.append('high blur')\n",
    "    \n",
    "    if issues:\n",
    "        needs_review.append({\n",
    "            'camera': r['camera'],\n",
    "            'issues': ', '.join(issues),\n",
    "            'brightness': r['avg_brightness'],\n",
    "            'blur_score': r['avg_blur_score']\n",
    "        })\n",
    "    \n",
    "    # track high quality\n",
    "    if r['avg_blur_score'] >= blur_top:\n",
    "        high_quality.append(r['camera'])\n",
    "\n",
    "# display results\n",
    "print(\"Quality Analysis:\")\n",
    "print(f\"  Brightness quartiles: Q1={brightness_low:.1f}, Q3={brightness_high:.1f}\")\n",
    "print(f\"  Blur score quartiles: Q1={blur_low:.1f}, Q3={blur_high:.1f}\")\n",
    "print(f\"  Top 10% blur threshold: {blur_top:.1f}\")\n",
    "\n",
    "if high_quality:\n",
    "    print(f\"\\nHighest quality cameras ({len(high_quality)}):\")\n",
    "    for cam in high_quality:\n",
    "        print(f\"  {cam}\")\n",
    "\n",
    "if needs_review:\n",
    "    print(f\"\\nCameras needing review ({len(needs_review)}):\")\n",
    "    for video in needs_review:\n",
    "        print(f\"  {video['camera']}: {video['issues']}\")\n",
    "\n",
    "# scatter plot with colors\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for idx, row in df_quality.iterrows():\n",
    "    if row['camera'] in high_quality:\n",
    "        color = 'green'\n",
    "        s = 150\n",
    "    elif any(r['camera'] == row['camera'] for r in needs_review):\n",
    "        color = 'red'\n",
    "        s = 100\n",
    "    else:\n",
    "        color = 'blue'\n",
    "        s = 100\n",
    "    \n",
    "    plt.scatter(row['brightness'], row['blur_score'], s=s, color=color)\n",
    "    plt.annotate(row['camera'], (row['brightness'], row['blur_score']), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "plt.axvline(brightness_low, color='red', linestyle='--', alpha=0.5, label='Brightness Q1')\n",
    "plt.axvline(brightness_high, color='red', linestyle='--', alpha=0.5, label='Brightness Q3')\n",
    "plt.axhline(blur_low, color='blue', linestyle='--', alpha=0.5, label='Blur Q1')\n",
    "plt.axhline(blur_top, color='green', linestyle='--', alpha=0.5, label='Top 10% Blur')\n",
    "\n",
    "# legend\n",
    "plt.scatter([], [], c='green', s=150, label='Highest quality')\n",
    "plt.scatter([], [], c='red', s=100, label='Needs review')\n",
    "plt.scatter([], [], c='blue', s=100, label='Normal')\n",
    "\n",
    "plt.xlabel('Brightness')\n",
    "plt.ylabel('Blur Score (higher = sharper)')\n",
    "plt.title(f'Video Quality Distribution - {CONFIG[\"PROCESS_DATE\"]}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab10c7dd-f177-4556-afc3-50bc9beee475",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# quality samples visualization\n",
    "categories = {\n",
    "    'highest_quality': sorted([(r['camera'], r['avg_blur_score']) for r in preview_results], \n",
    "                             key=lambda x: x[1], reverse=True)[:3],\n",
    "    'needs_review': [(r['camera'], r['avg_blur_score']) for r in preview_results \n",
    "                     if r['camera'] in [n['camera'] for n in needs_review]][:3],\n",
    "    'highest_blur': sorted([(r['camera'], r['avg_blur_score']) for r in preview_results], \n",
    "                          key=lambda x: x[1])[:3],\n",
    "    'lowest_blur': sorted([(r['camera'], r['avg_blur_score']) for r in preview_results], \n",
    "                         key=lambda x: x[1])[:3]\n",
    "}\n",
    "\n",
    "# create display\n",
    "fig, axes = plt.subplots(4, 3, figsize=(12, 13))\n",
    "\n",
    "row_labels = ['Highest Quality\\n(Sharpest)', 'Needs Review', 'Highest Blur\\n(Worst)', 'Lowest Blur Score\\n(Most Blurry)']\n",
    "\n",
    "for row_idx, (category, cameras) in enumerate(categories.items()):\n",
    "    for col_idx in range(3):\n",
    "        ax = axes[row_idx, col_idx]\n",
    "        \n",
    "        if col_idx < len(cameras):\n",
    "            camera, blur_score = cameras[col_idx]\n",
    "            \n",
    "            # find camera data\n",
    "            camera_data = next(r for r in preview_results if r['camera'] == camera)\n",
    "            \n",
    "            # display middle frame\n",
    "            if camera_data['frames_data']:\n",
    "                middle_frame = camera_data['frames_data'][len(camera_data['frames_data'])//2]\n",
    "                img = cv2.imread(str(middle_frame['path']))\n",
    "                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                ax.imshow(img_rgb)\n",
    "                ax.set_title(f\"{camera}\\nB:{camera_data['avg_brightness']:.0f} S:{blur_score:.0f}\", \n",
    "                            fontsize=10)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'N/A', ha='center', va='center', fontsize=12)\n",
    "        \n",
    "        ax.axis('off')\n",
    "        \n",
    "        # row label\n",
    "        if col_idx == 0:\n",
    "            ax.text(-0.2, 0.5, row_labels[row_idx], \n",
    "                   transform=ax.transAxes, rotation=90,\n",
    "                   ha='center', va='center', fontsize=12, weight='bold')\n",
    "\n",
    "plt.suptitle('Quality Category Samples', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a2b9a7-0f20-4868-a1e6-a0e45fa54b67",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83d\udcda Quality Metrics Reference\n",
    "\n",
    "### Brightness (Luminance)\n",
    "Average pixel intensity across the image, measured on a 0-255 scale for 8-bit images.\n",
    "- **Calculation**: Mean of grayscale pixel values\n",
    "- **Reference**: [OpenCV Image Processing](https://docs.opencv.org/4.x/d7/d4d/tutorial_py_thresholding.html)\n",
    "- **MLOps Context**: [Google Cloud - Image Quality Assessment](https://cloud.google.com/vision/docs/detecting-properties)\n",
    "\n",
    "### Blur Score (Laplacian Variance)\n",
    "Measures image sharpness by computing variance of the Laplacian operator output.\n",
    "- **Higher values** = Sharper image (more edge detail)\n",
    "- **Lower values** = Blurrier image (less edge detail)\n",
    "- **Technical Details**: [Laplacian Operator - SciPy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.laplace.html)\n",
    "- **Research Paper**: [Diatom autofocusing in brightfield microscopy](https://www.researchgate.net/publication/234073097_Diatom_autofocusing_in_brightfield_microscopy_A_comparative_study)\n",
    "\n",
    "### Quartile-Based Outlier Detection\n",
    "Statistical method using Q1 (25th percentile) and Q3 (75th percentile) to identify anomalies.\n",
    "- **Pandas Documentation**: [DataFrame.quantile](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.quantile.html)\n",
    "- **Statistical Background**: [NIST - Quartiles](https://www.itl.nist.gov/div898/handbook/prc/section2/prc252.htm)\n",
    "\n",
    "### Color Space Conversion (BGR to RGB)\n",
    "OpenCV uses BGR format by default; conversion needed for matplotlib display.\n",
    "- **OpenCV Reference**: [Color Space Conversions](https://docs.opencv.org/4.x/d8/d01/group__imgproc__color__conversions.html)\n",
    "- **Why BGR?**: [Historical reasons from Windows API](https://learnopencv.com/why-does-opencv-use-bgr-color-format/)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8a2e4376-91bf-4fe1-870b-0e7d7666736b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83d\udcbe Export Selection - Option 1: Automatic\n",
    "\n",
    "Automatically select cameras that were flagged for quality issues."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e6d719-f36d-40a7-ba9e-205084646833",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# automatic selection based on quality\n",
    "# selected_cameras = []\n",
    "\n",
    "# for video in needs_review:\n",
    "#     selected_cameras.append(video['camera'])\n",
    "\n",
    "# print(f\"Auto-selected {len(selected_cameras)} cameras:\")\n",
    "# for cam in selected_cameras:\n",
    "#     print(f\"  {cam}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec965e5a-e60c-4b70-844c-17696c90bdc7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83d\udcbe Export Selection - Option 2: Manual\n",
    "\n",
    "Manually specify which cameras to process. Uncomment and edit the list."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a65c954-99e2-49e1-8480-43bee357bec2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# manual selection (uncomment to use)\n",
    "selected_cameras = ['ATL-1005', 'ATL-0972', 'ATL-0610', 'ATL-0973']\n",
    "\n",
    "print(f\"Manually selected {len(selected_cameras)} cameras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d60229d-7541-4ae8-a190-d1a6894ace6e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83d\udcbe Export Selection - Option 3: All Cameras\n",
    "\n",
    "Select all previewed cameras for individual processing."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c674a5-90da-4c9c-9776-cb75a86c9bbd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# select all cameras (uncomment to use)\n",
    "# selected_cameras = [r['camera'] for r in preview_results]\n",
    "\n",
    "# print(f\"Selected all {len(selected_cameras)} cameras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ca4b46-8d48-4678-8e7d-0b65bf536674",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83d\udcbe Save Selection Queue\n",
    "\n",
    "Save the selected cameras to a queue file for the individual preprocessing notebook."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98e1b94-5a11-444c-8dc4-6b646b40444e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# save selection queue\n",
    "selection_data = {\n",
    "    'batch_date': CONFIG['PROCESS_DATE'],\n",
    "    'selected_cameras': selected_cameras,\n",
    "    'selection_criteria': 'quality_based',  # update based on option used\n",
    "    'created_at': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "selection_file = CONFIG['OUTPUT_DIR'] / f\"individual_queue_{CONFIG['PROCESS_DATE']}.json\"\n",
    "with open(selection_file, 'w') as f:\n",
    "    json.dump(selection_data, f, indent=2)\n",
    "\n",
    "print(f\"\u2713 Saved {len(selected_cameras)} cameras to queue\")\n",
    "print(f\"  File: {selection_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1462158-d664-48d8-b6e6-4b4f93c7d693",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83d\udccb Batch Processing Summary\n",
    "\n",
    "Generate final summary report of the batch preprocessing workflow."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa7989b-6bb3-4a06-ac68-372af1ff36d7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# batch processing summary\n",
    "print(\"=\"*60)\n",
    "print(f\"BATCH PROCESSING SUMMARY - {CONFIG['PROCESS_DATE']}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Processing Statistics:\")\n",
    "print(f\"  Total cameras: {len(camera_dirs)}\")\n",
    "print(f\"  Videos found: {len(target_videos)}\")\n",
    "print(f\"  Videos previewed: {len(preview_results)}\")\n",
    "print(f\"  Frames extracted: {len(preview_results) * PREVIEW_CONFIG['frames_per_video']}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcc8 Quality Overview:\")\n",
    "print(f\"  Avg brightness: {df_quality['brightness'].mean():.1f}\")\n",
    "print(f\"  Avg blur score: {df_quality['blur_score'].mean():.0f}\")\n",
    "print(f\"  Videos flagged: {len(needs_review)}\")\n",
    "print(f\"  High quality: {len(high_quality)}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcc1 Output Location:\")\n",
    "print(f\"  {CONFIG['OUTPUT_DIR']}/\")\n",
    "\n",
    "print(f\"\\n\u2705 Next Steps:\")\n",
    "print(f\"  1. Review quality samples above\")\n",
    "print(f\"  2. Run individual preprocessing notebook\")\n",
    "print(f\"  3. Load: {selection_file.name}\")\n",
    "\n",
    "print(f\"\\nCompleted: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc19162-c88b-43b5-8f85-83d2fb646c89",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## \ud83c\udfaf Final Video Selection\n",
    "\n",
    "Review and finalize which videos to process. This creates the configuration file for the individual preprocessing notebook."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0b8603-1c00-4e3d-b06f-bb362d76569b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# final video selection\n",
    "print(\"Current selection:\")\n",
    "for i, cam in enumerate(selected_cameras):\n",
    "    print(f\"  {i+1}. {cam}\")\n",
    "\n",
    "print(f\"\\nTotal: {len(selected_cameras)} cameras\")\n",
    "\n",
    "# create final config\n",
    "preprocessing_config = {\n",
    "    'batch_date': CONFIG['PROCESS_DATE'],\n",
    "    'videos_to_process': selected_cameras,\n",
    "    'source_manifest': str(manifest_file),\n",
    "    'quality_threshold': {\n",
    "        'brightness_min': brightness_low,\n",
    "        'brightness_max': brightness_high,\n",
    "        'blur_min': blur_low\n",
    "    },\n",
    "    'created_at': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# save config\n",
    "config_file = CONFIG['OUTPUT_DIR'] / 'preprocessing_config.json'\n",
    "with open(config_file, 'w') as f:\n",
    "    json.dump(preprocessing_config, f, indent=2)\n",
    "\n",
    "print(f\"\\n\u2713 Saved preprocessing config: {config_file}\")\n",
    "print(f\"  Videos marked for processing: {len(selected_cameras)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}