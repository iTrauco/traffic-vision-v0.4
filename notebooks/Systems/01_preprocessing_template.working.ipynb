{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b9d10c5-195d-41d2-b74c-12b3e06f642c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## 📓 Notebook Manager\n",
    "\n",
    "This cell initializes the widgets required for managing your research notebook. Please run the cell below to enable functionality for:\n",
    "- Exporting cells tagged with `export` into a `clean` notebook\n",
    "- Generating a dynamic Table of Contents (TOC)\n",
    "- Exporting the notebook to GitHub-compatible Markdown\n",
    "\n",
    "➡️ **Be sure to execute the next cell before continuing with any editing or exporting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a9b6c9-d4cd-479c-8137-b5d45e716ab4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell 1 - Workflow Tools\n",
    "import sys\n",
    "sys.path.insert(0, '../../lib')\n",
    "\n",
    "from notebook_tools import TOCWidget, ExportWidget\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "# Create widget instances\n",
    "toc = TOCWidget()\n",
    "export = ExportWidget()\n",
    "\n",
    "# Create horizontal layout\n",
    "left_side = widgets.VBox([toc.button, export.button, toc.status])\n",
    "right_side = widgets.VBox([toc.output, export.output])\n",
    "\n",
    "# Display side by side\n",
    "display(widgets.HBox([left_side, right_side]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95312728-5bf6-40b4-854e-fbc5ff4b14e1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell 1 - Workflow Tools\n",
    "import sys\n",
    "sys.path.insert(0, '../../lib')\n",
    "\n",
    "from notebook_tools import TOCWidget, ExportWidget\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "# Create widget instances\n",
    "toc = TOCWidget()\n",
    "export = ExportWidget()\n",
    "\n",
    "# Create horizontal layout\n",
    "left_side = widgets.VBox([toc.button, export.button, toc.status])\n",
    "right_side = widgets.VBox([toc.output, export.output])\n",
    "\n",
    "# Display side by side\n",
    "display(widgets.HBox([left_side, right_side]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ba2379-457a-4ef1-b843-5cd9235dcd47",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "# 🚦 Traffic Video Preprocessing - Methodology [VERSION]\n",
    "\n",
    "## 🎯 Purpose\n",
    "This notebook implements the data preprocessing workflow for GDOT traffic camera videos. We process raw video files into frame sequences suitable for computer vision tasks.\n",
    "\n",
    "## 📋 Context\n",
    "- **Data Source**: 30 GDOT traffic camera feeds (recorded locally)\n",
    "- **Video Specs**: 480 resolution, 15 fps\n",
    "- **Methodology Goal**: Establish reproducible preprocessing workflow with clear documentation\n",
    "\n",
    "## 🔄 Workflow Overview\n",
    "1. Video ingestion and cataloging\n",
    "2. Frame extraction\n",
    "3. Quality control\n",
    "4. Spatial transformations\n",
    "5. Color normalization\n",
    "6. Temporal downsampling\n",
    "7. Data organization\n",
    "8. Export and storage\n",
    "\n",
    "## ⚡ Key Improvements (Methodology [VERSION])\n",
    "- Added reproducibility checkpoints\n",
    "- Streamlined workflow touchpoints\n",
    "- Enhanced error handling and logging\n",
    "\n",
    "## 📚 Notebook Structure\n",
    "- **Setup**: Environment and dependencies\n",
    "- **Processing**: Step-by-step video preprocessing\n",
    "- **Validation**: Quality checks and verification\n",
    "- **Summary**: Results and analysis (see end of notebook)\n",
    "\n",
    "*Processing completed: [DATE] | Methodology version: [VERSION]*\n",
    "\n",
    "**Last Updated**: [DATE]  \n",
    "**Author**: [NAME]  \n",
    "**Version**: [VERSION]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc58eef-463e-465b-9b53-acdd3bc398dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## 📑 Table of Contents (Auto-Generated)\n",
    "\n",
    "This section will automatically generate a table of contents for your research notebook once you run the **Generate TOC** function. The table of contents will help you navigate through your data collection, analysis, and findings as your citizen science project develops.\n",
    "\n",
    "➡️ **Do not edit this cell manually. It will be overwritten automatically.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877dad16-2ca4-4acd-acc0-064c3f5a8cd0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "<!-- TOC -->\n",
    "# Table of Contents\n",
    "\n",
    "- [📓 Notebook Manager](#📓-notebook-manager)\n",
    "- [🎯 Purpose](#🎯-purpose)\n",
    "- [📋 Context](#📋-context)\n",
    "- [🔄 Workflow Overview](#🔄-workflow-overview)\n",
    "- [⚡ Key Improvements (Methodology [VERSION])](#⚡-key-improvements-(methodology-[version]))\n",
    "- [📚 Notebook Structure](#📚-notebook-structure)\n",
    "- [📑 Table of Contents (Auto-Generated)](#📑-table-of-contents-(auto-generated))\n",
    "- [🔧 Environment Setup](#🔧-environment-setup)\n",
    "  - [📊 Analysis & Observations](#📊-analysis-&-observations)\n",
    "    - [Results](#results)\n",
    "    - [Observations](#observations)\n",
    "    - [Notes](#notes)\n",
    "- [🔄 Progress Tracking & Checkpoint System](#🔄-progress-tracking-&-checkpoint-system)\n",
    "- [💾 Initialize Checkpoint and Progress Tracking Functions](#💾-initialize-checkpoint-and-progress-tracking-functions)\n",
    "  - [📊 Analysis & ObservationS](#📊-analysis-&-observations)\n",
    "    - [Results](#results)\n",
    "    - [Observations](#observations)\n",
    "    - [Notes](#notes)\n",
    "- [📹 Video Ingestion & Cataloging](#📹-video-ingestion-&-cataloging)\n",
    "  - [📊 Analysis & Observations](#📊-analysis-&-observations)\n",
    "    - [Results](#results)\n",
    "    - [Observations](#observations)\n",
    "    - [Notes](#notes)\n",
    "- [🎞️ Frame Extraction](#🎞️-frame-extraction)\n",
    "  - [📊 Analysis & Observations](#📊-analysis-&-observations)\n",
    "    - [Results](#results)\n",
    "    - [Observations](#observations)\n",
    "    - [Notes](#notes)\n",
    "- [🔍 Image Quality Control](#🔍-image-quality-control)\n",
    "  - [📊 Analysis & Observations](#📊-analysis-&-observations)\n",
    "    - [Results](#results)\n",
    "    - [Observations](#observations)\n",
    "    - [Notes](#notes)\n",
    "- [📐 Spatial Transformations](#📐-spatial-transformations)\n",
    "  - [📊 Analysis & Observations](#📊-analysis-&-observations)\n",
    "    - [Results](#results)\n",
    "    - [Observations](#observations)\n",
    "    - [Notes](#notes)\n",
    "- [🎨 Color Space Normalization](#🎨-color-space-normalization)\n",
    "  - [📊 Analysis & Observations](#📊-analysis-&-observations)\n",
    "    - [Results](#results)\n",
    "    - [Observations](#observations)\n",
    "    - [Notes](#notes)\n",
    "- [⏱️ Temporal Downsampling](#⏱️-temporal-downsampling)\n",
    "  - [📊 Analysis & Observations](#📊-analysis-&-observations)\n",
    "    - [Results](#results)\n",
    "    - [Observations](#observations)\n",
    "    - [Notes](#notes)\n",
    "  - [📊 Analysis & Observations](#📊-analysis-&-observations)\n",
    "    - [Results](#results)\n",
    "    - [Observations](#observations)\n",
    "    - [Notes](#notes)\n",
    "  - [📊 Analysis & Observations](#📊-analysis-&-observations)\n",
    "    - [Results](#results)\n",
    "    - [Observations](#observations)\n",
    "    - [Notes](#notes)\n",
    "\n",
    "<!-- /TOC -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fedfb01-d2fa-4487-9acc-d87ed78e77b5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "\n",
    "## 🔧 Environment Setup\n",
    "\n",
    "This cell establishes the preprocessing environment by:\n",
    "\n",
    "1. **Import Required Libraries**\n",
    "   - OpenCV (cv2) for video processing and frame extraction\n",
    "   - NumPy for array operations and numerical computations\n",
    "   - Pandas for data organization and metadata management\n",
    "   - Logging for process tracking and error reporting\n",
    "   - System utilities for path handling and file operations\n",
    "\n",
    "2. **Library Verification**\n",
    "   - Check OpenCV installation and version\n",
    "   - Verify NumPy and Pandas availability\n",
    "   - Display version information for debugging\n",
    "\n",
    "3. **Initialize Helper Functions**\n",
    "   - **calculate_brightness()**: Compute average pixel intensity (0-255)\n",
    "   - **calculate_blur_score()**: Measure sharpness using Laplacian variance\n",
    "   - **get_video_metadata()**: Extract video properties (fps, resolution, duration)\n",
    "\n",
    "4. **Directory Setup**\n",
    "   - Create output directory structure\n",
    "   - Ensure path exists before processing begins\n",
    "\n",
    "5. **Codec Validation**\n",
    "   - Test preferred video codec availability\n",
    "   - Confirm fourcc code generation\n",
    "\n",
    "**Note**: This cell must run successfully before proceeding with video processing. Any import errors or missing dependencies will be reported here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41584407-6c34-4cd4-bc68-51809fcaf8ec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "### 📐 Preprocessing Configuration Parameters\n",
    "\n",
    "This cell defines all parameters for individual video preprocessing. Parameters are organized into categories with emoji indicators:\n",
    "\n",
    "#### Target Parameters\n",
    "- 🎯 **VIDEO_ID**: Specific camera to process (e.g., ATL-1005)\n",
    "- 🎯 **BATCH_DATE**: Date from batch analysis (YYYYMMDD format)\n",
    "\n",
    "#### Path Configuration  \n",
    "- 📁 **INPUT_BASE**: Root directory for video recordings\n",
    "- 📁 **OUTPUT_BASE**: Root directory for processed output\n",
    "- 📁 **VIDEO_DIR**: Derived path to specific camera/date videos\n",
    "- 📁 **OUTPUT_DIR**: Derived path for this preprocessing run\n",
    "\n",
    "#### Processing Settings\n",
    "- 📊 **FRAMES_TO_EXTRACT**: Total number of frames to extract\n",
    "- 📊 **SAMPLE_RATE**: Extract every Nth frame from video\n",
    "\n",
    "#### Quality Thresholds\n",
    "Values from batch analysis quality metrics:\n",
    "- 🔍 **brightness_min**: Minimum acceptable brightness (0-255)\n",
    "- 🔍 **brightness_max**: Maximum acceptable brightness (0-255)  \n",
    "- 🔍 **blur_min**: Minimum blur score (Laplacian variance)\n",
    "\n",
    "#### Video Settings\n",
    "- 🎥 **PREFERRED_CODEC**: Primary video codec for processing\n",
    "- 🎥 **FALLBACK_CODECS**: Alternative codecs if primary fails\n",
    "- 🎥 **MAX_FRAME_WIDTH**: Maximum frame width for resizing\n",
    "- 🎥 **MAX_FRAME_HEIGHT**: Maximum frame height for resizing\n",
    "- 🎥 **JPEG_QUALITY**: Output quality for saved frames (0-100)\n",
    "\n",
    "**Note**: These values are hardcoded for initial testing. Future versions will read from `preprocessing_config.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276a0658-274e-4798-9220-d3db425eb890",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# preprocessing configuration parameters\n",
    "from pathlib import Path\n",
    "\n",
    "CONFIG = {\n",
    "    # target parameters\n",
    "    'VIDEO_ID': 'ATL-1005',  # 🎯 camera to process\n",
    "    'BATCH_DATE': '20250620',  # 🎯 date from batch analysis\n",
    "    'TARGET_HOUR': 12,  # 🎯 target hour (noon)\n",
    "    \n",
    "    # path configuration  \n",
    "    'INPUT_BASE': Path.home() / 'traffic-recordings',  # 📁 video source\n",
    "    'OUTPUT_BASE': Path('../../data/preprocessing/individual_analysis'),  # 📁 output base\n",
    "    \n",
    "    # processing settings\n",
    "    'FRAMES_TO_EXTRACT': 300,  # 📊 total frames to extract\n",
    "    'SAMPLE_RATE': 15,  # 📊 extract every Nth frame\n",
    "    \n",
    "    # quality thresholds (from batch analysis)\n",
    "    'QUALITY_THRESHOLD': {\n",
    "        'brightness_min': 104.47,  # 🔍 minimum brightness\n",
    "        'brightness_max': 114.75,  # 🔍 maximum brightness  \n",
    "        'blur_min': 3494.35  # 🔍 minimum blur score\n",
    "    },\n",
    "    \n",
    "    # video settings\n",
    "    'PREFERRED_CODEC': 'mp4v',  # 🎥 primary codec\n",
    "    'FALLBACK_CODECS': ['h264', 'xvid'],  # 🎥 alternatives\n",
    "    'MAX_FRAME_WIDTH': 1920,  # 🎥 max width\n",
    "    'MAX_FRAME_HEIGHT': 1080,  # 🎥 max height\n",
    "    'JPEG_QUALITY': 95  # 🎥 output quality (0-100)\n",
    "}\n",
    "\n",
    "# derived paths\n",
    "date_formatted = f\"{CONFIG['BATCH_DATE'][:4]}-{CONFIG['BATCH_DATE'][4:6]}-{CONFIG['BATCH_DATE'][6:8]}\"\n",
    "CONFIG['OUTPUT_DIR'] = CONFIG['OUTPUT_BASE'] / date_formatted / CONFIG['VIDEO_ID']\n",
    "CONFIG['VIDEO_DIR'] = CONFIG['INPUT_BASE'] / CONFIG['VIDEO_ID'] / date_formatted\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Processing: {CONFIG['VIDEO_ID']} from {date_formatted}\")\n",
    "print(f\"  Output to: {CONFIG['OUTPUT_DIR']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c646e51-3fae-4fd0-bed5-487a444cc447",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "### Environment Initialization\n",
    "\n",
    "The preprocessing configuration parameters defined above will now be used to initialize the environment, import required libraries, and set up helper functions for video processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8108366a-28e8-4598-99a5-8f283a9f076c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# environment setup\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# verify opencv\n",
    "try:\n",
    "    print(f\"✓ OpenCV version: {cv2.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"⚠️ OpenCV not installed. Install with: pip install opencv-python\")\n",
    "    \n",
    "print(f\"✓ Python version: {sys.version.split()[0]}\")\n",
    "print(f\"✓ NumPy version: {np.__version__}\")\n",
    "print(f\"✓ Pandas version: {pd.__version__}\")\n",
    "\n",
    "# helper functions\n",
    "def calculate_brightness(frame):\n",
    "    \"\"\"Calculate average brightness of frame\"\"\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    return np.mean(gray)\n",
    "\n",
    "def calculate_blur_score(frame):\n",
    "    \"\"\"Calculate Laplacian variance (higher = sharper)\"\"\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    return cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "\n",
    "def get_video_metadata(video_path):\n",
    "    \"\"\"Extract video metadata\"\"\"\n",
    "    metadata = {}\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if cap.isOpened():\n",
    "        metadata['fps'] = cap.get(cv2.CAP_PROP_FPS)\n",
    "        metadata['frame_count'] = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        metadata['width'] = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        metadata['height'] = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        metadata['duration_seconds'] = metadata['frame_count'] / metadata['fps'] if metadata['fps'] > 0 else 0\n",
    "        metadata['codec'] = int(cap.get(cv2.CAP_PROP_FOURCC))\n",
    "        cap.release()\n",
    "    return metadata\n",
    "\n",
    "# create output directory\n",
    "CONFIG['OUTPUT_DIR'].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# verify codec support\n",
    "fourcc_test = cv2.VideoWriter_fourcc(*CONFIG['PREFERRED_CODEC'])\n",
    "print(f\"✓ Preferred codec '{CONFIG['PREFERRED_CODEC']}' fourcc: {fourcc_test}\")\n",
    "\n",
    "print(f\"\\n✓ Environment setup complete\")\n",
    "print(f\"  Output directory created: {CONFIG['OUTPUT_DIR']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2287d492-558d-4eb7-808f-039d01294068",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "### 📊 Analysis & Observations\n",
    "\n",
    "**Record your findings from the code execution above:**\n",
    "\n",
    "#### Results\n",
    "*What outputs or data were generated?*\n",
    "\n",
    "#### Observations\n",
    "*What patterns or behaviors did you notice?*\n",
    "\n",
    "#### Notes\n",
    "*Any issues, performance observations, or follow-up needed?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a86a85-9a42-43a6-9524-a78860aab6a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "*End of Environment Setup*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dba3644-cdd9-435d-9bdd-0fdaa83420d8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## 🔄 Progress Tracking & Checkpoint System\n",
    "\n",
    "The following cells implement simple progress tracking and checkpoint functionality to:\n",
    "\n",
    "1. **Track Processing Progress**\n",
    "   - Monitor which video is currently being processed\n",
    "   - Count successful vs failed videos\n",
    "   - Display elapsed time\n",
    "\n",
    "2. **Enable Restart Capability**\n",
    "   - Save progress after each video completes\n",
    "   - Automatically skip already-processed videos on rerun\n",
    "   - Maintain list of failed videos for retry\n",
    "\n",
    "This ensures we don't lose work if the kernel crashes and provides visibility into long-running processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9533f832-2bea-401c-9181-c0cc41e572bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## 💾 Initialize Checkpoint and Progress Tracking Functions\n",
    "\n",
    "This module establishes checkpoint and progress tracking capabilities for the preprocessing workflow. The system creates functions for saving and loading processing state, initializes timing and counting variables, recovers from any existing checkpoints, and provides real-time progress monitoring with completion status.\n",
    "\n",
    "**Implemented below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf61bec-dcc8-4f36-89c5-59e8726db565",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize tracking variables\n",
    "CHECKPOINT_FILE = \"preprocessing_checkpoint.json\"\n",
    "start_time = time.time()\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"Load previous progress if it exists\"\"\"\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        with open(CHECKPOINT_FILE, 'r') as f:\n",
    "            checkpoint = json.load(f)\n",
    "            print(f\"✓ Loaded checkpoint: {len(checkpoint['processed'])} videos already processed\")\n",
    "            return checkpoint\n",
    "    return {\n",
    "        \"processed\": [], \n",
    "        \"failed\": [], \n",
    "        \"last_completed\": None, \n",
    "        \"start_time\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "def save_checkpoint(checkpoint):\n",
    "    \"\"\"Save current progress\"\"\"\n",
    "    checkpoint['last_updated'] = datetime.now().isoformat()\n",
    "    with open(CHECKPOINT_FILE, 'w') as f:\n",
    "        json.dump(checkpoint, f, indent=2)\n",
    "\n",
    "def log_progress(video_name, status, checkpoint, total_videos):\n",
    "    \"\"\"Log progress and update checkpoint\"\"\"\n",
    "    if status == \"success\":\n",
    "        checkpoint['processed'].append(video_name)\n",
    "    else:\n",
    "        checkpoint['failed'].append(video_name)\n",
    "    \n",
    "    checkpoint['last_completed'] = video_name\n",
    "    save_checkpoint(checkpoint)\n",
    "    \n",
    "    # Display progress\n",
    "    elapsed = time.time() - start_time\n",
    "    processed_count = len(checkpoint['processed'])\n",
    "    failed_count = len(checkpoint['failed'])\n",
    "    \n",
    "    print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] {video_name}: {status}\")\n",
    "    print(f\"Progress: {processed_count}/{total_videos} | Failed: {failed_count} | Elapsed: {elapsed/60:.1f}min\")\n",
    "\n",
    "# Load any existing checkpoint\n",
    "checkpoint = load_checkpoint()\n",
    "print(f\"Ready to process videos. Checkpoint system initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a99890-dc00-47e7-ad4f-111e8514e520",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "### 📊 Analysis & ObservationS\n",
    "\n",
    "**Record your findings from the code execution above:**\n",
    "\n",
    "#### Results\n",
    "*What outputs or data were generated?*\n",
    "\n",
    "#### Observations\n",
    "*What patterns or behaviors did you notice?*\n",
    "\n",
    "#### Notes\n",
    "*Any issues, performance observations, or follow-up needed?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638ff93b-ff91-4ad6-b1fa-8dadc2e4239e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "*Initialize Checkpoint and Progress Tracking Functions*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470fa738-95f0-4932-b604-080b8d4e8116",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## 📹 Video Ingestion & Cataloging\n",
    "\n",
    "This module loads video files from the source directory and extracts technical metadata including resolution, frame rate, duration, and codec specifications. The cataloging process builds a comprehensive data inventory and identifies format variations that may impact downstream processing stages.\n",
    "\n",
    "*The following code cell implements the video ingestion module using FFmpeg and OpenCV for metadata extraction.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa976e32-bb3a-4896-b462-16ae23f24d2a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# video ingestion and cataloging\n",
    "def parse_timestamp(filename):\n",
    "    \"\"\"extract timestamp from filename\"\"\"\n",
    "    parts = filename.stem.split('_')\n",
    "    if len(parts) >= 3:\n",
    "        time_str = parts[2]\n",
    "        hours = int(time_str[:2])\n",
    "        minutes = int(time_str[2:4])\n",
    "        return hours * 60 + minutes  # minutes from midnight\n",
    "    return None\n",
    "\n",
    "# find videos\n",
    "video_files = list(CONFIG['VIDEO_DIR'].glob(f\"{CONFIG['VIDEO_ID']}_*.mp4\"))\n",
    "\n",
    "if not video_files:\n",
    "    raise FileNotFoundError(f\"No videos found for {CONFIG['VIDEO_ID']} on {CONFIG['BATCH_DATE']}\")\n",
    "\n",
    "# find closest to noon\n",
    "target_minutes = CONFIG['TARGET_HOUR'] * 60  # 720 minutes\n",
    "closest_video = None\n",
    "min_diff = float('inf')\n",
    "\n",
    "for video in video_files:\n",
    "    minutes = parse_timestamp(video)\n",
    "    if minutes is not None:\n",
    "        diff = abs(minutes - target_minutes)\n",
    "        if diff < min_diff:\n",
    "            min_diff = diff\n",
    "            closest_video = video\n",
    "\n",
    "CONFIG['selected_video'] = closest_video\n",
    "time_str = closest_video.stem.split('_')[2]\n",
    "print(f\"Selected: {closest_video.name}\")\n",
    "print(f\"  Starts at: {time_str[:2]}:{time_str[2:4]}:{time_str[4:6]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e2ddcf-b619-457c-8234-a8274b19fa72",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "### 📊 Analysis & Observations\n",
    "\n",
    "**Record your findings from the code execution above:**\n",
    "\n",
    "#### Results\n",
    "*What outputs or data were generated?*\n",
    "\n",
    "#### Observations\n",
    "*What patterns or behaviors did you notice?*\n",
    "\n",
    "#### Notes\n",
    "*Any issues, performance observations, or follow-up needed?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4131dd0-1091-4d35-9b00-2d0cc99177cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "*End of Video Ingestion & Cataloging*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debdeb37-39f5-4e56-93ae-28a4c289767f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## 🎞️ Frame Extraction\n",
    "\n",
    "This module samples frames from video sequences at specified temporal intervals. The extraction process converts temporal video data into spatial image representations suitable for computer vision processing and analysis.\n",
    "\n",
    "*The following code cell implements frame extraction using OpenCV with configurable sampling rates and output formats.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d498e8d1-9cb0-416c-b19e-ff0f1c82e0de",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# frame extraction\n",
    "import cv2\n",
    "\n",
    "print(f\"Frame Extraction\")\n",
    "print(f\"Extracting {CONFIG['FRAMES_TO_EXTRACT']} frames (every {CONFIG['SAMPLE_RATE']} frames)\")\n",
    "\n",
    "video_path = CONFIG['selected_video']\n",
    "cap = cv2.VideoCapture(str(video_path))\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise ValueError(f\"Cannot open video: {video_path}\")\n",
    "\n",
    "# create frames directory\n",
    "frames_dir = CONFIG['OUTPUT_DIR'] / 'frames'\n",
    "frames_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# extract frames\n",
    "frames_extracted = 0\n",
    "frame_index = 0\n",
    "\n",
    "while frames_extracted < CONFIG['FRAMES_TO_EXTRACT'] and cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # extract every Nth frame\n",
    "    if frame_index % CONFIG['SAMPLE_RATE'] == 0:\n",
    "        frame_filename = f\"frame_{frames_extracted:04d}.jpg\"\n",
    "        frame_path = frames_dir / frame_filename\n",
    "        \n",
    "        # save frame\n",
    "        cv2.imwrite(str(frame_path), frame, [cv2.IMWRITE_JPEG_QUALITY, CONFIG['JPEG_QUALITY']])\n",
    "        \n",
    "        frames_extracted += 1\n",
    "        if frames_extracted % 50 == 0:\n",
    "            print(f\"  Extracted {frames_extracted}/{CONFIG['FRAMES_TO_EXTRACT']} frames\")\n",
    "    \n",
    "    frame_index += 1\n",
    "\n",
    "cap.release()\n",
    "\n",
    "CONFIG['frames_dir'] = frames_dir\n",
    "CONFIG['frames_extracted'] = frames_extracted\n",
    "\n",
    "print(f\"\\n✓ Extracted {frames_extracted} frames to {frames_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7fa3c9-5eeb-42b5-8ce3-1d2f08629c0b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "### 📊 Analysis & Observations\n",
    "\n",
    "**Record your findings from the code execution above:**\n",
    "\n",
    "#### Results\n",
    "*What outputs or data were generated?*\n",
    "\n",
    "#### Observations\n",
    "*What patterns or behaviors did you notice?*\n",
    "\n",
    "#### Notes\n",
    "*Any issues, performance observations, or follow-up needed?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbc7a1a-18d4-4a72-825a-861e8e8c405a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "*End of Frame Extraction*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc524ea6-eb7c-4032-b0cf-ba8b6869aaaf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## 🔍 Image Quality Control\n",
    "\n",
    "\n",
    "This module filters out blurry, dark, or corrupted frames using automated quality metrics. The quality control process ensures only processable frames continue through the workflow, optimizing compute resources and improving downstream analysis reliability.\n",
    "\n",
    "\n",
    "*The following code cell implements quality filtering using Laplacian variance for blur detection, histogram analysis for exposure assessment, and file integrity checks.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18287815-edf0-4ae8-8fd8-ec181d18d1a5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# image quality control\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Image Quality Control\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# get all extracted frames\n",
    "frame_files = sorted(CONFIG['frames_dir'].glob(\"frame_*.jpg\"))\n",
    "print(f\"Checking quality of {len(frame_files)} frames\")\n",
    "\n",
    "print(\"\\nQuality Metrics Explained:\")\n",
    "print(\"- Brightness: Average pixel intensity (0-255)\")\n",
    "print(\"  Calculated as: mean(grayscale_image)\")\n",
    "print(\"- Blur Score: Laplacian variance (higher = sharper)\")\n",
    "print(\"  Calculated as: variance(Laplacian(grayscale_image))\")\n",
    "\n",
    "print(f\"\\nThresholds from batch analysis:\")\n",
    "print(f\"- Brightness must be: {CONFIG['QUALITY_THRESHOLD']['brightness_min']:.1f} - {CONFIG['QUALITY_THRESHOLD']['brightness_max']:.1f}\")\n",
    "print(f\"- Blur score must be: ≥ {CONFIG['QUALITY_THRESHOLD']['blur_min']:.1f}\")\n",
    "\n",
    "quality_results = []\n",
    "good_frames = []\n",
    "poor_frames = []\n",
    "\n",
    "for frame_path in frame_files:\n",
    "    # read frame\n",
    "    frame = cv2.imread(str(frame_path))\n",
    "    if frame is None:\n",
    "        poor_frames.append(frame_path)\n",
    "        continue\n",
    "    \n",
    "    # calculate metrics\n",
    "    brightness = calculate_brightness(frame)\n",
    "    blur_score = calculate_blur_score(frame)\n",
    "    \n",
    "    # check thresholds\n",
    "    passes_quality = (\n",
    "        brightness >= CONFIG['QUALITY_THRESHOLD']['brightness_min'] and\n",
    "        brightness <= CONFIG['QUALITY_THRESHOLD']['brightness_max'] and\n",
    "        blur_score >= CONFIG['QUALITY_THRESHOLD']['blur_min']\n",
    "    )\n",
    "    \n",
    "    result = {\n",
    "        'frame': frame_path.name,\n",
    "        'brightness': brightness,\n",
    "        'blur_score': blur_score,\n",
    "        'passes': passes_quality\n",
    "    }\n",
    "    quality_results.append(result)\n",
    "    \n",
    "    if passes_quality:\n",
    "        good_frames.append(frame_path)\n",
    "    else:\n",
    "        poor_frames.append(frame_path)\n",
    "\n",
    "# save quality report\n",
    "quality_df = pd.DataFrame(quality_results)\n",
    "quality_df.to_csv(CONFIG['OUTPUT_DIR'] / 'quality_report.csv', index=False)\n",
    "\n",
    "# visualize samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "fig.suptitle('Quality Control Preview - Sample Frames with Metrics')\n",
    "\n",
    "# show good frames\n",
    "axes[0, 0].text(0.5, 0.5, 'GOOD\\nFRAMES', ha='center', va='center', fontsize=14, weight='bold', color='green')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "for i in range(4):\n",
    "    if i < len(good_frames):\n",
    "        img = cv2.imread(str(good_frames[i]))\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        idx = quality_df[quality_df['frame'] == good_frames[i].name].index[0]\n",
    "        \n",
    "        axes[0, i+1].imshow(img_rgb)\n",
    "        axes[0, i+1].set_title(f\"B:{quality_df.loc[idx, 'brightness']:.0f} S:{quality_df.loc[idx, 'blur_score']:.0f}\", fontsize=10)\n",
    "        axes[0, i+1].axis('off')\n",
    "    else:\n",
    "        axes[0, i+1].axis('off')\n",
    "\n",
    "# show poor frames\n",
    "axes[1, 0].text(0.5, 0.5, 'POOR\\nFRAMES', ha='center', va='center', fontsize=14, weight='bold', color='red')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "for i in range(4):\n",
    "    if i < len(poor_frames):\n",
    "        img = cv2.imread(str(poor_frames[i]))\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        idx = quality_df[quality_df['frame'] == poor_frames[i].name].index[0]\n",
    "        \n",
    "        axes[1, i+1].imshow(img_rgb)\n",
    "        axes[1, i+1].set_title(f\"B:{quality_df.loc[idx, 'brightness']:.0f} S:{quality_df.loc[idx, 'blur_score']:.0f}\", fontsize=10)\n",
    "        axes[1, i+1].axis('off')\n",
    "    else:\n",
    "        axes[1, i+1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "CONFIG['good_frames'] = good_frames\n",
    "CONFIG['quality_results'] = quality_df\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Good frames: {len(good_frames)} (meet all thresholds)\")\n",
    "print(f\"  Poor frames: {len(poor_frames)} (fail one or more thresholds)\")\n",
    "print(f\"  Pass rate: {len(good_frames)/len(frame_files)*100:.1f}%\")\n",
    "\n",
    "# show why frames failed\n",
    "if poor_frames:\n",
    "    print(\"\\nFailure reasons (first 5):\")\n",
    "    for i, frame in enumerate(poor_frames[:5]):\n",
    "        idx = quality_df[quality_df['frame'] == frame.name].index[0]\n",
    "        b = quality_df.loc[idx, 'brightness']\n",
    "        s = quality_df.loc[idx, 'blur_score']\n",
    "        reasons = []\n",
    "        if b < CONFIG['QUALITY_THRESHOLD']['brightness_min']:\n",
    "            reasons.append(f\"too dark ({b:.0f})\")\n",
    "        elif b > CONFIG['QUALITY_THRESHOLD']['brightness_max']:\n",
    "            reasons.append(f\"too bright ({b:.0f})\")\n",
    "        if s < CONFIG['QUALITY_THRESHOLD']['blur_min']:\n",
    "            reasons.append(f\"too blurry ({s:.0f})\")\n",
    "        print(f\"  {frame.name}: {', '.join(reasons)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c4530-0a70-43d8-8cf2-88006a6b0a7c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "### 📊 Analysis & Observations\n",
    "**Record your findings from the code execution above:**\n",
    "\n",
    "#### Results\n",
    "*What outputs or data were generated?*\n",
    "\n",
    "#### Observations\n",
    "*What patterns or behaviors did you notice?*\n",
    "\n",
    "#### Notes\n",
    "*Any issues, performance observations, or follow-up needed?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cf554d-f7c8-418c-bb6a-9040699fb23f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "*End of Image Quality Control*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25076404-a9b5-4169-8da2-e88bf2268f40",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## 📐 Spatial Transformations\n",
    "\n",
    "This module applies geometric transformations to frames from the selected video. The transformations ensure consistent dimensions for downstream processing while maintaining aspect ratio.\n",
    "\n",
    "*The following code cell implements spatial transformations for frames from the single video being processed.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8675b31-af9f-4c14-b780-8a33aa8675bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# spatial transformations\n",
    "print(\"Spatial Transformations\")\n",
    "\n",
    "# only process good frames\n",
    "frames_to_transform = CONFIG['good_frames']\n",
    "print(f\"Transforming {len(frames_to_transform)} frames from video: {CONFIG['selected_video'].name}\")\n",
    "\n",
    "# create transformed directory\n",
    "transformed_dir = CONFIG['OUTPUT_DIR'] / 'transformed'\n",
    "transformed_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# target dimensions\n",
    "target_width = CONFIG['MAX_FRAME_WIDTH']\n",
    "target_height = CONFIG['MAX_FRAME_HEIGHT']\n",
    "\n",
    "transformed_frames = []\n",
    "\n",
    "for frame_path in frames_to_transform:\n",
    "    # read frame\n",
    "    frame = cv2.imread(str(frame_path))\n",
    "    height, width = frame.shape[:2]\n",
    "    \n",
    "    # resize if needed\n",
    "    if width > target_width or height > target_height:\n",
    "        scale = min(target_width/width, target_height/height)\n",
    "        new_width = int(width * scale)\n",
    "        new_height = int(height * scale)\n",
    "        frame = cv2.resize(frame, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # save transformed frame\n",
    "    output_path = transformed_dir / frame_path.name\n",
    "    cv2.imwrite(str(output_path), frame, [cv2.IMWRITE_JPEG_QUALITY, CONFIG['JPEG_QUALITY']])\n",
    "    transformed_frames.append(output_path)\n",
    "\n",
    "CONFIG['transformed_frames'] = transformed_frames\n",
    "print(f\"\\n✓ Completed spatial transformations for {CONFIG['VIDEO_ID']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2474e0dd-05bb-4fd6-84de-c25e642756e5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# spatial transformations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Spatial Transformations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "frames_to_transform = CONFIG['good_frames']\n",
    "print(f\"Processing {len(frames_to_transform)} frames from: {CONFIG['selected_video'].name}\")\n",
    "\n",
    "print(\"\\nWhat Spatial Transformations do:\")\n",
    "print(\"- Check frame dimensions against max allowed size\")\n",
    "print(\"- Resize if needed while maintaining aspect ratio\")\n",
    "print(\"- Apply consistent output format and quality\")\n",
    "print(\"- In this case: 480p videos are below max dimensions, so no resize needed\")\n",
    "\n",
    "# create transformed directory\n",
    "transformed_dir = CONFIG['OUTPUT_DIR'] / 'transformed'\n",
    "transformed_dir.mkdir(exist_ok=True)\n",
    "\n",
    "transformed_frames = []\n",
    "\n",
    "# process frames\n",
    "for i, frame_path in enumerate(frames_to_transform):\n",
    "    frame = cv2.imread(str(frame_path))\n",
    "    height, width = frame.shape[:2]\n",
    "    \n",
    "    # save frame (no resize needed for 480p)\n",
    "    output_path = transformed_dir / frame_path.name\n",
    "    cv2.imwrite(str(output_path), frame, [cv2.IMWRITE_JPEG_QUALITY, CONFIG['JPEG_QUALITY']])\n",
    "    transformed_frames.append(output_path)\n",
    "    \n",
    "    # show first frame details\n",
    "    if i == 0:\n",
    "        print(f\"\\nFrame dimensions: {width}x{height}\")\n",
    "        print(f\"Target max: {CONFIG['MAX_FRAME_WIDTH']}x{CONFIG['MAX_FRAME_HEIGHT']}\")\n",
    "        print(f\"Action: No resize needed (within limits)\")\n",
    "\n",
    "# visualize before/after\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# show sample frame\n",
    "sample_idx = min(5, len(frames_to_transform)-1)\n",
    "orig = cv2.imread(str(frames_to_transform[sample_idx]))\n",
    "trans = cv2.imread(str(transformed_frames[sample_idx]))\n",
    "\n",
    "ax1.imshow(cv2.cvtColor(orig, cv2.COLOR_BGR2RGB))\n",
    "ax1.set_title(f\"Original Frame\\n{orig.shape[1]}x{orig.shape[0]} pixels\")\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2.imshow(cv2.cvtColor(trans, cv2.COLOR_BGR2RGB))\n",
    "ax2.set_title(f\"Transformed Frame\\n{trans.shape[1]}x{trans.shape[0]} pixels\\nJPEG Quality: {CONFIG['JPEG_QUALITY']}%\")\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.suptitle('Spatial Transformation (480p video - no resize needed)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "CONFIG['transformed_frames'] = transformed_frames\n",
    "\n",
    "print(f\"\\n✓ Transformation complete\")\n",
    "print(f\"  Frames processed: {len(transformed_frames)}\")\n",
    "print(f\"  Output location: {transformed_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0abf14e-b84c-4031-a8fc-88e4c651b98c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "### 📊 Analysis & Observations\n",
    "**Record your findings from the code execution above:**\n",
    "\n",
    "#### Results\n",
    "*What outputs or data were generated?*\n",
    "\n",
    "#### Observations\n",
    "*What patterns or behaviors did you notice?*\n",
    "\n",
    "#### Notes\n",
    "*Any issues, performance observations, or follow-up needed?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93493f0b-1f8a-4256-a863-f9645372ce57",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "*End of Spatial Transformations*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74a8cfe-eef3-415c-9338-f49bce337a6b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## 🎨 Color Space Normalization\n",
    "\n",
    "This module normalizes color representation across frames from the processed video. The normalization ensures consistent color space handling between OpenCV (BGR) and display/ML frameworks (RGB).\n",
    "\n",
    "*The following code cell implements color space conversion and normalization for frames from the single video being processed.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ea6a9a-e727-482f-a05c-7444a673a84c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# color space normalization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"Color Space Normalization\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "frames_to_normalize = CONFIG['transformed_frames']\n",
    "print(f\"Normalizing {len(frames_to_normalize)} frames\")\n",
    "\n",
    "print(\"\\nTechnical Details:\")\n",
    "print(\"- OpenCV uses BGR (Blue-Green-Red) channel order by default\")\n",
    "print(\"- This is legacy from early Windows bitmap format\")\n",
    "print(\"- Most display systems and ML frameworks expect RGB order\")\n",
    "print(\"- BGR: pixel[0]=Blue, pixel[1]=Green, pixel[2]=Red\")\n",
    "print(\"- RGB: pixel[0]=Red, pixel[1]=Green, pixel[2]=Blue\")\n",
    "\n",
    "print(\"\\nWhy this matters:\")\n",
    "print(\"- Displaying BGR image as RGB swaps red and blue channels\")\n",
    "print(\"- Traffic lights would appear blue instead of red!\")\n",
    "print(\"- ML models trained on RGB data would see wrong colors\")\n",
    "\n",
    "# create normalized directory\n",
    "normalized_dir = CONFIG['OUTPUT_DIR'] / 'normalized'\n",
    "normalized_dir.mkdir(exist_ok=True)\n",
    "\n",
    "normalized_frames = []\n",
    "\n",
    "for i, frame_path in enumerate(frames_to_normalize):\n",
    "    # read frame (BGR)\n",
    "    frame_bgr = cv2.imread(str(frame_path))\n",
    "    \n",
    "    # convert BGR to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # save as RGB (PIL expects RGB)\n",
    "    output_path = normalized_dir / frame_path.name\n",
    "    # convert back to BGR for cv2.imwrite\n",
    "    cv2.imwrite(str(output_path), cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR), \n",
    "                [cv2.IMWRITE_JPEG_QUALITY, CONFIG['JPEG_QUALITY']])\n",
    "    normalized_frames.append(output_path)\n",
    "    \n",
    "    if i == 0:\n",
    "        print(f\"\\nFirst frame shape: {frame_bgr.shape}\")\n",
    "        print(f\"Data type: {frame_bgr.dtype}\")\n",
    "        print(f\"Value range: 0-255 (8-bit per channel)\")\n",
    "\n",
    "# show color space difference\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "sample = cv2.imread(str(frames_to_normalize[0]))\n",
    "\n",
    "# BGR channels\n",
    "axes[0, 0].imshow(sample[:,:,0], cmap='Blues')\n",
    "axes[0, 0].set_title('BGR - Blue Channel (index 0)')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(sample[:,:,1], cmap='Greens')\n",
    "axes[0, 1].set_title('BGR - Green Channel (index 1)')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[0, 2].imshow(sample[:,:,2], cmap='Reds')\n",
    "axes[0, 2].set_title('BGR - Red Channel (index 2)')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# RGB display\n",
    "sample_rgb = cv2.cvtColor(sample, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "axes[1, 0].imshow(sample)\n",
    "axes[1, 0].set_title('Direct Display (BGR - Wrong Colors)')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(sample_rgb)\n",
    "axes[1, 1].set_title('After Conversion (RGB - Correct Colors)')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# show pixel values\n",
    "pixel_y, pixel_x = 100, 100\n",
    "bgr_pixel = sample[pixel_y, pixel_x]\n",
    "rgb_pixel = sample_rgb[pixel_y, pixel_x]\n",
    "axes[1, 2].text(0.1, 0.7, f\"Sample pixel at ({pixel_x},{pixel_y}):\", fontsize=12, weight='bold')\n",
    "axes[1, 2].text(0.1, 0.5, f\"BGR: [{bgr_pixel[0]}, {bgr_pixel[1]}, {bgr_pixel[2]}]\", fontsize=11)\n",
    "axes[1, 2].text(0.1, 0.3, f\"RGB: [{rgb_pixel[0]}, {rgb_pixel[1]}, {rgb_pixel[2]}]\", fontsize=11)\n",
    "axes[1, 2].text(0.1, 0.1, \"Note: B↔R values swapped\", fontsize=10, style='italic')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.suptitle('Color Space Normalization: BGR → RGB')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "CONFIG['normalized_frames'] = normalized_frames\n",
    "\n",
    "print(f\"\\nConversion formula:\")\n",
    "print(f\"  RGB[0] = BGR[2]  (Red ← Blue position)\")\n",
    "print(f\"  RGB[1] = BGR[1]  (Green stays same)\")\n",
    "print(f\"  RGB[2] = BGR[0]  (Blue ← Red position)\")\n",
    "\n",
    "print(f\"\\n✓ Color normalization complete\")\n",
    "print(f\"  Frames processed: {len(normalized_frames)}\")\n",
    "print(f\"  Output: {normalized_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accea736-aaf3-4710-9ff3-892ac6766ecd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "### 📊 Analysis & Observations\n",
    "\n",
    "**Record your findings from the code execution above:**\n",
    "\n",
    "#### Results\n",
    "*What outputs or data were generated?*\n",
    "\n",
    "#### Observations\n",
    "*What patterns or behaviors did you notice?*\n",
    "\n",
    "#### Notes\n",
    "*Any issues, performance observations, or follow-up needed?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb15063-6cfc-4514-8230-5776f4371f6d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "*End of Color Space Normalization*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1da84f-4635-430d-a9ea-092d08a3f2f8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## ⏱️ Temporal Downsampling\n",
    "\n",
    "This module identifies frames with significant changes (vehicle movement) to reduce redundancy in the dataset. For traffic analysis, we want to keep frames showing vehicles and discard empty road frames.\n",
    "\n",
    "*The following code cell implements motion-based frame selection to identify frames with traffic activity.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e53307-f209-4897-b90e-56421b700ada",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# temporal downsampling\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Temporal Downsampling\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "frames_to_analyze = CONFIG['normalized_frames']\n",
    "print(f\"Analyzing {len(frames_to_analyze)} frames for motion\")\n",
    "\n",
    "print(\"\\nMotion Detection Method:\")\n",
    "print(\"- Compare consecutive frames pixel-by-pixel\")\n",
    "print(\"- Calculate absolute difference at each pixel location\")\n",
    "print(\"- Average all pixel differences to get motion score\")\n",
    "print(\"- Higher score = more pixels changed = likely vehicle movement\")\n",
    "\n",
    "print(\"\\nTechnical Process:\")\n",
    "print(\"1. Convert frames to grayscale (simplifies comparison)\")\n",
    "print(\"2. cv2.absdiff(frame1, frame2) computes |frame1 - frame2|\")\n",
    "print(\"3. For each pixel: if value changed from 100 to 120, difference = 20\")\n",
    "print(\"4. Motion score = average of all pixel differences\")\n",
    "\n",
    "# motion detection\n",
    "motion_scores = []\n",
    "frame_pairs = []\n",
    "\n",
    "for i in range(len(frames_to_analyze) - 1):\n",
    "    frame1 = cv2.imread(str(frames_to_analyze[i]), cv2.IMREAD_GRAYSCALE)\n",
    "    frame2 = cv2.imread(str(frames_to_analyze[i+1]), cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # calculate difference\n",
    "    diff = cv2.absdiff(frame1, frame2)\n",
    "    motion_score = np.mean(diff)\n",
    "    \n",
    "    motion_scores.append({\n",
    "        'frame': frames_to_analyze[i].name,\n",
    "        'next_frame': frames_to_analyze[i+1].name,\n",
    "        'motion_score': motion_score,\n",
    "        'has_motion': motion_score > 5.0  # threshold for traffic\n",
    "    })\n",
    "    \n",
    "    if i < 4:  # store first few for visualization\n",
    "        frame_pairs.append((frame1, frame2, diff, motion_score))\n",
    "\n",
    "# visualize motion detection\n",
    "fig, axes = plt.subplots(4, 3, figsize=(12, 10))\n",
    "\n",
    "print(\"\\nHeatmap Visualization Explained:\")\n",
    "print(\"- Black areas: No change between frames (static background)\")\n",
    "print(\"- Red/Orange areas: Pixel changes (moving vehicles)\")\n",
    "print(\"- Brighter colors = larger pixel value changes\")\n",
    "print(\"- Each bright spot represents a moving object\\n\")\n",
    "\n",
    "for i, (f1, f2, diff, score) in enumerate(frame_pairs):\n",
    "    axes[i,0].imshow(f1, cmap='gray')\n",
    "    axes[i,0].set_title(f'Frame {i}')\n",
    "    axes[i,0].axis('off')\n",
    "    \n",
    "    axes[i,1].imshow(f2, cmap='gray')\n",
    "    axes[i,1].set_title(f'Frame {i+1}')\n",
    "    axes[i,1].axis('off')\n",
    "    \n",
    "    axes[i,2].imshow(diff, cmap='hot')\n",
    "    axes[i,2].set_title(f'Motion: {score:.1f}')\n",
    "    axes[i,2].axis('off')\n",
    "\n",
    "plt.suptitle('Motion Detection Between Consecutive Frames')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# select frames with motion\n",
    "motion_df = pd.DataFrame(motion_scores)\n",
    "frames_with_motion = motion_df[motion_df['has_motion']]['frame'].tolist()\n",
    "\n",
    "# create downsampled directory\n",
    "downsampled_dir = CONFIG['OUTPUT_DIR'] / 'downsampled'\n",
    "downsampled_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# copy frames with motion\n",
    "downsampled_frames = []\n",
    "for frame_name in frames_with_motion:\n",
    "    src = normalized_dir / frame_name\n",
    "    dst = downsampled_dir / frame_name\n",
    "    frame = cv2.imread(str(src))\n",
    "    cv2.imwrite(str(dst), frame, [cv2.IMWRITE_JPEG_QUALITY, CONFIG['JPEG_QUALITY']])\n",
    "    downsampled_frames.append(dst)\n",
    "\n",
    "CONFIG['downsampled_frames'] = downsampled_frames\n",
    "\n",
    "print(f\"\\nMotion Score Interpretation:\")\n",
    "print(f\"- Score 0-2: Camera noise/compression artifacts\")\n",
    "print(f\"- Score 2-5: Minor changes (shadows, small movements)\")\n",
    "print(f\"- Score 5-10: Vehicle movement detected\")\n",
    "print(f\"- Score >10: Multiple vehicles or fast movement\")\n",
    "\n",
    "print(f\"\\nThreshold Selection:\")\n",
    "print(f\"- Threshold: > 5.0 (empirically chosen for traffic)\")\n",
    "print(f\"- Too low: Keeps frames with just noise\")\n",
    "print(f\"- Too high: Misses slow-moving vehicles\")\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"Original frames: {len(frames_to_analyze)}\")\n",
    "print(f\"Frames with motion: {len(downsampled_frames)}\")\n",
    "print(f\"Reduction: {(1 - len(downsampled_frames)/len(frames_to_analyze))*100:.1f}%\")\n",
    "\n",
    "# show motion distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot([s['motion_score'] for s in motion_scores])\n",
    "plt.axhline(y=5.0, color='r', linestyle='--', label='Threshold')\n",
    "plt.xlabel('Frame Pair')\n",
    "plt.ylabel('Motion Score (avg pixel difference)')\n",
    "plt.title('Motion Scores Across Video - Peaks Indicate Vehicle Movement')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nWhy This Matters for Traffic Analysis:\")\n",
    "print(f\"- Reduces dataset to frames with actual vehicles\")\n",
    "print(f\"- Eliminates redundant empty road frames\")\n",
    "print(f\"- Focuses ML training on relevant traffic patterns\")\n",
    "print(f\"- Saves storage and processing time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129648f9-36b8-49ec-b90e-82dbf66a9963",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "### 📊 Analysis & Observations\n",
    "\n",
    "**Record your findings from the code execution above:**\n",
    "\n",
    "#### Results\n",
    "*What outputs or data were generated?*\n",
    "\n",
    "#### Observations\n",
    "*What patterns or behaviors did you notice?*\n",
    "\n",
    "#### Notes\n",
    "*Any issues, performance observations, or follow-up needed?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86293766-f77a-468a-8d09-77e3d4e18d81",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "*End of Temporal Downsampling*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c5193-17d0-42d8-b21f-cc5fe04bed55",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "# 📁 Data Organization\n",
    "\n",
    "This module structures processed frames with comprehensive metadata linking back to source videos. The organization system maintains full traceability throughout the processing workflow and enables efficient data loading for downstream analysis.\n",
    "\n",
    "*The following code cell implements data structuring using JSON metadata files and hierarchical directory organization with pandas for efficient data indexing and retrieval.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f8dd7b-2abd-4d1f-bf39-4fbce636156a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# data organization\n",
    "print(\"Data Organization\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# compile metadata\n",
    "metadata = {\n",
    "    'camera_id': CONFIG['VIDEO_ID'],\n",
    "    'batch_date': CONFIG['BATCH_DATE'],\n",
    "    'source_video': CONFIG['selected_video'].name,\n",
    "    'processing_timestamp': datetime.now().isoformat(),\n",
    "    'parameters': {\n",
    "        'target_hour': CONFIG['TARGET_HOUR'],\n",
    "        'frames_extracted': CONFIG['FRAMES_TO_EXTRACT'],\n",
    "        'sample_rate': CONFIG['SAMPLE_RATE'],\n",
    "        'quality_thresholds': CONFIG['QUALITY_THRESHOLD'],\n",
    "        'jpeg_quality': CONFIG['JPEG_QUALITY']\n",
    "    },\n",
    "    'processing_summary': {\n",
    "        'frames_extracted': CONFIG['frames_extracted'],\n",
    "        'frames_good_quality': len(CONFIG['good_frames']),\n",
    "        'frames_poor_quality': len(frame_files) - len(CONFIG['good_frames']),\n",
    "        'frames_with_motion': len(CONFIG['downsampled_frames'])\n",
    "    }\n",
    "}\n",
    "\n",
    "# build frame inventory\n",
    "frame_inventory = []\n",
    "\n",
    "for stage, frame_list in [\n",
    "    ('extracted', frame_files),\n",
    "    ('quality_passed', CONFIG['good_frames']),\n",
    "    ('transformed', CONFIG['transformed_frames']),\n",
    "    ('normalized', CONFIG['normalized_frames']),\n",
    "    ('downsampled', CONFIG['downsampled_frames'])\n",
    "]:\n",
    "    for frame_path in frame_list:\n",
    "        # get quality metrics if available\n",
    "        quality_row = CONFIG['quality_results'][CONFIG['quality_results']['frame'] == frame_path.name]\n",
    "        \n",
    "        entry = {\n",
    "            'frame_name': frame_path.name,\n",
    "            'stage': stage,\n",
    "            'path': str(frame_path),\n",
    "            'brightness': quality_row['brightness'].values[0] if not quality_row.empty else None,\n",
    "            'blur_score': quality_row['blur_score'].values[0] if not quality_row.empty else None\n",
    "        }\n",
    "        frame_inventory.append(entry)\n",
    "\n",
    "# save metadata\n",
    "metadata_path = CONFIG['OUTPUT_DIR'] / 'metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "# save frame inventory\n",
    "inventory_df = pd.DataFrame(frame_inventory)\n",
    "inventory_path = CONFIG['OUTPUT_DIR'] / 'frame_inventory.csv'\n",
    "inventory_df.to_csv(inventory_path, index=False)\n",
    "\n",
    "print(f\"\\nOrganized data structure:\")\n",
    "print(f\"  {CONFIG['OUTPUT_DIR']}/\")\n",
    "print(f\"  ├── frames/          ({CONFIG['frames_extracted']} raw frames)\")\n",
    "print(f\"  ├── transformed/     ({len(CONFIG['transformed_frames'])} frames)\")\n",
    "print(f\"  ├── normalized/      ({len(CONFIG['normalized_frames'])} frames)\")\n",
    "print(f\"  ├── downsampled/     ({len(CONFIG['downsampled_frames'])} frames)\")\n",
    "print(f\"  ├── metadata.json\")\n",
    "print(f\"  ├── frame_inventory.csv\")\n",
    "print(f\"  └── quality_report.csv\")\n",
    "\n",
    "CONFIG['metadata'] = metadata\n",
    "CONFIG['inventory'] = inventory_df\n",
    "\n",
    "print(f\"\\n✓ Data organization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d02c98d-7040-420b-b724-81fc0d7e2ecc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "### 📊 Analysis & Observations\n",
    "\n",
    "**Record your findings from the code execution above:**\n",
    "\n",
    "#### Results\n",
    "*What outputs or data were generated?*\n",
    "\n",
    "#### Observations\n",
    "*What patterns or behaviors did you notice?*\n",
    "\n",
    "#### Notes\n",
    "*Any issues, performance observations, or follow-up needed?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594814c5-c34c-409e-931c-51146bdf0235",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "*End of Data Organization*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7105f31-f8eb-4dbb-9f98-082f9fe78304",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "## 💾 Export & Storage\n",
    "\n",
    "This module creates a final summary of the preprocessing workflow and organizes the processed frames for manual annotation. Since annotation will be done manually, this step simply documents what was processed and where the final frames are located.\n",
    "\n",
    "\n",
    "*The following code cell creates a preprocessing summary report and confirms the location of frames ready for annotation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ada715-bb53-4035-9655-671042f4f22f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# export and storage\n",
    "print(\"Export & Storage Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# create summary report\n",
    "summary = {\n",
    "    'preprocessing_complete': datetime.now().isoformat(),\n",
    "    'camera': CONFIG['VIDEO_ID'],\n",
    "    'video_processed': CONFIG['selected_video'].name,\n",
    "    'frames_ready_for_annotation': len(CONFIG['downsampled_frames']),\n",
    "    'annotation_directory': str(CONFIG['OUTPUT_DIR'] / 'downsampled'),\n",
    "    'processing_stages': {\n",
    "        '1_extracted': CONFIG['frames_extracted'],\n",
    "        '2_quality_filtered': len(CONFIG['good_frames']),\n",
    "        '3_transformed': len(CONFIG['transformed_frames']),\n",
    "        '4_normalized': len(CONFIG['normalized_frames']),\n",
    "        '5_motion_filtered': len(CONFIG['downsampled_frames'])\n",
    "    }\n",
    "}\n",
    "\n",
    "# save summary\n",
    "summary_path = CONFIG['OUTPUT_DIR'] / 'preprocessing_summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"Preprocessing Complete for {CONFIG['VIDEO_ID']}\")\n",
    "print(f\"\\nFrames ready for annotation: {len(CONFIG['downsampled_frames'])}\")\n",
    "print(f\"Location: {CONFIG['OUTPUT_DIR'] / 'downsampled'}\")\n",
    "print(f\"\\nProcessing reduction: {CONFIG['frames_extracted']} → {len(CONFIG['downsampled_frames'])} frames\")\n",
    "print(f\"Data saved: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ca80aa-1968-4a35-a760-910497aee906",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "### 📊 Analysis & Observations\n",
    "\n",
    "**Record your findings from the code execution above:**\n",
    "\n",
    "#### Results\n",
    "*What outputs or data were generated?*\n",
    "\n",
    "#### Observations\n",
    "*What patterns or behaviors did you notice?*\n",
    "\n",
    "#### Notes\n",
    "*Any issues, performance observations, or follow-up needed?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd234ea-a33d-4a91-87a5-e27c73e9c750",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "*End of Export & Storage*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f02280-67a5-4613-869e-e937541dd359",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "export"
    ]
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7049047-f055-4555-9a37-5b48a59022be",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
